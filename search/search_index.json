{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Packaging is an important and time-consuming part of authoring and maintaining Python packages. This is particularly true for projects that are not pure Python but contain code that needs to be compiled, and have to deal with distributing compiled extensions and with build dependencies. Many projects in the PyData ecosystem - which includes scientific computing, data science and ML/AI projects - fall into that category. This site aims to provide an overview of the most important Python packaging issues for such projects, with in-depth explanations and references. The content on this site is meant to provide insights and good reference material. This will hopefully provide common ground when discussing potential solutions for those problems or design changes in Python packaging as a whole or in individual packaging tools. The content is divided into \"meta topics\" and \"key issues\". Meta topics are mainly descriptions of aspects of Python packaging that are more or less inherent to the whole design of it, and consequences and limitations that follow from that. Key issues are more specific pain points felt by projects with native code. Key issues may also be more tractable to devise solutions or workarounds for. How are these topics chosen and ranked? The initial list of topics was constructed by soliciting input from ~25 people, who together are a representative subset of stakeholders: maintainers of widely used PyData projects like NumPy, scikit-learn, Apache Arrow, CuPy, Matplotlib, SciPy, H5py, Jupyter Hub and Spyder, maintainers of package repositories, package managers and build systems (Pip, PyPI, Conda, Conda-forge, Spack, Nix, pypa/build , Meson, and numpy.distutils ), engineers from hardware vendors like Intel and NVIDIA, engineers responsible for deploying software for HPC users, educators and organisers of user groups (WiMLDS, SciPy Lectures, Data Umbrella), Adding new topics and making changes to existing content on this site happens through community input on GitHub . Meta topics Build & package management concepts and terminology The multiple purposes of PyPI PyPI's author-led social model and its limitations Lack of a build farm for PyPI Expectations that projects provide ever more wheels Key issues Native dependencies This is, by some distance, the most important issue. Several types of native dependencies are discussed in detail: BLAS, LAPACK and OpenMP The Geospatial stack Complex C++ dependencies Depending on packages for which an ABI matters Packaging projects with GPU code Metadata handling on PyPI Distributing a package containing SIMD code Unsuspecting users getting failing from source builds Contributing All contributions are very welcome and appreciated! Ways to contribute include: Improving existing content on the website: extending or clarifying descriptions, adding relevant references, diagrams, etc. Providing feedback on existing content Proposing new topics for inclusion on the website, and writing the content for them ... and anything else you consider useful! The content for this website is maintained on GitHub . Acknowledgements Initial development of this website was sponsored by Intel , Initial development effort was led by Quansight Labs ,","title":"Home"},{"location":"#introduction","text":"Packaging is an important and time-consuming part of authoring and maintaining Python packages. This is particularly true for projects that are not pure Python but contain code that needs to be compiled, and have to deal with distributing compiled extensions and with build dependencies. Many projects in the PyData ecosystem - which includes scientific computing, data science and ML/AI projects - fall into that category. This site aims to provide an overview of the most important Python packaging issues for such projects, with in-depth explanations and references. The content on this site is meant to provide insights and good reference material. This will hopefully provide common ground when discussing potential solutions for those problems or design changes in Python packaging as a whole or in individual packaging tools. The content is divided into \"meta topics\" and \"key issues\". Meta topics are mainly descriptions of aspects of Python packaging that are more or less inherent to the whole design of it, and consequences and limitations that follow from that. Key issues are more specific pain points felt by projects with native code. Key issues may also be more tractable to devise solutions or workarounds for. How are these topics chosen and ranked? The initial list of topics was constructed by soliciting input from ~25 people, who together are a representative subset of stakeholders: maintainers of widely used PyData projects like NumPy, scikit-learn, Apache Arrow, CuPy, Matplotlib, SciPy, H5py, Jupyter Hub and Spyder, maintainers of package repositories, package managers and build systems (Pip, PyPI, Conda, Conda-forge, Spack, Nix, pypa/build , Meson, and numpy.distutils ), engineers from hardware vendors like Intel and NVIDIA, engineers responsible for deploying software for HPC users, educators and organisers of user groups (WiMLDS, SciPy Lectures, Data Umbrella), Adding new topics and making changes to existing content on this site happens through community input on GitHub .","title":"Introduction"},{"location":"#meta-topics","text":"Build & package management concepts and terminology The multiple purposes of PyPI PyPI's author-led social model and its limitations Lack of a build farm for PyPI Expectations that projects provide ever more wheels","title":"Meta topics"},{"location":"#key-issues","text":"Native dependencies This is, by some distance, the most important issue. Several types of native dependencies are discussed in detail: BLAS, LAPACK and OpenMP The Geospatial stack Complex C++ dependencies Depending on packages for which an ABI matters Packaging projects with GPU code Metadata handling on PyPI Distributing a package containing SIMD code Unsuspecting users getting failing from source builds","title":"Key issues"},{"location":"#contributing","text":"All contributions are very welcome and appreciated! Ways to contribute include: Improving existing content on the website: extending or clarifying descriptions, adding relevant references, diagrams, etc. Providing feedback on existing content Proposing new topics for inclusion on the website, and writing the content for them ... and anything else you consider useful! The content for this website is maintained on GitHub .","title":"Contributing"},{"location":"#acknowledgements","text":"Initial development of this website was sponsored by Intel , Initial development effort was led by Quansight Labs ,","title":"Acknowledgements"},{"location":"glossary/","text":"Glossary Acronyms Acronym ... stands for Explanation ABI Application Binary Interface See here API Application Programming Interface The sum total of available functions, classes, etc. of a given program ARM Advanced RISC Machines Family of RISC architectures, second-most widely used processor family after x86 AVX Advanced Vector eXtensions Various extensions to the x86 instruction set (AVX, AVX2, AVX512), evolution after SSE BLAS Basic Linear Algebra Subprograms Specification resp. implementation for low-level linear algebra routines BOLT Binary Optimization and Layout Tool See here cffi The C FFI for Python See here CI Continuous Integration Testing all changes made to a given software, resp. the infrastructure that makes this possible CLI Command Line Interface CPU Central Processing Unit Main circuitry for executing machine instructions on a computer; contrast GPU CRAN Comprehensive R Archive Network Main index for R language packages, comparable to PyPI CUDA Compute Unified Device Architecture Parallel Computing Framework for NVIDIA GPUs DRY Don't Repeat Yourself Principle in software development aimed at reducing repetition GCC GNU Compiler Collection Main compiler family (for C / C++ / Fortran etc.) on Linux GUI Graphical UI GNU GNU's Not Unix Collection of free software packages under GPL License GPL (GNU) General Public License Foundational \"copyleft\" license of the free software movement glibc GNU C Library Widely used implemetation of the C standard library FFI Foreign Function Interface Calling functions written in a different language than the one currently used GPU Graphics Processing Unit Specialized circuitry for quickly computing graphics-related tasks ILP64 - Name used for the standard 64-bit interface to BLAS/LAPACK. Also see \"(64 bit) Data Models\" below IR Intermediate Representation Language-agnostic yet still semantic representation of code within a compiler LAPACK Linear Algebra PACKage Standard software library for numerical linear algebra ISA Instruction Set Architecture Specification of an instruction set for a CPU; e.g. x86-64, arm64, ... JIT Just-in-time Compilation Compiling code just before execution; used in CUDA, PyTorch, PyPy, Numba etc. LLVM - Cross-platform compiler framework, home of Clang, MLIR, BOLT etc. LTO Link-Time Optimization See here LTS Long-Term Support Version of a given software/library/distribution designated for long-term support musl - An alternative implementation of the C standard library MPI Message Passing Interface Standard for message-passing in parallel computing MLIR Multi-Level IR Higher-level IR within LLVM; used i.a. in machine learning frameworks MSVC Microsoft Visual C++ Main compiler on Windows NEP Numpy Enhancement Proposal See here OpenMP Open Multi Processing Multi-platform API for enabling multi-processing in C/C++/Fortran OS Operating System E.g. Linux, MacOS, Windows PEP Python Enhancement Proposal See here pip Pip Installs Packages Default installer for Python packages, distributed with CPython itself; see here PGO Profile-Guided Optimization See here PSF Python Software Foundation See here PyPA Python Packaging Authorithy Group which maintains core set of projects in Python packaging PyPI Python Package Index Main index where packages get installed from PyPy - An implementation of the Python specification in (quasi-)Python, with JIT capabilities QEMU Quick EMUlator Predominant emulation framework on Linux RHEL Red Hat Enterprise Linux Commercial distribution with some of the longest-running support timelines RISC Reduced Instruction Set Computer Paradigm underlying many past and current CPU architectures ROCm Radeon Open Compute Software stack for AMD GPUs; comparable to CUDA sdist Source DISTribution An archive of the source code of a Python project with metadata SIMD Single Instruction, Multiple Data CPU-specific instructions that can process more data in a single instruction SIG Special Interest Group E.g., Distutils-SIG (now replaced by Discourse ) SSE Streaming SIMD Extensions Various extensions to the x86 instruction set (SSE, SSE2, SSE3, SSSE3, SSE4) for SIMD TOML Tom's Obvious Minimal Language Configuration language chosen for pyproject.toml , cargo etc., see here UCRT Universal C Runtime Windows equivalent to glibc/musl UI User Interface UX User eXperience VCS Version Control System Tool to keep track of changes in source code, e.g. git venv Virtual ENVironments Python standard library module for creating environments; distinct from virtualenv Terms Term Explanation Examples / References Architecture In the context of packaging software, this generally refers to the CPU architecture (=ISA) ABI Break Failing to maintain the ABI See here Binary Compatibility Succeeding to maintain the ABI (e.g. across versions / upgrades) See here Build Backend Specifically in the context of pyproject.toml builds, the tool responsible for building a Python package setuptools , flit , hatch , ... Build Frontend Specifically in the context of pyproject.toml builds, the tool used to trigger a build Predominantly pip Calling Convention Agreed-upon contract with describes how to interact with a given CPU (family) See here cargo Package manager for the Rust language, often upheld as a positive example for installation UX See here Conda Cross-platform package & environment manager, based on distribution channels separate from PyPI See here Conda-forge Community-led packaging effort for (predominantly) Python packages See here Cross-compilation Compiling on a given platform for another platform See here (64 bit) Data Models Choice of bit-widths for int / long integer types ILP32, ILP64, LP64; see here Distribution An entity distributing (consistent) binary artefacts, often forming its own ecosystem Incl. OS: Debian, Fedora, Ubuntu, RHEL... OS-less: Chocolatey, Conda, Spack, ... distutils Python standard library module for building and installing packages; added in 1.6, to be removed in 3.12 See here easy_install Deprecated method for installing Python packages, superseded by pip install See here Egg Historical format for distributing Python packages See here Emulation Pretending to run on a different CPU architecture; this can be used to avoid cross-compilation See QEMU, resp. here Linker A tool to correctly find the required third-party symbols for a given project GNU's gold, LLVM's lld, mold Mamba Alternative implementation of the conda CLI tool with a faster solver See here Manylinux Baseline tooling to allow distributing wheels across various Linux distributions See PEP 600 and the PEPs it replaces numpy.distutils Extension to distutils , adding i.a. support for BLAS/LAPACK, Fortran, SIMD etc. See here Platform Colloquially used as interchangeable with the OS, though really only fully specified by the target triple pyproject.toml Standard metadata file for Python packages See PEP 517 & 518 setuptools Most widely used tool for building Python packages; new home of distutils See here Symbol A compiled version of a function See here Tarball Colloquial name for various flavors of .tar archive files See here (Target) Triple Unambiguous specification of the platform for the purpose of (cross-)compiling software for it, usually <arch>-<vendor>-<OS> See PEP 11 , resp. here or here virtualenv Installable module for handling virtual environments; largely a superset of venv See here Wheel A format for distributing and installing binary artefacts for Python packages; essentially a tarball plus metadata See here","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#acronyms","text":"Acronym ... stands for Explanation ABI Application Binary Interface See here API Application Programming Interface The sum total of available functions, classes, etc. of a given program ARM Advanced RISC Machines Family of RISC architectures, second-most widely used processor family after x86 AVX Advanced Vector eXtensions Various extensions to the x86 instruction set (AVX, AVX2, AVX512), evolution after SSE BLAS Basic Linear Algebra Subprograms Specification resp. implementation for low-level linear algebra routines BOLT Binary Optimization and Layout Tool See here cffi The C FFI for Python See here CI Continuous Integration Testing all changes made to a given software, resp. the infrastructure that makes this possible CLI Command Line Interface CPU Central Processing Unit Main circuitry for executing machine instructions on a computer; contrast GPU CRAN Comprehensive R Archive Network Main index for R language packages, comparable to PyPI CUDA Compute Unified Device Architecture Parallel Computing Framework for NVIDIA GPUs DRY Don't Repeat Yourself Principle in software development aimed at reducing repetition GCC GNU Compiler Collection Main compiler family (for C / C++ / Fortran etc.) on Linux GUI Graphical UI GNU GNU's Not Unix Collection of free software packages under GPL License GPL (GNU) General Public License Foundational \"copyleft\" license of the free software movement glibc GNU C Library Widely used implemetation of the C standard library FFI Foreign Function Interface Calling functions written in a different language than the one currently used GPU Graphics Processing Unit Specialized circuitry for quickly computing graphics-related tasks ILP64 - Name used for the standard 64-bit interface to BLAS/LAPACK. Also see \"(64 bit) Data Models\" below IR Intermediate Representation Language-agnostic yet still semantic representation of code within a compiler LAPACK Linear Algebra PACKage Standard software library for numerical linear algebra ISA Instruction Set Architecture Specification of an instruction set for a CPU; e.g. x86-64, arm64, ... JIT Just-in-time Compilation Compiling code just before execution; used in CUDA, PyTorch, PyPy, Numba etc. LLVM - Cross-platform compiler framework, home of Clang, MLIR, BOLT etc. LTO Link-Time Optimization See here LTS Long-Term Support Version of a given software/library/distribution designated for long-term support musl - An alternative implementation of the C standard library MPI Message Passing Interface Standard for message-passing in parallel computing MLIR Multi-Level IR Higher-level IR within LLVM; used i.a. in machine learning frameworks MSVC Microsoft Visual C++ Main compiler on Windows NEP Numpy Enhancement Proposal See here OpenMP Open Multi Processing Multi-platform API for enabling multi-processing in C/C++/Fortran OS Operating System E.g. Linux, MacOS, Windows PEP Python Enhancement Proposal See here pip Pip Installs Packages Default installer for Python packages, distributed with CPython itself; see here PGO Profile-Guided Optimization See here PSF Python Software Foundation See here PyPA Python Packaging Authorithy Group which maintains core set of projects in Python packaging PyPI Python Package Index Main index where packages get installed from PyPy - An implementation of the Python specification in (quasi-)Python, with JIT capabilities QEMU Quick EMUlator Predominant emulation framework on Linux RHEL Red Hat Enterprise Linux Commercial distribution with some of the longest-running support timelines RISC Reduced Instruction Set Computer Paradigm underlying many past and current CPU architectures ROCm Radeon Open Compute Software stack for AMD GPUs; comparable to CUDA sdist Source DISTribution An archive of the source code of a Python project with metadata SIMD Single Instruction, Multiple Data CPU-specific instructions that can process more data in a single instruction SIG Special Interest Group E.g., Distutils-SIG (now replaced by Discourse ) SSE Streaming SIMD Extensions Various extensions to the x86 instruction set (SSE, SSE2, SSE3, SSSE3, SSE4) for SIMD TOML Tom's Obvious Minimal Language Configuration language chosen for pyproject.toml , cargo etc., see here UCRT Universal C Runtime Windows equivalent to glibc/musl UI User Interface UX User eXperience VCS Version Control System Tool to keep track of changes in source code, e.g. git venv Virtual ENVironments Python standard library module for creating environments; distinct from virtualenv","title":"Acronyms"},{"location":"glossary/#terms","text":"Term Explanation Examples / References Architecture In the context of packaging software, this generally refers to the CPU architecture (=ISA) ABI Break Failing to maintain the ABI See here Binary Compatibility Succeeding to maintain the ABI (e.g. across versions / upgrades) See here Build Backend Specifically in the context of pyproject.toml builds, the tool responsible for building a Python package setuptools , flit , hatch , ... Build Frontend Specifically in the context of pyproject.toml builds, the tool used to trigger a build Predominantly pip Calling Convention Agreed-upon contract with describes how to interact with a given CPU (family) See here cargo Package manager for the Rust language, often upheld as a positive example for installation UX See here Conda Cross-platform package & environment manager, based on distribution channels separate from PyPI See here Conda-forge Community-led packaging effort for (predominantly) Python packages See here Cross-compilation Compiling on a given platform for another platform See here (64 bit) Data Models Choice of bit-widths for int / long integer types ILP32, ILP64, LP64; see here Distribution An entity distributing (consistent) binary artefacts, often forming its own ecosystem Incl. OS: Debian, Fedora, Ubuntu, RHEL... OS-less: Chocolatey, Conda, Spack, ... distutils Python standard library module for building and installing packages; added in 1.6, to be removed in 3.12 See here easy_install Deprecated method for installing Python packages, superseded by pip install See here Egg Historical format for distributing Python packages See here Emulation Pretending to run on a different CPU architecture; this can be used to avoid cross-compilation See QEMU, resp. here Linker A tool to correctly find the required third-party symbols for a given project GNU's gold, LLVM's lld, mold Mamba Alternative implementation of the conda CLI tool with a faster solver See here Manylinux Baseline tooling to allow distributing wheels across various Linux distributions See PEP 600 and the PEPs it replaces numpy.distutils Extension to distutils , adding i.a. support for BLAS/LAPACK, Fortran, SIMD etc. See here Platform Colloquially used as interchangeable with the OS, though really only fully specified by the target triple pyproject.toml Standard metadata file for Python packages See PEP 517 & 518 setuptools Most widely used tool for building Python packages; new home of distutils See here Symbol A compiled version of a function See here Tarball Colloquial name for various flavors of .tar archive files See here (Target) Triple Unambiguous specification of the platform for the purpose of (cross-)compiling software for it, usually <arch>-<vendor>-<OS> See PEP 11 , resp. here or here virtualenv Installable module for handling virtual environments; largely a superset of venv See here Wheel A format for distributing and installing binary artefacts for Python packages; essentially a tarball plus metadata See here","title":"Terms"},{"location":"other_issues/","text":"Other issues This page contains a collection of issues that do come up in the context of scientific and data science projects and packaging those, but are deemed less high-impact than the key issues. Lack of support for symlinks in wheels Shared libraries on Linux and other non-Windows platforms are often provided and versioned via symlinks (examples: cupy#6261 , libarrow in this Discourse thread , pip#5919 , and pypa/wheel issues #203 , #400 , and #453 ). In order to build wheels containing versioned shared libraries, symlink support is needed. In the absence of that, the symlinks get materialized into full copies of the symlinked files, blowing up wheel sizes. A second use case for symlinks is for editable installs when the build system uses out-of-place builds. Out-of-place builds are the only option in Meson, and also good practice for CMake. For out-of-place builds, you end up with compiled extension modules and generated files in the build directory, and .py files in the source directory. To put those together into a working editable install, the most straightforward solution is putting symlinks to all files in a wheel - see meson-python#47 . It looks like there is an understanding now that symlink support is needed, and that it requires a new wheel format spec (and hence a PEP) - see Clarifications to the wheel specification . An experimental setuptools extension, wheel-axle , implements support for producing a wheel containing symlinks. Dropping support for old manylinux versions is difficult Due to how wheel tags work, they need to be explicitly recognized by build and install tools. Old versions of pip tend to be used for years (especially in Linux distros), which means that when a project starts distributing wheels in a newer format (e.g., manylinux2014 instead of manylinux1 ), those new wheels will not be recognized for part of the user base for a long time. As a result, projects are forced to also continue distributing the older format, to avoid those users getting no wheels and a build from sdist instead. Being forced to produce duplicate wheels for years is a lot of extra work and CI time. This is in principle a problem on all platforms, it tends to show up more for Linux because of the combination of old pip versions and more changes to platform tags (we've had manylinux1 , manylinux2010 , manylinux2014 and now, with PEP 600, \"perennial manylinux\" - but that still requires agreeing on new glibc versions to start shipping in practice). Wheel build tooling is implemented in a scattered fashion When working with native dependencies, one must use a tool to vendor dependencies that aren't part of the platform by wheel standards. There are at least three different tools for this: auditwheel (Linux), delocate (macOS) and delvewheel (Windows). They have the same job, but are three independent projects with different capabilities. This is bad from a usability perspective, and when improvements to this tooling needs to be made, the discussion may have to be had multiple times (example: adding an --exclude option to not vendor certain libraries: auditwheel#368 ). This scattering issue can also be observed in the many support packages to deal with metadata, wheel tags, and other aspects of producing wheels, e.g.: packaging , distlib , pyproject-hooks , and pyproject-metadata . And with pip and build not using the same UX for things like --config-settings and --no-isolation/--no-build-isolation . Bootstrapping and circular dependencies of Python packaging tools Python packaging tools have a bit of a bootstrapping issue, which is a problem for other packaging systems when they want to incorporate those packages. If one wants to build and install pip / setuptools / wheel from source, one needs pip / setuptools / wheel already installed. Same for pypa/build and pypa/installer and flit ( poetry does better here, it uses setuptools to build itself). This is getting better - pip vendors all of its runtime dependencies so it can produce a wheel to install itself, and flit now vendors a TOML parser - but there is still a ways to go. See this Bootstrapping a specific version of pip thread for some discussion on this. No good way to install headers or non-Python libraries If a library provides functionality that is meant to be used from C or C++ code in another package, one needs to install headers and libraries. To make that work well, those headers and libraries should be installed in a place where other tools can find them. There are standard places for thison a system, e.g. for a prefix /usr the headers may go into /usr/include/ and the libraries in /usr/lib . This is technically possible with wheels, but recommended against because the install process may clobber system files. As a result, what projects like NumPy, Pybind11 and PyArrow end up doing is installing into their own tree under site-packages/pkgname (which certainly won't be on a search path), and then recommending that consuming packages query the location with a get_include function. E.g.: import pyarrow pyarrow . get_include () This isn't great, because it assumes that the build tool can run Python - and that breaks under cross-compilation. This would work much better under Conda/Spack/Homebrew/etc., however the packages themselves have to decide where to install to. Hence they choose to always install inside site-packages, due to the limitations that PyPI/wheels impose. More issues These still have to be worked out: UX for build and install tools is painful and easy to shoot oneself in the foot with (e.g., most users and maintainers don't understand the details of build isolation) Tooling will often assume virtualenvs only, and/or deal with environment activation when it really shouldn't.","title":"Other issues"},{"location":"other_issues/#other-issues","text":"This page contains a collection of issues that do come up in the context of scientific and data science projects and packaging those, but are deemed less high-impact than the key issues.","title":"Other issues"},{"location":"other_issues/#lack-of-support-for-symlinks-in-wheels","text":"Shared libraries on Linux and other non-Windows platforms are often provided and versioned via symlinks (examples: cupy#6261 , libarrow in this Discourse thread , pip#5919 , and pypa/wheel issues #203 , #400 , and #453 ). In order to build wheels containing versioned shared libraries, symlink support is needed. In the absence of that, the symlinks get materialized into full copies of the symlinked files, blowing up wheel sizes. A second use case for symlinks is for editable installs when the build system uses out-of-place builds. Out-of-place builds are the only option in Meson, and also good practice for CMake. For out-of-place builds, you end up with compiled extension modules and generated files in the build directory, and .py files in the source directory. To put those together into a working editable install, the most straightforward solution is putting symlinks to all files in a wheel - see meson-python#47 . It looks like there is an understanding now that symlink support is needed, and that it requires a new wheel format spec (and hence a PEP) - see Clarifications to the wheel specification . An experimental setuptools extension, wheel-axle , implements support for producing a wheel containing symlinks.","title":"Lack of support for symlinks in wheels"},{"location":"other_issues/#dropping-support-for-old-manylinux-versions-is-difficult","text":"Due to how wheel tags work, they need to be explicitly recognized by build and install tools. Old versions of pip tend to be used for years (especially in Linux distros), which means that when a project starts distributing wheels in a newer format (e.g., manylinux2014 instead of manylinux1 ), those new wheels will not be recognized for part of the user base for a long time. As a result, projects are forced to also continue distributing the older format, to avoid those users getting no wheels and a build from sdist instead. Being forced to produce duplicate wheels for years is a lot of extra work and CI time. This is in principle a problem on all platforms, it tends to show up more for Linux because of the combination of old pip versions and more changes to platform tags (we've had manylinux1 , manylinux2010 , manylinux2014 and now, with PEP 600, \"perennial manylinux\" - but that still requires agreeing on new glibc versions to start shipping in practice).","title":"Dropping support for old manylinux versions is difficult"},{"location":"other_issues/#wheel-build-tooling-is-implemented-in-a-scattered-fashion","text":"When working with native dependencies, one must use a tool to vendor dependencies that aren't part of the platform by wheel standards. There are at least three different tools for this: auditwheel (Linux), delocate (macOS) and delvewheel (Windows). They have the same job, but are three independent projects with different capabilities. This is bad from a usability perspective, and when improvements to this tooling needs to be made, the discussion may have to be had multiple times (example: adding an --exclude option to not vendor certain libraries: auditwheel#368 ). This scattering issue can also be observed in the many support packages to deal with metadata, wheel tags, and other aspects of producing wheels, e.g.: packaging , distlib , pyproject-hooks , and pyproject-metadata . And with pip and build not using the same UX for things like --config-settings and --no-isolation/--no-build-isolation .","title":"Wheel build tooling is implemented in a scattered fashion"},{"location":"other_issues/#bootstrapping-and-circular-dependencies-of-python-packaging-tools","text":"Python packaging tools have a bit of a bootstrapping issue, which is a problem for other packaging systems when they want to incorporate those packages. If one wants to build and install pip / setuptools / wheel from source, one needs pip / setuptools / wheel already installed. Same for pypa/build and pypa/installer and flit ( poetry does better here, it uses setuptools to build itself). This is getting better - pip vendors all of its runtime dependencies so it can produce a wheel to install itself, and flit now vendors a TOML parser - but there is still a ways to go. See this Bootstrapping a specific version of pip thread for some discussion on this.","title":"Bootstrapping and circular dependencies of Python packaging tools"},{"location":"other_issues/#no-good-way-to-install-headers-or-non-python-libraries","text":"If a library provides functionality that is meant to be used from C or C++ code in another package, one needs to install headers and libraries. To make that work well, those headers and libraries should be installed in a place where other tools can find them. There are standard places for thison a system, e.g. for a prefix /usr the headers may go into /usr/include/ and the libraries in /usr/lib . This is technically possible with wheels, but recommended against because the install process may clobber system files. As a result, what projects like NumPy, Pybind11 and PyArrow end up doing is installing into their own tree under site-packages/pkgname (which certainly won't be on a search path), and then recommending that consuming packages query the location with a get_include function. E.g.: import pyarrow pyarrow . get_include () This isn't great, because it assumes that the build tool can run Python - and that breaks under cross-compilation. This would work much better under Conda/Spack/Homebrew/etc., however the packages themselves have to decide where to install to. Hence they choose to always install inside site-packages, due to the limitations that PyPI/wheels impose.","title":"No good way to install headers or non-Python libraries"},{"location":"other_issues/#more-issues","text":"These still have to be worked out: UX for build and install tools is painful and easy to shoot oneself in the foot with (e.g., most users and maintainers don't understand the details of build isolation) Tooling will often assume virtualenvs only, and/or deal with environment activation when it really shouldn't.","title":"More issues"},{"location":"references/","text":"References Build systems & building wheels Tarek Ziad\u00e9 - The fate of Distutils - Pycon Summit + Packaging Sprint detailed report (2010) Pauli Virtanen - Building Python wheels with Fortran for Windows (2017) Uwe Korn - How we build Apache Arrow's manylinux wheels (2019) Matthias Bussonnier - IPython reproducible builds (2020) Ralf Gommers - Moving SciPy to the Meson build system (2021) Henry Schreiner - Scikit-build proposal (2021) Dependency management Mike McGarr - Dependency Hell, Monorepos and beyond (2017) A talk that illustrates the problems with dependency management and transitive dependencies quite well. Pradyun Gedam - Testing the next-gen pip dependency resolver (2020) Sumana Harihareswara - Releasing pip 20.3, featuring new dependency resolver (2020) Henry Schreiner - Should You Use Upper Bound Version Constraints? (2021) A blog post taking a thorough look at upper bounds on versions of dependencies. PyPI & the wheel format Donald Stufft - Powering the Python Package Index (2016) Nathaniel Smith - pynativelib proposal (2016) Dustin Ingram - Inside the Cheeseshop: How Python Packaging Works (2018) Package management Travis Oliphant - Why I promote conda (2013) Jake VanderPlas - Conda: Myths and Misconceptions (2016) Wes McKinney - conda-forge and PyData's CentOS moment (2016) Other Ralf Gommers - Python packaging in 2021 - pain points and bright spots (2021)","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"references/#build-systems-building-wheels","text":"Tarek Ziad\u00e9 - The fate of Distutils - Pycon Summit + Packaging Sprint detailed report (2010) Pauli Virtanen - Building Python wheels with Fortran for Windows (2017) Uwe Korn - How we build Apache Arrow's manylinux wheels (2019) Matthias Bussonnier - IPython reproducible builds (2020) Ralf Gommers - Moving SciPy to the Meson build system (2021) Henry Schreiner - Scikit-build proposal (2021)","title":"Build systems &amp; building wheels"},{"location":"references/#dependency-management","text":"Mike McGarr - Dependency Hell, Monorepos and beyond (2017) A talk that illustrates the problems with dependency management and transitive dependencies quite well. Pradyun Gedam - Testing the next-gen pip dependency resolver (2020) Sumana Harihareswara - Releasing pip 20.3, featuring new dependency resolver (2020) Henry Schreiner - Should You Use Upper Bound Version Constraints? (2021) A blog post taking a thorough look at upper bounds on versions of dependencies.","title":"Dependency management"},{"location":"references/#pypi-the-wheel-format","text":"Donald Stufft - Powering the Python Package Index (2016) Nathaniel Smith - pynativelib proposal (2016) Dustin Ingram - Inside the Cheeseshop: How Python Packaging Works (2018)","title":"PyPI &amp; the wheel format"},{"location":"references/#package-management","text":"Travis Oliphant - Why I promote conda (2013) Jake VanderPlas - Conda: Myths and Misconceptions (2016) Wes McKinney - conda-forge and PyData's CentOS moment (2016)","title":"Package management"},{"location":"references/#other","text":"Ralf Gommers - Python packaging in 2021 - pain points and bright spots (2021)","title":"Other"},{"location":"background/binary_interface/","text":"Application Binary Interface (ABI) The Application Binary Interface (ABI) is a strange, emergent phenomenon. Neither the C nor C++ standard recognize its existence, however it is ubiquitous in discussions about the evolution of those standards. This is broadly because, in a world where everything always gets recompiled from scratch (essentially the purview of those standards), ABI would be irrelevant. However, reality has shown that the always-recompile model is not practical in the vast majority of contexts, and binary artifacts need to be distributed. As soon as this is attempted, we run into all the many potential problems that can appear at the interface between source code and the actual physical execution that is baked into an artifact. For some common terms, it's recommended to familiarize yourself with compilation concepts first. Note that ABI appears for all distribution of binary artifacts, not just those compiled from C/C++. The concepts are the same in all cases though, so for brevity, we restrict ourselves to these languages here. A key focus for binary packaging systems is maintaining binary compatibility . If the packagers are not successful in maintaining that compability, we then get an ABI break , which can lead to crashes, segfaults, corrupted data etc. Unsurprisingly, a lot of effort goes into avoiding them. An example of an ABI break Let's assume we are using a function f from a C library libfoo , and this function takes an argument of type long long . extern int f ( long long value ); int main () { f ( 1 ); return 0 ; } If we inspect the assembly mov edi , 1 call f we see that a single register, edi , is used to pass the value argument to the function f , which makes sense since our registers are 64 bits wide, and long long matches this on unix platforms. If we replace the first line with extern int f (__int128_t value); , the assembly becomes: mov edi , 1 xor esi , esi call f Here we see that we now need two registers, edi and esi , to correctly pass value into the function f . Imagine then a situation where we did not change our code away from long long , but where the author of libfoo (that contains f ) wants to upgrade to wider integers because their users are asking for that. If we upgrade the shared library libfoo (that our code is linked against) without recompilation of our own application, then our code would still run (because the linker would find a symbol with the right name f due to the lack of name mangling). However, because the artifact we had compiled expects f to take long long , it would only set up the edi register correctly, but the upgraded symbol for f would now use both edi and esi . If we're \"lucky\", this only leads to a crash, but it might lead to pretty much arbitrary behavior based on whatever happens to be in esi . This can include anything from wrong results to Heisenbugs based on any other code that might leave some random values in the esi register, all in a way that's generally extremely hard to debug. This is but a trivial example, there are innumerable ways for libraries you rely on to break their ABI, including: Changing anything about a function signature (C) Changing the function return type (C++) Changing (almost) anything about templating (C++) Adding a data member or virtual functions to a class Making something inline that previously wasn\u2019t Changing your compiler or any of many relevant compilation flags Etc. Interaction with shared libraries What the above means is that, bascially, you can never change anything about a C symbol that has been distributed to consumers in binary form, especially those in shared libraries. Since some code effectively must use shared libraries (not least the C standard library, in the form of glibc/ musl on linux, resp. the UCRT on Windows), this is one if not the reason for the importance of distributions, because those will ensure that all their packages are compiled consistently against a given version of libc (and other libraries like the C++ standard library). Similarly, this is why distributions only upgrade their glibc across major releases, because doing so for an LTS release would risk too much breakage, even though libc takes extreme care about remaining backwards compatible. The different levels of ABI breaks Fundamentally, ABI can break at pretty much any point that's involved in the computation of our program. For the sake of clarity, let us divide these into three different levels: ABI breaks in third-party libraries ABI breaks in compiler or due to compiler configuration ABI breaks in the language standard The further down we go this list, the more impactful an ABI break becomes, so much so that the latter two happen rarely if ever. When they do happen, they tend to leave long-lasting traces. ABI breaks on language level The last substantial ABI break in C++ was when the standard committee decided to standardize an implementation of std::string that made illegal a previously deployed GCC-extension that used copy-on-write behavior. This meant that all C++ code compiled with GCC against older C++ standards needed to be recompiled. Due to the timelines involved with LTS distributions like RHEL, this took a long time to percolate through the ecosystem. Episodes like these have led to extreme reluctance in the C & C++ committees to change anything that even remotely touches the ABI, even though there are often substantial improvements left on the table due to this (famously, std::regex is excruciatingly slow and cannot be fixed without breaking ABI). This leads to extreme scrutiny (and therefore sluggish pace) in standards development, which has its own knock-on effects (see section about Abseil below). This is also the reason why there are no larger integer types (e.g. int128 ) that are officially supported by the standard, because their introduction would be an ABI break . Generally, the contortions that the C/C++ committees put themselves through to avoid breaking ABI are spectacular (and have very explicit opportunity costs) 1 , but ease ABI problems for the levels above. Finally, the unrealized performance gains left on the table due to ABI stability (and millions of dollar cost of single percentage performance pessimization) were what led Google to push hard for an ABI break in the C++ committee, and their defeat at the C++ meeting in Prague is ultimately what led to them withdrawing their substantial resources from compiler development (principally clang), and start their own C++-alike language, Carbon . ABI breaks on compiler level For completeness, we need to distinguish that here we are not talking about ABI breaks in the compiler infrastructure (in many ways, compilers are less exposed because using them, i.e. recompiling, drastically lessens the exposure to ABI), but rather of ABI breaks in the artifacts produced by them. Generally, compiler authors are almost as reluctant to break ABI as the language committees, because the effects are largely the same. An exception was MSVC, which for a long time used to change the ABI it generated with every release, meaning that each Visual Studio release (before VS2015) required recompilation of all involved binary artifacts. Starting from VS2015 (up to including VS2022 currently), MSVC has not broken ABI anymore, which means it's possible to, for example, compile with VS2019 against a shared library produced by VS2017. However, compilers expose a vast majority of flags, some of which have impacts on the ABI of the produced artifacts. It's therefore essential to have some degree of homogeneity (resp. control / auditability) about the compilation flags being used in an ecosystem. ABI breaks in third party libraries This is the most common case; various libraries make different kinds of promises about the stability of their ABI. Some (certainly those lower in the stack, like OpenSSL) promise stringent ABI stability (except across major versions), whereas others might break ABI in every patch release. Knowing which library versions are compatible how is a pretty involved job, but services like abi-laboratory exist to ease this work. For distributions that focus on using shared libraries, this means they need to be able to track which packages are dependent on any given library, and then rebuild all those in a short timespan, in order to roll out a new version of that library (due to the fact that, absent explicit inline namespacing, there can only be one version of a shared library in any given environment). Contrast with default approach in Python packaging Because the standard packaging in Python (wheels) cannot express a dependency on non-Python artifacts, the default approach is to fully copy (\"vendor\") the respective binaries into the wheel, and mangle their symbols in such a way that they do not get accidentally picked up from elsewhere. Abseil Google's Abseil project fills a particular role, which is to backport advances in newer C++ standards, and make those facilities to projects that still need to compile with older standard versions (c.f. the speed of the standardization process due to ABI concerns above). One prominent example of such usage is absl::string_view , which backports the C++17 std::string_view back to C++11 & C++14 (this feature allows to heavily cut down on useless copies involving strings, which has a substantial performance impact). However, these backports are generally not ABI-compatible with the implementations for later standard versions. This puts Abseil in the curious position that the C++ standard version used to compile it has an impact on its ABI. This is because Abseil will, by default, pick standard facilities when available, and otherwise fall back to its backports. As an example, absl::string_view compiled with C++17 will use the C++17 std::string_view ABI, whereas for C++14 and below, it will have a different ABI. Due to the constraint around having only one shared library per environment, Abseil strongly recommends against distribution of its binary artifacts, especially in shared builds. In fact, the only mode of operation that is really considered supported by upstream Abseil is compiling all dependencies with the same C++ standard version. This is obviously incompatible with servicing a large ecosystem, where some libraries might still require C++11, and some already require C++20. This issue would be somewhat manageable if Abseil types were only ever used internally in libraries, meaning things could be solved with a certain degree of care which (static) builds of Abseil are available at build time. However, the situation is exacerbated drastically by the fact that several projects are using (or beginning to use) Abseil types in their public API, e.g. protobuf . In the worst case, this means a full bifurcation of the necessary builds, though more realistically, it means that all Abseil consumers more or less need to agree on a given ABI. Further alternatives exist (e.g. always using the backport types, even if newer C++ standards are used), but are non-trivial to realize at scale. Despite this article being very colorful, it's worth noting that it was written by the current editor of the C language, as well as a prolific proposal author for both C & C++. \u21a9","title":"Application Binary Interface (ABI)"},{"location":"background/binary_interface/#application-binary-interface-abi","text":"The Application Binary Interface (ABI) is a strange, emergent phenomenon. Neither the C nor C++ standard recognize its existence, however it is ubiquitous in discussions about the evolution of those standards. This is broadly because, in a world where everything always gets recompiled from scratch (essentially the purview of those standards), ABI would be irrelevant. However, reality has shown that the always-recompile model is not practical in the vast majority of contexts, and binary artifacts need to be distributed. As soon as this is attempted, we run into all the many potential problems that can appear at the interface between source code and the actual physical execution that is baked into an artifact. For some common terms, it's recommended to familiarize yourself with compilation concepts first. Note that ABI appears for all distribution of binary artifacts, not just those compiled from C/C++. The concepts are the same in all cases though, so for brevity, we restrict ourselves to these languages here. A key focus for binary packaging systems is maintaining binary compatibility . If the packagers are not successful in maintaining that compability, we then get an ABI break , which can lead to crashes, segfaults, corrupted data etc. Unsurprisingly, a lot of effort goes into avoiding them.","title":"Application Binary Interface (ABI)"},{"location":"background/binary_interface/#an-example-of-an-abi-break","text":"Let's assume we are using a function f from a C library libfoo , and this function takes an argument of type long long . extern int f ( long long value ); int main () { f ( 1 ); return 0 ; } If we inspect the assembly mov edi , 1 call f we see that a single register, edi , is used to pass the value argument to the function f , which makes sense since our registers are 64 bits wide, and long long matches this on unix platforms. If we replace the first line with extern int f (__int128_t value); , the assembly becomes: mov edi , 1 xor esi , esi call f Here we see that we now need two registers, edi and esi , to correctly pass value into the function f . Imagine then a situation where we did not change our code away from long long , but where the author of libfoo (that contains f ) wants to upgrade to wider integers because their users are asking for that. If we upgrade the shared library libfoo (that our code is linked against) without recompilation of our own application, then our code would still run (because the linker would find a symbol with the right name f due to the lack of name mangling). However, because the artifact we had compiled expects f to take long long , it would only set up the edi register correctly, but the upgraded symbol for f would now use both edi and esi . If we're \"lucky\", this only leads to a crash, but it might lead to pretty much arbitrary behavior based on whatever happens to be in esi . This can include anything from wrong results to Heisenbugs based on any other code that might leave some random values in the esi register, all in a way that's generally extremely hard to debug. This is but a trivial example, there are innumerable ways for libraries you rely on to break their ABI, including: Changing anything about a function signature (C) Changing the function return type (C++) Changing (almost) anything about templating (C++) Adding a data member or virtual functions to a class Making something inline that previously wasn\u2019t Changing your compiler or any of many relevant compilation flags Etc.","title":"An example of an ABI break"},{"location":"background/binary_interface/#interaction-with-shared-libraries","text":"What the above means is that, bascially, you can never change anything about a C symbol that has been distributed to consumers in binary form, especially those in shared libraries. Since some code effectively must use shared libraries (not least the C standard library, in the form of glibc/ musl on linux, resp. the UCRT on Windows), this is one if not the reason for the importance of distributions, because those will ensure that all their packages are compiled consistently against a given version of libc (and other libraries like the C++ standard library). Similarly, this is why distributions only upgrade their glibc across major releases, because doing so for an LTS release would risk too much breakage, even though libc takes extreme care about remaining backwards compatible.","title":"Interaction with shared libraries"},{"location":"background/binary_interface/#the-different-levels-of-abi-breaks","text":"Fundamentally, ABI can break at pretty much any point that's involved in the computation of our program. For the sake of clarity, let us divide these into three different levels: ABI breaks in third-party libraries ABI breaks in compiler or due to compiler configuration ABI breaks in the language standard The further down we go this list, the more impactful an ABI break becomes, so much so that the latter two happen rarely if ever. When they do happen, they tend to leave long-lasting traces.","title":"The different levels of ABI breaks"},{"location":"background/binary_interface/#abi-breaks-on-language-level","text":"The last substantial ABI break in C++ was when the standard committee decided to standardize an implementation of std::string that made illegal a previously deployed GCC-extension that used copy-on-write behavior. This meant that all C++ code compiled with GCC against older C++ standards needed to be recompiled. Due to the timelines involved with LTS distributions like RHEL, this took a long time to percolate through the ecosystem. Episodes like these have led to extreme reluctance in the C & C++ committees to change anything that even remotely touches the ABI, even though there are often substantial improvements left on the table due to this (famously, std::regex is excruciatingly slow and cannot be fixed without breaking ABI). This leads to extreme scrutiny (and therefore sluggish pace) in standards development, which has its own knock-on effects (see section about Abseil below). This is also the reason why there are no larger integer types (e.g. int128 ) that are officially supported by the standard, because their introduction would be an ABI break . Generally, the contortions that the C/C++ committees put themselves through to avoid breaking ABI are spectacular (and have very explicit opportunity costs) 1 , but ease ABI problems for the levels above. Finally, the unrealized performance gains left on the table due to ABI stability (and millions of dollar cost of single percentage performance pessimization) were what led Google to push hard for an ABI break in the C++ committee, and their defeat at the C++ meeting in Prague is ultimately what led to them withdrawing their substantial resources from compiler development (principally clang), and start their own C++-alike language, Carbon .","title":"ABI breaks on language level"},{"location":"background/binary_interface/#abi-breaks-on-compiler-level","text":"For completeness, we need to distinguish that here we are not talking about ABI breaks in the compiler infrastructure (in many ways, compilers are less exposed because using them, i.e. recompiling, drastically lessens the exposure to ABI), but rather of ABI breaks in the artifacts produced by them. Generally, compiler authors are almost as reluctant to break ABI as the language committees, because the effects are largely the same. An exception was MSVC, which for a long time used to change the ABI it generated with every release, meaning that each Visual Studio release (before VS2015) required recompilation of all involved binary artifacts. Starting from VS2015 (up to including VS2022 currently), MSVC has not broken ABI anymore, which means it's possible to, for example, compile with VS2019 against a shared library produced by VS2017. However, compilers expose a vast majority of flags, some of which have impacts on the ABI of the produced artifacts. It's therefore essential to have some degree of homogeneity (resp. control / auditability) about the compilation flags being used in an ecosystem.","title":"ABI breaks on compiler level"},{"location":"background/binary_interface/#abi-breaks-in-third-party-libraries","text":"This is the most common case; various libraries make different kinds of promises about the stability of their ABI. Some (certainly those lower in the stack, like OpenSSL) promise stringent ABI stability (except across major versions), whereas others might break ABI in every patch release. Knowing which library versions are compatible how is a pretty involved job, but services like abi-laboratory exist to ease this work. For distributions that focus on using shared libraries, this means they need to be able to track which packages are dependent on any given library, and then rebuild all those in a short timespan, in order to roll out a new version of that library (due to the fact that, absent explicit inline namespacing, there can only be one version of a shared library in any given environment). Contrast with default approach in Python packaging Because the standard packaging in Python (wheels) cannot express a dependency on non-Python artifacts, the default approach is to fully copy (\"vendor\") the respective binaries into the wheel, and mangle their symbols in such a way that they do not get accidentally picked up from elsewhere.","title":"ABI breaks in third party libraries"},{"location":"background/binary_interface/#abseil","text":"Google's Abseil project fills a particular role, which is to backport advances in newer C++ standards, and make those facilities to projects that still need to compile with older standard versions (c.f. the speed of the standardization process due to ABI concerns above). One prominent example of such usage is absl::string_view , which backports the C++17 std::string_view back to C++11 & C++14 (this feature allows to heavily cut down on useless copies involving strings, which has a substantial performance impact). However, these backports are generally not ABI-compatible with the implementations for later standard versions. This puts Abseil in the curious position that the C++ standard version used to compile it has an impact on its ABI. This is because Abseil will, by default, pick standard facilities when available, and otherwise fall back to its backports. As an example, absl::string_view compiled with C++17 will use the C++17 std::string_view ABI, whereas for C++14 and below, it will have a different ABI. Due to the constraint around having only one shared library per environment, Abseil strongly recommends against distribution of its binary artifacts, especially in shared builds. In fact, the only mode of operation that is really considered supported by upstream Abseil is compiling all dependencies with the same C++ standard version. This is obviously incompatible with servicing a large ecosystem, where some libraries might still require C++11, and some already require C++20. This issue would be somewhat manageable if Abseil types were only ever used internally in libraries, meaning things could be solved with a certain degree of care which (static) builds of Abseil are available at build time. However, the situation is exacerbated drastically by the fact that several projects are using (or beginning to use) Abseil types in their public API, e.g. protobuf . In the worst case, this means a full bifurcation of the necessary builds, though more realistically, it means that all Abseil consumers more or less need to agree on a given ABI. Further alternatives exist (e.g. always using the backport types, even if newer C++ standards are used), but are non-trivial to realize at scale. Despite this article being very colorful, it's worth noting that it was written by the current editor of the C language, as well as a prolific proposal author for both C & C++. \u21a9","title":"Abseil"},{"location":"background/compilation_concepts/","text":"Basic code compilation concepts What does this have to do with Python? Python as a glue language is essentially exposed to the sum of all problems that other languages have with their binary distribution, and this is a really vast field (even just for C/C++). Needless to say, adequately summarizing decades of work and developments that have lead us to where we are now is not easy. If you find errors or things to improve, please open an issue or PR! In order to get a computer to execute a given unit of work (say, application X calling a function f from a previously compiled library libfoo ), a lot of preconditions have to be met: The function needs to have been compiled into instructions that the computer understands; a symbol . The symbol for f needs to be named (resp. \"mangled\") in a consistent manner between the compilation of the current code (for X ), resp. the compilation of the library ( libfoo , which contains the symbol for f ). The symbol for f needs to have be discoverable from within the currently running process; assuming libfoo is available on the machine where we are compiling, this is ensured by the linker . Variables passed to the function need to be loaded into the right CPU registers; this is highly dependent on the calling convention of a given CPU, or rather, CPU family. The code (in X ) calling a given symbol (e.g. for f ) needs to be excruciatingly compatible with the actual implementation of that symbol (in libfoo ). It's not useful for the average programmer to consider this level of detail when trying to get work done, but it is unfortunately unavoidable when considering the realities of packaging and distributing software as pre-compiled binary artifacts. Symbols, mangling and linkers Symbols For any given function f you might have written in a language that needs to be compiled (C, C++, ...), you can consider a symbol as a translation of your function to a level that can actually be executed by your computer. So, for example, a simple square function int square ( int num ) { return num * num ; } will be translated to the following assembly on x86-64 hardware: square : push rbp mov rbp , rsp mov DWORD PTR [ rbp - 4 ] , edi mov eax , DWORD PTR [ rbp - 4 ] imul eax , eax pop rbp ret There is a lot of ceremony for reading in the argument into a register, setting up another register for the result, while the actual work is done in: imul eax , eax If you click the link above, you will see that this assembly looks completely different when compiled for another processor architecture (e.g. arm64, as used in Apple M1 and newer). Symbol name mangling Note as well that, in C, the symbol square has exactly the same name as the function - there is a 1:1 relationship, or in other words, there is no \"mangling\" of the symbol name. This is because C has no concept of overloading functions (i.e. having different functions of the same name but different signatures). It also means that you can never change anything about a C symbol that has been distributed to consumers in binary form. We return to this in the background content about ABI . In C++, the same function name can have several different signatures; for example template < class T > int square ( T num ) { return num * num ; } allows us to call square with all kinds of integer, floats, etc. The compiler will keep track of which flavor of the function has been used in the program and generate the respective symbols for each one of them. In order to distinguish these symbols, they get names that look like gibberish, but really simply bake the types of their input arguments into the identifier, so that we can ensure the right symbol gets called when we actually execute the function. For more details about the most widespread convention about this, see here . Linkers When building an executable or a library, any code that references functions from third-party libraries needs to be resolved to the respective symbols, and those need to be found and copied into the executable, or alternatively, loaded at runtime. The tool for this job is called the linker, and it's a hopefully invisible task, at least, until things break (symbols not found, etc.). For an in-depth introduction to linkers see here . Note that the field has evolved a lot since then, and new-generation linkers have appeared (see e.g. mold ), but the basic operations have remained essentially unchanged. One crucial aspect about the way things have historically grown is that there is no name-spacing of these symbols in any way. All symbols are fully global, and this brings a lot of constraints. The linker will search for a given symbol within different paths on the system, in order, but this is obviously very fragile, in case symbols appear in several libraries on the linker path. Key take-aways Functions are compiled into symbols. Symbol names are 1:1 with function names in C, mangled according to their signature in C++. These symbols share a global name space. Symbols are picked by the linker in order of precedence on the path. Shared vs. static libraries From a high-level point of view, libraries are collections of symbols, and can come in two different flavors: static and dynamic. In very crude terms, static libraries are only useful at compile time (i.e. compiling an app X against a static library libfoo will copy the symbols from libfoo required by X into the final artifact), whereas for dynamic libraries, the symbols will only be referenced , and then loaded from the dynamic library at runtime. As a rough overview: Static libfoo Dynamic libfoo Compiling app X Copies required symbols from libfoo into final artifact for X References required symbols from libfoo Running app X - Needs to load required symbols from libfoo Storage footprints O(#libs) for libraries using a given symbol Symbol only saved once Binary Interface Self-contained Sensitive to ABI breaks The trade-offs involved here are very painful. Either we duplicate symbols for every artifact that uses them (and the bloat can be extreme, e.g. statically linking the MSVC runtime 1 can increase storage / memory footprint by 100MB), or we face the constraints of subjecting us to ABI stability, and finding the right symbols at runtime. For a further deep-dive into how Microsoft evolves its UCRT (Universal C Runtime), see here . In general, large parts of the wider computing ecosystem have found the duplication inherent in static libraries unacceptable, and would rather deal with the constraints of ABI stability. For example, standard functions like printf (or standard mathematical functions, or ...) are used so pervasively that copying those symbols into every produced binary would be extremely wasteful. It's worth noting that on Windows, symbols in shared libraries work quite differently than on unix. Due to various reasons, symbols on Windows have to be explicitly marked __declspec(dllexport) & __declspec(dllimport) in the source with a macro that switches appropriately. This is quite an intrusive change for libraries aiming to be cross-platform, and the reason that several libraries developed primarily on unix do not support shared builds on Windows. Even using workarounds like CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS still doesn't cover all necessary symbols (e.g. global static data members). Using the latter may also have a performance impact, as it keeps the compiler from inlining calls to symbols that have been exported. Key take-aways Static libraries are only useful at compile-time; they cause duplication of symbols, but are much less susceptible to the intricacies of ABIs. Shared libraries need to be available both at compilation as well as at runtime, solve the symbol duplication, but are extremely susceptible to ABI breaks. Due to the global symbol namespace, there can only be one version / build of a shared library per environment (unless there is explicit versioning for the symbols or libraries themselves, or for C++, explicitly different inline namespaces are used). Foreign-Function Interface (FFI) For any given task, a function may already exist in a library written in another language than the one at hand. In order to avoid rewriting (and maintaining) that functionality, it's desirable to have a way to call such functions from a \"foreign\" language. A common example are the BLAS/LAPACK routines, which are written in Fortran, but provide a C interface in addition to the Fortran one. In addition to the considerations above, this needs to ensure that the types between the language of the callee and the types of the language that the function is implemented in are transformed correctly. Transpilation Many Python projects do not deal with C/C++ directly, but use transpilers like Cython or Pythran that generate C resp. C++ code from (quasi-)Python source code. Aside from almost always being exposed to the NumPy C API & ABI, these modules compile into shared libraries themselves, with all the caveats mentioned above that this implies. However, few projects expose their cythonized functions as a C API, so there are generally fewer concerns about ABI stability in this scenario. Cross-compilation Many projects use public CI services, which might not offer some of the more exotic or emerging architectures a project wants to support. This means that publishing binary artifacts for such architectures can often only be achieved with cross-compilation, i.e. compiling on one architecture (e.g., x86-64), for another (e.g., aarch64). A recent example where this was necessary at scale was the introduction of a new processor architecture for Apple's M1 notebooks, for which (almost) no CI agents were available. Distributors of packages for linux-aarch64 , linux-ppc64le or windows-arm64 are often in similar situations. The difficulty in cross-compilation is that it needs further attention to and separation (both conceptually, as well as in metadata) of the different requirements for the build environment (e.g. the necessary build tools that are executed on the x86-64 agent), as well as the host environment (i.e. the libraries that need to match the target architecture). Additionally, many build procedures assume they can execute arbitrary code (e.g. code generation) on the same architecture as the host, which is not a given in this case and may need to be worked around. Performance optimization Compiler flags TODO: Optimization levels; inlining functions; problems with -Ofast For -Ofast , see in particular: https://github.com/conda-forge/conda-forge.github.io/issues/1824 Link-Time Optimization (LTO) In the search for speed, it's possible to do a whole-program analysis after everything has been compiled, and let the compiler identify which functions can be inlined. This is generally out of scope for Python packaging, because it is too involved. However, CPython release builds make use of it. Profile-Guided Optimization (PGO) If a program is instrumented (compiled with tracking capabilities) to profile common usage patterns, it is possible to optimize the layout of the final artifact to ensure that hot paths get preferred in branch prediction. This is generally out of scope for Python packaging, because it is too involved. However, CPython release builds make use of it. Binary Optimization and Layout Tool (BOLT) Further optimization of the produced binary artifacts can be achieved by arranging their layout to avoid cache misses under profiled behavior. The respective tool is based on LLVM and still under heavy development, and not suitable for all usecases. It is generally out of scope for Python packaging, because it is too involved. However, CPython added experimental support as of 3.12. This is an extreme example, but it is used in the wild, see e.g. protobuf . Note that Microsoft itself discourages this pattern, but still supports it: We strongly recommend against static linking of the Visual C++ libraries, for both performance and serviceability reasons, but we recognize that there are some use cases that require static libraries and we will continue to support the static libraries for those reasons. \u21a9","title":"Basic code compilation concepts"},{"location":"background/compilation_concepts/#basic-code-compilation-concepts","text":"What does this have to do with Python? Python as a glue language is essentially exposed to the sum of all problems that other languages have with their binary distribution, and this is a really vast field (even just for C/C++). Needless to say, adequately summarizing decades of work and developments that have lead us to where we are now is not easy. If you find errors or things to improve, please open an issue or PR! In order to get a computer to execute a given unit of work (say, application X calling a function f from a previously compiled library libfoo ), a lot of preconditions have to be met: The function needs to have been compiled into instructions that the computer understands; a symbol . The symbol for f needs to be named (resp. \"mangled\") in a consistent manner between the compilation of the current code (for X ), resp. the compilation of the library ( libfoo , which contains the symbol for f ). The symbol for f needs to have be discoverable from within the currently running process; assuming libfoo is available on the machine where we are compiling, this is ensured by the linker . Variables passed to the function need to be loaded into the right CPU registers; this is highly dependent on the calling convention of a given CPU, or rather, CPU family. The code (in X ) calling a given symbol (e.g. for f ) needs to be excruciatingly compatible with the actual implementation of that symbol (in libfoo ). It's not useful for the average programmer to consider this level of detail when trying to get work done, but it is unfortunately unavoidable when considering the realities of packaging and distributing software as pre-compiled binary artifacts.","title":"Basic code compilation concepts"},{"location":"background/compilation_concepts/#symbols-mangling-and-linkers","text":"","title":"Symbols, mangling and linkers"},{"location":"background/compilation_concepts/#symbols","text":"For any given function f you might have written in a language that needs to be compiled (C, C++, ...), you can consider a symbol as a translation of your function to a level that can actually be executed by your computer. So, for example, a simple square function int square ( int num ) { return num * num ; } will be translated to the following assembly on x86-64 hardware: square : push rbp mov rbp , rsp mov DWORD PTR [ rbp - 4 ] , edi mov eax , DWORD PTR [ rbp - 4 ] imul eax , eax pop rbp ret There is a lot of ceremony for reading in the argument into a register, setting up another register for the result, while the actual work is done in: imul eax , eax If you click the link above, you will see that this assembly looks completely different when compiled for another processor architecture (e.g. arm64, as used in Apple M1 and newer).","title":"Symbols"},{"location":"background/compilation_concepts/#symbol-name-mangling","text":"Note as well that, in C, the symbol square has exactly the same name as the function - there is a 1:1 relationship, or in other words, there is no \"mangling\" of the symbol name. This is because C has no concept of overloading functions (i.e. having different functions of the same name but different signatures). It also means that you can never change anything about a C symbol that has been distributed to consumers in binary form. We return to this in the background content about ABI . In C++, the same function name can have several different signatures; for example template < class T > int square ( T num ) { return num * num ; } allows us to call square with all kinds of integer, floats, etc. The compiler will keep track of which flavor of the function has been used in the program and generate the respective symbols for each one of them. In order to distinguish these symbols, they get names that look like gibberish, but really simply bake the types of their input arguments into the identifier, so that we can ensure the right symbol gets called when we actually execute the function. For more details about the most widespread convention about this, see here .","title":"Symbol name mangling"},{"location":"background/compilation_concepts/#linkers","text":"When building an executable or a library, any code that references functions from third-party libraries needs to be resolved to the respective symbols, and those need to be found and copied into the executable, or alternatively, loaded at runtime. The tool for this job is called the linker, and it's a hopefully invisible task, at least, until things break (symbols not found, etc.). For an in-depth introduction to linkers see here . Note that the field has evolved a lot since then, and new-generation linkers have appeared (see e.g. mold ), but the basic operations have remained essentially unchanged. One crucial aspect about the way things have historically grown is that there is no name-spacing of these symbols in any way. All symbols are fully global, and this brings a lot of constraints. The linker will search for a given symbol within different paths on the system, in order, but this is obviously very fragile, in case symbols appear in several libraries on the linker path.","title":"Linkers"},{"location":"background/compilation_concepts/#key-take-aways","text":"Functions are compiled into symbols. Symbol names are 1:1 with function names in C, mangled according to their signature in C++. These symbols share a global name space. Symbols are picked by the linker in order of precedence on the path.","title":"Key take-aways"},{"location":"background/compilation_concepts/#shared-vs-static-libraries","text":"From a high-level point of view, libraries are collections of symbols, and can come in two different flavors: static and dynamic. In very crude terms, static libraries are only useful at compile time (i.e. compiling an app X against a static library libfoo will copy the symbols from libfoo required by X into the final artifact), whereas for dynamic libraries, the symbols will only be referenced , and then loaded from the dynamic library at runtime. As a rough overview: Static libfoo Dynamic libfoo Compiling app X Copies required symbols from libfoo into final artifact for X References required symbols from libfoo Running app X - Needs to load required symbols from libfoo Storage footprints O(#libs) for libraries using a given symbol Symbol only saved once Binary Interface Self-contained Sensitive to ABI breaks The trade-offs involved here are very painful. Either we duplicate symbols for every artifact that uses them (and the bloat can be extreme, e.g. statically linking the MSVC runtime 1 can increase storage / memory footprint by 100MB), or we face the constraints of subjecting us to ABI stability, and finding the right symbols at runtime. For a further deep-dive into how Microsoft evolves its UCRT (Universal C Runtime), see here . In general, large parts of the wider computing ecosystem have found the duplication inherent in static libraries unacceptable, and would rather deal with the constraints of ABI stability. For example, standard functions like printf (or standard mathematical functions, or ...) are used so pervasively that copying those symbols into every produced binary would be extremely wasteful. It's worth noting that on Windows, symbols in shared libraries work quite differently than on unix. Due to various reasons, symbols on Windows have to be explicitly marked __declspec(dllexport) & __declspec(dllimport) in the source with a macro that switches appropriately. This is quite an intrusive change for libraries aiming to be cross-platform, and the reason that several libraries developed primarily on unix do not support shared builds on Windows. Even using workarounds like CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS still doesn't cover all necessary symbols (e.g. global static data members). Using the latter may also have a performance impact, as it keeps the compiler from inlining calls to symbols that have been exported.","title":"Shared vs. static libraries"},{"location":"background/compilation_concepts/#key-take-aways_1","text":"Static libraries are only useful at compile-time; they cause duplication of symbols, but are much less susceptible to the intricacies of ABIs. Shared libraries need to be available both at compilation as well as at runtime, solve the symbol duplication, but are extremely susceptible to ABI breaks. Due to the global symbol namespace, there can only be one version / build of a shared library per environment (unless there is explicit versioning for the symbols or libraries themselves, or for C++, explicitly different inline namespaces are used).","title":"Key take-aways"},{"location":"background/compilation_concepts/#foreign-function-interface-ffi","text":"For any given task, a function may already exist in a library written in another language than the one at hand. In order to avoid rewriting (and maintaining) that functionality, it's desirable to have a way to call such functions from a \"foreign\" language. A common example are the BLAS/LAPACK routines, which are written in Fortran, but provide a C interface in addition to the Fortran one. In addition to the considerations above, this needs to ensure that the types between the language of the callee and the types of the language that the function is implemented in are transformed correctly.","title":"Foreign-Function Interface (FFI)"},{"location":"background/compilation_concepts/#transpilation","text":"Many Python projects do not deal with C/C++ directly, but use transpilers like Cython or Pythran that generate C resp. C++ code from (quasi-)Python source code. Aside from almost always being exposed to the NumPy C API & ABI, these modules compile into shared libraries themselves, with all the caveats mentioned above that this implies. However, few projects expose their cythonized functions as a C API, so there are generally fewer concerns about ABI stability in this scenario.","title":"Transpilation"},{"location":"background/compilation_concepts/#cross-compilation","text":"Many projects use public CI services, which might not offer some of the more exotic or emerging architectures a project wants to support. This means that publishing binary artifacts for such architectures can often only be achieved with cross-compilation, i.e. compiling on one architecture (e.g., x86-64), for another (e.g., aarch64). A recent example where this was necessary at scale was the introduction of a new processor architecture for Apple's M1 notebooks, for which (almost) no CI agents were available. Distributors of packages for linux-aarch64 , linux-ppc64le or windows-arm64 are often in similar situations. The difficulty in cross-compilation is that it needs further attention to and separation (both conceptually, as well as in metadata) of the different requirements for the build environment (e.g. the necessary build tools that are executed on the x86-64 agent), as well as the host environment (i.e. the libraries that need to match the target architecture). Additionally, many build procedures assume they can execute arbitrary code (e.g. code generation) on the same architecture as the host, which is not a given in this case and may need to be worked around.","title":"Cross-compilation"},{"location":"background/compilation_concepts/#performance-optimization","text":"","title":"Performance optimization"},{"location":"background/compilation_concepts/#compiler-flags","text":"TODO: Optimization levels; inlining functions; problems with -Ofast For -Ofast , see in particular: https://github.com/conda-forge/conda-forge.github.io/issues/1824","title":"Compiler flags"},{"location":"background/compilation_concepts/#link-time-optimization-lto","text":"In the search for speed, it's possible to do a whole-program analysis after everything has been compiled, and let the compiler identify which functions can be inlined. This is generally out of scope for Python packaging, because it is too involved. However, CPython release builds make use of it.","title":"Link-Time Optimization (LTO)"},{"location":"background/compilation_concepts/#profile-guided-optimization-pgo","text":"If a program is instrumented (compiled with tracking capabilities) to profile common usage patterns, it is possible to optimize the layout of the final artifact to ensure that hot paths get preferred in branch prediction. This is generally out of scope for Python packaging, because it is too involved. However, CPython release builds make use of it.","title":"Profile-Guided Optimization (PGO)"},{"location":"background/compilation_concepts/#binary-optimization-and-layout-tool-bolt","text":"Further optimization of the produced binary artifacts can be achieved by arranging their layout to avoid cache misses under profiled behavior. The respective tool is based on LLVM and still under heavy development, and not suitable for all usecases. It is generally out of scope for Python packaging, because it is too involved. However, CPython added experimental support as of 3.12. This is an extreme example, but it is used in the wild, see e.g. protobuf . Note that Microsoft itself discourages this pattern, but still supports it: We strongly recommend against static linking of the Visual C++ libraries, for both performance and serviceability reasons, but we recognize that there are some use cases that require static libraries and we will continue to support the static libraries for those reasons. \u21a9","title":"Binary Optimization and Layout Tool (BOLT)"},{"location":"key-issues/abi/","text":"Depending on packages for which an ABI matters When a library exposes an API in a compiled language, any other library or package that uses that API has to concern themselves with the ABI (Application Binary Interface) as soon as the API provider and API consumer are distributed separately. As a rule, it's best for ABI compatibility if the different packages are built with the same compiler toolchain, command-line flags and other configuration options. If that is not possible or guaranteed, one has to start thinking carefully about whether the two build environments will result in packages that are ABI-compatible. There are a lot of things with an ABI that a Python library maintainer may have to care about: The C standard library (the libc flavor) The C++ standard library ( libstdc++ old vs. new string ABI, libc++ , MSVC runtime, etc.) Fortran (gfortran vs. g77 ABI) CPython Python packages with C APIs: (NumPy, SciPy) Python packages with C++ APIs (PyTorch, Apache Arrow - see complex C++ dependencies ) Common non-Python dependencies for scientific computing BLAS, LAPACK, OpenMP , MPI 1 (see mpich.org/abi ). As an example of an ABI issue that many Python developers will encounter: a debug build of CPython is not ABI-compatible with a release build, hence if one wants to use a debug Python, one has to rebuild every package that uses the Python C API. Current state A package manager has to know about ABI, either implicitly or explicitly. In the case of PyPI, it's implicit. There are conventions that anyone publishing a wheel should adhere to. For example, on Windows and macOS, use compilers and flags compatible with those used to produce the Python installers published on python.org . And on Linux, what the various manylinux PEPs say (more complex, so best to use the manylinux -provided Docker images). Other package managers are more explicit about managing ABI, to varying degrees. They also have the advantage of being able to enforce using a consistent compiler toolchain. Inspecting the dependency tree of a SciPy install (which uses the Python and NumPy C APIs/ABIs) will show this: PyPI conda-forge Spack Arch Linux Homebrew $ pipdeptree -p scipy scipy == 1 .9.3 - numpy [ required: > = 1 .18.5,< 1 .26.0, installed: 1 .23.5 ] $ # Note: output edited to remove duplicate packages and python's dependencies $ mamba repoquery depends scipy --tree scipy [ 1 .9.3 ] \u251c\u2500 libgfortran-ng [ 12 .1.0 ] \u2502 \u2514\u2500 libgfortran5 [ 12 .1.0 ] \u251c\u2500 libgcc-ng [ 12 .1.0 ] \u2502 \u251c\u2500 _libgcc_mutex [ 0 .1 ] \u2502 \u2514\u2500 _openmp_mutex [ 4 .5 ] \u2502 \u2514\u2500 llvm-openmp [ 14 .0.4 ] \u2502 \u2514\u2500 libzlib [ 1 .2.13 ] \u251c\u2500 libstdcxx-ng [ 12 .1.0 ] \u251c\u2500 python_abi [ 3 .10 ] \u2502 \u2514\u2500 python [ 3 .10.8 ] \u2514\u2500 numpy [ 1 .23.3 ] \u251c\u2500 libblas [ 3 .9.0 ] \u2502 \u2514\u2500 libopenblas [ 0 .3.21 ] \u251c\u2500 libcblas [ 3 .9.0 ] \u251c\u2500 liblapack [ 3 .9.0 ] \u251c\u2500 libblas already visited \u251c\u2500 liblapack already visited $ # Note: output edited to remove build-only dependencies $ ./spack spec py-scipy%gcc py-scipy@1.9.2%gcc@12.2.0 arch = linux-endeavourosrolling-skylake_avx512 ^openblas@0.3.21%gcc@12.2.0~bignuma~consistent_fpcsr+fortran~ilp64+locking+pic+shared symbol_suffix = none threads = none arch = linux-endeavourosrolling-skylake_avx512 ^python@3.9.13%gcc@12.2.0+bz2+ctypes+dbm~debug+libxml2+lzma~nis~optimizations+pic+pyexpat+pythoncmd+readline+shared+sqlite3+ssl~tix~tkinter~ucs4+uuid+zlib patches = 0d98e93,4c24573,f2fd060 arch = linux-endeavourosrolling-skylake_avx512 ^py-numpy@1.23.3%gcc@12.2.0+blas+lapack patches = 873745d arch = linux-endeavourosrolling-skylake_avx512 $ # Note: output edited to remove duplicates and some transitive dependencies $ pactree python-scipy python-scipy \u2514\u2500python-numpy \u251c\u2500cblas \u2502 \u2514\u2500openblas provides blas \u2502 \u2514\u2500gcc-libs \u2502 \u2514\u2500glibc> = 2 .27 \u251c\u2500lapack \u2502 \u2514\u2500openblas provides blas \u2514\u2500python \u251c\u2500bzip2 \u2502 \u251c\u2500glibc \u251c\u2500expat \u251c\u2500gdbm \u251c\u2500libffi \u251c\u2500libnsl \u251c\u2500libxcrypt \u251c\u2500openssl \u2514\u2500zlib % # Note: output edited to remove duplicate packages % brew deps --tree --installed scipy scipy \u251c\u2500\u2500 gcc \u2502 \u251c\u2500\u2500 gmp \u2502 \u251c\u2500\u2500 isl \u2502 \u251c\u2500\u2500 libmpc \u2502 \u251c\u2500\u2500 mpfr \u2502 \u2514\u2500\u2500 zstd \u2502 \u251c\u2500\u2500 lz4 \u2502 \u2514\u2500\u2500 xz \u251c\u2500\u2500 numpy \u2502 \u2514\u2500\u2500 openblas \u2502 \u2514\u2500\u2500 gcc \u251c\u2500\u2500 openblas \u2502 \u2514\u2500\u2500 gcc \u2514\u2500\u2500 python@3.11 \u251c\u2500\u2500 mpdecimal \u251c\u2500\u2500 openssl@1.1 \u251c\u2500\u2500 sqlite \u2514\u2500\u2500 xz For example, we see python_abi and libgcc_mutex in the conda-forge output; detailed compiler, BLAS interface, CPU architecture and library info in the Spack output; and glibc version info in the Arch Linux output. In general, the more dependencies and more languages one uses, the more ABI compatibility starts to matter. If the Python C API is the only thing used by a package, the rules are relatively straightforward: rebuild for every minor Python version (unless one can use the limited API , then it's even easier), with compatible compilers. It already gets a lot harder as soon as one uses the NumPy C API. Which many packages do, often via Cython. While the NumPy ABI is much more stable than the CPython one (NumPy hasn't broken compatibility in a nontrivial way in over a decade), one still has to understand the rules for building against NumPy: Example: Using the NumPy C API NumPy has a C API, which Cython, SciPy, and many other packages use. That ABI is forward but not backward compatible, meaning if you use it then you must build your wheels against the lowest NumPy version that you expect your users to use. So if you build against version 1.X.Y then the runtime requirement you get is numpy>=1.X.Y . That lowest version may depend on Python version and platform. There is no good way to express a dependency like that in pyproject.toml , or even to keep track of what the lowest version should be. Because of that, the oldest-supported-numpy metapackage is being used by projects that depend on numpy as an imperfect hack to obtain the correct numpy== build requirement pins per Python version and platform. It can be used like so: [build-system] requires = [ \"oldest-supported-numpy\" , # your other build requirements here ] Each time NumPy adds support for a new Python version or platform, oldest-supported-numpy is updated so that each user of the NumPy C API does not have to do that. Despite this complexity, the solution is imperfect - oldest-supported-numpy is unable to communicate back the correct numpy>= runtime requirement, so those requirements are generally incorrect for all packages using NumPy (all wheels will have a generic numpy>=1.Y.Z for the lowest value of 1.Y.Z across all wheels). See Adding a dependency on NumPy for more details. The most difficult cases arise with dependencies outside of Python/PyPI (also see native dependencies ). At that point one can pick up arbitrary libraries during the build phase of a package, and with complex dependencies the ABI of that dependency may be unknown or have to be introspected. This paper on SciPy's Cython API for BLAS and LAPACK explains how and why SciPy added a Cython API to hide the ABI variations across BLAS/LAPACK implementations from other Python packages. These three \"levels of complexity\" cases are illustrated by these diagrams of package stacks: pure Python only pure & C API-using complete stack Package stack: CPython and pure Python packages only Package stack: CPython, pure Python & Python C API-using packages Package stack: all the way down to libc C++ APIs are a problem onto themselves. When a library is written in C++ but wants to expose an API with a stable ABI, it often exposes a C rather than a C++ API with extern \"C\" . Keeping ABI stability has a large cost though, it takes a lot of extra work and may prevent changes to internals of a library. Hence it's not always done. For example, PyTorch has a large C++ API and does not promise any ABI stability: Example: Using the PyTorch C++ API PyTorch is mostly written in C++, and exposes a C++ API in addition to its Python API. C++ ABI stability is a tricky topic - it's implementation-defined, and because name mangling can change between compilers (or even compiler versions), mixing binaries built with different compilers isn't possible. PyTorch does not attempt to provide a stable ABI; even bug fix releases aren't guaranteed to be compatible. As a result, all packages using PyTorch's C API must use a runtime requirement like: [project] dependencies = [ \"torch == X.Y.Z\" , # X.Y.Z is the version of PyTorch used to build against ] This requirement also implies synchronized releases . If PyTorch does a new release, the team ensures that simultaneous releases of torchvision , torchaudio , torchtext , torchdata and other dependent packages are made. This level of coordination doesn't scale well though, and therefore may limit the use of the PyTorch C++ API - especially by community open source projects whose authors may not have the bandwidth to keep up with releases. The project description for pypi/torchvision is a good example to illustrate the tight version coupling. Problems A problem for PyPI and wheels is that there is little coordination on compiler toolchains, ABI changes, etc. So maintainers of every package are on their own trying to figure this out. Other package managers don't have this problem - they build everything with a consistent toolchain (as much as possible at least), either including libc or on top of the libc provided by the operating system. See no build farm and PyPI's author-led social model for more details. CPython breaks its ABI every minor release (unless one's needs are limited, then there is the limited API). This has a huge cost: packages have to build wheels for every minor Python release. NumPy is effectively forced to do the opposite: NumPy needs to not break its ABI in order to avoid exploding the build matrix of every project using its C API. This has a large opportunity cost; there are a lot of improvements and cleanups that NumPy cannot implement. If PyPI either had a build farm or could be disregarded for binaries with ABI stability requirements, NumPy would have broken ABI compatibility multiple times by now 2 , with positive impacts on maintainability, performance, and functionality. Python packaging build tooling has no understanding of ABI constraints, making it hard to add runtime version constraints to a wheel that are correct (and tighter than those in the corresponding sdist). Manylinux versions still using the old C++ ABI, while most C++ projects want to use the new ABI. manylinux_2_28 may change this finally, but isn't yet in use. See complex C++ dependencies for more details on this topic. History TODO Relevant resources \"Circumventing the Linker: using SciPy\u2019s BLAS and LAPACK within Cython\" , Ian Henriksen, SciPy 2015. \" archspec : A library for detecting, labeling, and reasoning about microarchitectures\" , Culpo et al. (2020). \"C++ binary compatibility between Visual Studio versions\" GCC docs on libstdc++ dual ABI and on ABI Policy and Guidelines . NumPy docs for downstream package authors . PEP 384 - Defining a Stable ABI . PEP 652 - Maintaining the Stable ABI . Python docs on C API Stability . \"Let\u2019s get rid of the stable ABI, but keep the limited API\" thread on Discourse (2022). trailofbits/abi3audit . \"ABI compatibility in Python: How hard could it be?\" (2022). HPy - A better C API for Python . Conda-forge FAQ entry on \"How to handle breaking of a package due to ABI incompatibility?\" . Potential solutions or mitigations Expressing constraints imposed by an ABI on versions at runtime can be done with something like conda-forge's pin_compatible and run_exports features . See meson-python#29 for ideas about implementing that in a build backend. HPy and its hybrid and universal ABIs may be the way forward for improvements in constraints that the CPython ABI imposes, as well as enabling use of alternative interpreters. Coordinated rebuilds that are needed because of == runtime constraints would be a lot easier with a build farm (as discussed here ). h5py is an example of a project that support MPI but, despite regular user requests, does not ship MPI-enabled wheels. MPI has multiple implementations, and does not have a stable ABI (see, e.g., mpi-issues#654 for (lack of) MPI ABI stability). \u21a9 Actually, NumPy did break its ABI once, in the 1.4.0 release (2010). This resulted in some mayhem and very long and painful discussions. The ABI was unbroken, and hasn't been touched since. \u21a9","title":"Depending on packages for which an ABI matters"},{"location":"key-issues/abi/#depending-on-packages-for-which-an-abi-matters","text":"When a library exposes an API in a compiled language, any other library or package that uses that API has to concern themselves with the ABI (Application Binary Interface) as soon as the API provider and API consumer are distributed separately. As a rule, it's best for ABI compatibility if the different packages are built with the same compiler toolchain, command-line flags and other configuration options. If that is not possible or guaranteed, one has to start thinking carefully about whether the two build environments will result in packages that are ABI-compatible. There are a lot of things with an ABI that a Python library maintainer may have to care about: The C standard library (the libc flavor) The C++ standard library ( libstdc++ old vs. new string ABI, libc++ , MSVC runtime, etc.) Fortran (gfortran vs. g77 ABI) CPython Python packages with C APIs: (NumPy, SciPy) Python packages with C++ APIs (PyTorch, Apache Arrow - see complex C++ dependencies ) Common non-Python dependencies for scientific computing BLAS, LAPACK, OpenMP , MPI 1 (see mpich.org/abi ). As an example of an ABI issue that many Python developers will encounter: a debug build of CPython is not ABI-compatible with a release build, hence if one wants to use a debug Python, one has to rebuild every package that uses the Python C API.","title":"Depending on packages for which an ABI matters"},{"location":"key-issues/abi/#current-state","text":"A package manager has to know about ABI, either implicitly or explicitly. In the case of PyPI, it's implicit. There are conventions that anyone publishing a wheel should adhere to. For example, on Windows and macOS, use compilers and flags compatible with those used to produce the Python installers published on python.org . And on Linux, what the various manylinux PEPs say (more complex, so best to use the manylinux -provided Docker images). Other package managers are more explicit about managing ABI, to varying degrees. They also have the advantage of being able to enforce using a consistent compiler toolchain. Inspecting the dependency tree of a SciPy install (which uses the Python and NumPy C APIs/ABIs) will show this: PyPI conda-forge Spack Arch Linux Homebrew $ pipdeptree -p scipy scipy == 1 .9.3 - numpy [ required: > = 1 .18.5,< 1 .26.0, installed: 1 .23.5 ] $ # Note: output edited to remove duplicate packages and python's dependencies $ mamba repoquery depends scipy --tree scipy [ 1 .9.3 ] \u251c\u2500 libgfortran-ng [ 12 .1.0 ] \u2502 \u2514\u2500 libgfortran5 [ 12 .1.0 ] \u251c\u2500 libgcc-ng [ 12 .1.0 ] \u2502 \u251c\u2500 _libgcc_mutex [ 0 .1 ] \u2502 \u2514\u2500 _openmp_mutex [ 4 .5 ] \u2502 \u2514\u2500 llvm-openmp [ 14 .0.4 ] \u2502 \u2514\u2500 libzlib [ 1 .2.13 ] \u251c\u2500 libstdcxx-ng [ 12 .1.0 ] \u251c\u2500 python_abi [ 3 .10 ] \u2502 \u2514\u2500 python [ 3 .10.8 ] \u2514\u2500 numpy [ 1 .23.3 ] \u251c\u2500 libblas [ 3 .9.0 ] \u2502 \u2514\u2500 libopenblas [ 0 .3.21 ] \u251c\u2500 libcblas [ 3 .9.0 ] \u251c\u2500 liblapack [ 3 .9.0 ] \u251c\u2500 libblas already visited \u251c\u2500 liblapack already visited $ # Note: output edited to remove build-only dependencies $ ./spack spec py-scipy%gcc py-scipy@1.9.2%gcc@12.2.0 arch = linux-endeavourosrolling-skylake_avx512 ^openblas@0.3.21%gcc@12.2.0~bignuma~consistent_fpcsr+fortran~ilp64+locking+pic+shared symbol_suffix = none threads = none arch = linux-endeavourosrolling-skylake_avx512 ^python@3.9.13%gcc@12.2.0+bz2+ctypes+dbm~debug+libxml2+lzma~nis~optimizations+pic+pyexpat+pythoncmd+readline+shared+sqlite3+ssl~tix~tkinter~ucs4+uuid+zlib patches = 0d98e93,4c24573,f2fd060 arch = linux-endeavourosrolling-skylake_avx512 ^py-numpy@1.23.3%gcc@12.2.0+blas+lapack patches = 873745d arch = linux-endeavourosrolling-skylake_avx512 $ # Note: output edited to remove duplicates and some transitive dependencies $ pactree python-scipy python-scipy \u2514\u2500python-numpy \u251c\u2500cblas \u2502 \u2514\u2500openblas provides blas \u2502 \u2514\u2500gcc-libs \u2502 \u2514\u2500glibc> = 2 .27 \u251c\u2500lapack \u2502 \u2514\u2500openblas provides blas \u2514\u2500python \u251c\u2500bzip2 \u2502 \u251c\u2500glibc \u251c\u2500expat \u251c\u2500gdbm \u251c\u2500libffi \u251c\u2500libnsl \u251c\u2500libxcrypt \u251c\u2500openssl \u2514\u2500zlib % # Note: output edited to remove duplicate packages % brew deps --tree --installed scipy scipy \u251c\u2500\u2500 gcc \u2502 \u251c\u2500\u2500 gmp \u2502 \u251c\u2500\u2500 isl \u2502 \u251c\u2500\u2500 libmpc \u2502 \u251c\u2500\u2500 mpfr \u2502 \u2514\u2500\u2500 zstd \u2502 \u251c\u2500\u2500 lz4 \u2502 \u2514\u2500\u2500 xz \u251c\u2500\u2500 numpy \u2502 \u2514\u2500\u2500 openblas \u2502 \u2514\u2500\u2500 gcc \u251c\u2500\u2500 openblas \u2502 \u2514\u2500\u2500 gcc \u2514\u2500\u2500 python@3.11 \u251c\u2500\u2500 mpdecimal \u251c\u2500\u2500 openssl@1.1 \u251c\u2500\u2500 sqlite \u2514\u2500\u2500 xz For example, we see python_abi and libgcc_mutex in the conda-forge output; detailed compiler, BLAS interface, CPU architecture and library info in the Spack output; and glibc version info in the Arch Linux output. In general, the more dependencies and more languages one uses, the more ABI compatibility starts to matter. If the Python C API is the only thing used by a package, the rules are relatively straightforward: rebuild for every minor Python version (unless one can use the limited API , then it's even easier), with compatible compilers. It already gets a lot harder as soon as one uses the NumPy C API. Which many packages do, often via Cython. While the NumPy ABI is much more stable than the CPython one (NumPy hasn't broken compatibility in a nontrivial way in over a decade), one still has to understand the rules for building against NumPy: Example: Using the NumPy C API NumPy has a C API, which Cython, SciPy, and many other packages use. That ABI is forward but not backward compatible, meaning if you use it then you must build your wheels against the lowest NumPy version that you expect your users to use. So if you build against version 1.X.Y then the runtime requirement you get is numpy>=1.X.Y . That lowest version may depend on Python version and platform. There is no good way to express a dependency like that in pyproject.toml , or even to keep track of what the lowest version should be. Because of that, the oldest-supported-numpy metapackage is being used by projects that depend on numpy as an imperfect hack to obtain the correct numpy== build requirement pins per Python version and platform. It can be used like so: [build-system] requires = [ \"oldest-supported-numpy\" , # your other build requirements here ] Each time NumPy adds support for a new Python version or platform, oldest-supported-numpy is updated so that each user of the NumPy C API does not have to do that. Despite this complexity, the solution is imperfect - oldest-supported-numpy is unable to communicate back the correct numpy>= runtime requirement, so those requirements are generally incorrect for all packages using NumPy (all wheels will have a generic numpy>=1.Y.Z for the lowest value of 1.Y.Z across all wheels). See Adding a dependency on NumPy for more details. The most difficult cases arise with dependencies outside of Python/PyPI (also see native dependencies ). At that point one can pick up arbitrary libraries during the build phase of a package, and with complex dependencies the ABI of that dependency may be unknown or have to be introspected. This paper on SciPy's Cython API for BLAS and LAPACK explains how and why SciPy added a Cython API to hide the ABI variations across BLAS/LAPACK implementations from other Python packages. These three \"levels of complexity\" cases are illustrated by these diagrams of package stacks: pure Python only pure & C API-using complete stack Package stack: CPython and pure Python packages only Package stack: CPython, pure Python & Python C API-using packages Package stack: all the way down to libc C++ APIs are a problem onto themselves. When a library is written in C++ but wants to expose an API with a stable ABI, it often exposes a C rather than a C++ API with extern \"C\" . Keeping ABI stability has a large cost though, it takes a lot of extra work and may prevent changes to internals of a library. Hence it's not always done. For example, PyTorch has a large C++ API and does not promise any ABI stability: Example: Using the PyTorch C++ API PyTorch is mostly written in C++, and exposes a C++ API in addition to its Python API. C++ ABI stability is a tricky topic - it's implementation-defined, and because name mangling can change between compilers (or even compiler versions), mixing binaries built with different compilers isn't possible. PyTorch does not attempt to provide a stable ABI; even bug fix releases aren't guaranteed to be compatible. As a result, all packages using PyTorch's C API must use a runtime requirement like: [project] dependencies = [ \"torch == X.Y.Z\" , # X.Y.Z is the version of PyTorch used to build against ] This requirement also implies synchronized releases . If PyTorch does a new release, the team ensures that simultaneous releases of torchvision , torchaudio , torchtext , torchdata and other dependent packages are made. This level of coordination doesn't scale well though, and therefore may limit the use of the PyTorch C++ API - especially by community open source projects whose authors may not have the bandwidth to keep up with releases. The project description for pypi/torchvision is a good example to illustrate the tight version coupling.","title":"Current state"},{"location":"key-issues/abi/#problems","text":"A problem for PyPI and wheels is that there is little coordination on compiler toolchains, ABI changes, etc. So maintainers of every package are on their own trying to figure this out. Other package managers don't have this problem - they build everything with a consistent toolchain (as much as possible at least), either including libc or on top of the libc provided by the operating system. See no build farm and PyPI's author-led social model for more details. CPython breaks its ABI every minor release (unless one's needs are limited, then there is the limited API). This has a huge cost: packages have to build wheels for every minor Python release. NumPy is effectively forced to do the opposite: NumPy needs to not break its ABI in order to avoid exploding the build matrix of every project using its C API. This has a large opportunity cost; there are a lot of improvements and cleanups that NumPy cannot implement. If PyPI either had a build farm or could be disregarded for binaries with ABI stability requirements, NumPy would have broken ABI compatibility multiple times by now 2 , with positive impacts on maintainability, performance, and functionality. Python packaging build tooling has no understanding of ABI constraints, making it hard to add runtime version constraints to a wheel that are correct (and tighter than those in the corresponding sdist). Manylinux versions still using the old C++ ABI, while most C++ projects want to use the new ABI. manylinux_2_28 may change this finally, but isn't yet in use. See complex C++ dependencies for more details on this topic.","title":"Problems"},{"location":"key-issues/abi/#history","text":"TODO","title":"History"},{"location":"key-issues/abi/#relevant-resources","text":"\"Circumventing the Linker: using SciPy\u2019s BLAS and LAPACK within Cython\" , Ian Henriksen, SciPy 2015. \" archspec : A library for detecting, labeling, and reasoning about microarchitectures\" , Culpo et al. (2020). \"C++ binary compatibility between Visual Studio versions\" GCC docs on libstdc++ dual ABI and on ABI Policy and Guidelines . NumPy docs for downstream package authors . PEP 384 - Defining a Stable ABI . PEP 652 - Maintaining the Stable ABI . Python docs on C API Stability . \"Let\u2019s get rid of the stable ABI, but keep the limited API\" thread on Discourse (2022). trailofbits/abi3audit . \"ABI compatibility in Python: How hard could it be?\" (2022). HPy - A better C API for Python . Conda-forge FAQ entry on \"How to handle breaking of a package due to ABI incompatibility?\" .","title":"Relevant resources"},{"location":"key-issues/abi/#potential-solutions-or-mitigations","text":"Expressing constraints imposed by an ABI on versions at runtime can be done with something like conda-forge's pin_compatible and run_exports features . See meson-python#29 for ideas about implementing that in a build backend. HPy and its hybrid and universal ABIs may be the way forward for improvements in constraints that the CPython ABI imposes, as well as enabling use of alternative interpreters. Coordinated rebuilds that are needed because of == runtime constraints would be a lot easier with a build farm (as discussed here ). h5py is an example of a project that support MPI but, despite regular user requests, does not ship MPI-enabled wheels. MPI has multiple implementations, and does not have a stable ABI (see, e.g., mpi-issues#654 for (lack of) MPI ABI stability). \u21a9 Actually, NumPy did break its ABI once, in the 1.4.0 release (2010). This resulted in some mayhem and very long and painful discussions. The ABI was unbroken, and hasn't been touched since. \u21a9","title":"Potential solutions or mitigations"},{"location":"key-issues/gpus/","text":"Packaging projects with GPU code Modern Graphics Processing Units (GPUs) can be used, in addition to their original purpose (rendering graphics), for high-performance numerical computing. They are particularly important for deep learning, but also widely used for data science and traditional scientific computing and image processing application. GPUs from NVIDIA using the CUDA programming language are dominant in deep learning and scientific computing as of today. With both AMD and Intel releasing GPUs and other programming languages for them ( ROCm , SYCL , OpenCL ), the landscape may become more diverse in the future. In addtion, Google provides Tensor Processing Units access in Google Cloud Platform, and a host of startups are developing custom accelerator hardware for high-performance computing applications. Prominent projects which rely on GPUs and are either Python-only or widely used from Python include TensorFlow , PyTorch , CuPy , JAX , RAPIDS , MXNet , XGBoost , Numba , OpenCV , Horovod and PyMC . Packaging such projects for PyPI has been, and still is, quite challenging. Current state As of December 2022, PyPI and Python packaging tools are completely unaware of GPUs, and of CUDA. There is no way to mark a package as needing a GPU in sdist or wheel metadata, or as containing GPU-specific code (CUDA or otherwise). A GPU is hardware that may or may not be present in a machine that a Python package is being installed on - pip and other installers are unaware of this. If wheels contain CUDA code, they require CUDA Toolkit (a specific version of it at that) to be installed. Again, installers do not know this and there is no way to express this dependency. The same will be true for ROCm and other types of GPU hardware and languages. NVIDIA has made steps towards better support for CUDA on PyPI, through CUDA Python ( website , PyPI package ), however this is quite new and not used by other projects (nor does it target large projects like PyTorch and TensorFlow). For most of its own projects, it uses a Private PyPI Index - and that also includes rebuilds of TensorFlow and other packages. A single CUDA version supports a reasonable range of GPU architectures. New CUDA versions get released regularly, and - because they come with increased performance or new functionality - it may be necessary or desirable to build new wheels for that CUDA version. If only the supported CUDA version is different between two wheels, the wheel tags and filename will be identical. Hence it is not possible to upload more than one of those wheels under the same package name. To work around that, a project may either support only one CUDA version on PyPI, or create different packages. PyTorch and TensorFlow do the former, with TensorFlow supporting only a single CUDA version, and PyTorch providing more wheels for other CUDA versions and a CPU-only version in a separate wheelhouse (see pytorch.org/get-started ). CuPy provides a number of packages: cupy , cupy-cuda102 , cupy-cuda110 , cupy-cuda111 , cupy-cuda11x , cupy-rocm-4-3 , cupy-rocm-5-0 . Other projects do similar things - none of it works very well. GPU packages tend to result in very large wheels. This is true in particular for deep learning packages, because they link in cuDNN . For example, the most recent manylinux2014 wheels for TensorFlow are 588 MB ( 2.11.0 files ), and for PyTorch those are 890 MB ( 1.13.0 files ). The problems around and causes of GPU wheel sizes were discussed in depth in this Packaging thread on Discourse . So far we have only discussed individual projects containing GPU code. Those projects are the most fundamental libraries in larger stacks of packages (perhaps even whole ecosystems). Hence, other projects will want to declare a dependency on them. This is currently quite difficult, because of the implicit coupling through a shared CUDA version. If a project like PyTorch releases a new version and bumps the default CUDA version used in the torch wheels, then any downstream package which also contains CUDA code will break unless it has an exact == pin on the older torch version, and then releases a new version of its own for the new CUDA version. Such synchronized releases are hard to do. If there where a way to declare a dependency on CUDA version (e.g., through a metapackage on PyPI), that strong coupling between packages would not be necessary. Other package managers typically do have support for CUDA: Conda: provides all CUDA versions through a cudatoolkit conda-forge package and a virtual __cuda package , Spack: supports building with or without CUDA, and allows specifying supported GPU architectures: docs . CUDA itself can be specified as externally provided, and is recommended to be installed directly from NVIDIA: docs . ROCm is supported in a similar fashion , Ubuntu: provides one CUDA version per Ubuntu release: nvidia-cuda-toolkit package , Arch Linux: provides one CUDA version: cuda package . Those package managers typically also provide CUDA-related development tools, and build all the most popular deep learning and numerical computing packages for the CUDA version they ship. Problems The problems around GPU packages include: User-friendliness: Installs depend on a specific CUDA or ROCm version, and pip does not know about this. Hence installs may succeed, followed by errors at runtime, CUDA or ROCm must be installed through another package manager or a direct download from the vendor. And the other package manager upgrading CUDA or ROCm may silently break the installed Python package, Wheels may have to come from a separate wheelhouse, requiring install commands like python -m pip install torch --extra-index-url https://download.pytorch.org/whl/cu116 which are easy to get wrong, The very large download sizes are problematic for users on slow network connections or plans with a maximum amount of bandwidth usage for a given month ( pip potentially downloading multiple wheels because of backtracking in the resolver is extra painful here). Maintainer effort: Keeping wheel sizes below either the 1 GB hard limit or the current PyPI file size or total project size limits can be a lot of work (or even impossible), Hosting your own wheelhouse to support multiple CUDA or ROCm versions is a lot of work, Depending on another GPU package is difficult, and likely requires a == pin, A dependendency on CUDA, ROCm, or a specific version of them cannot be expressed in metadata, hence maintaining build environments is more error-prone than it has to be. For PyPI itself: The large amount of space and bandwidth consumed by GPU packages. pypi.org/stats shows under \"top projects by total package size\" that many of the largest package are GPU ones, and that together they consume a significant fraction (estimated at ~20% for the ones listed in the top 100) of the total size for all of PyPI. History Support for GPUs and CUDA has been discussed on and off on distutils-sig and the Packaging Discourse: Environment markers for GPU/CUDA availibility thread on distutils-sig (2018), The next manylinux specification thread on Discourse (2019), with a specific comment about presence/absence of GPU hardware and CUDA libraries being out of scope, What to do about GPUs? (and the built distributions that support them) on a Packaging thread on Discourse (2021), None of the suggested ideas in those threads gained traction, mostly due to a combination of the complexity of the problem, difficulty of implementing support in packaging tools, and lack of people to work on a solution. Relevant resources TODO Potential solutions or mitigations Potential solutions on the PyPI side include: add specific wheel tags or metadata for the most popular libraries, make an environment marker or selector package approach work, improve interoperability with other package managers, in order to be able to declare a dependency on a CUDA or ROCm version as externally provided,","title":"Packaging projects with GPU code"},{"location":"key-issues/gpus/#packaging-projects-with-gpu-code","text":"Modern Graphics Processing Units (GPUs) can be used, in addition to their original purpose (rendering graphics), for high-performance numerical computing. They are particularly important for deep learning, but also widely used for data science and traditional scientific computing and image processing application. GPUs from NVIDIA using the CUDA programming language are dominant in deep learning and scientific computing as of today. With both AMD and Intel releasing GPUs and other programming languages for them ( ROCm , SYCL , OpenCL ), the landscape may become more diverse in the future. In addtion, Google provides Tensor Processing Units access in Google Cloud Platform, and a host of startups are developing custom accelerator hardware for high-performance computing applications. Prominent projects which rely on GPUs and are either Python-only or widely used from Python include TensorFlow , PyTorch , CuPy , JAX , RAPIDS , MXNet , XGBoost , Numba , OpenCV , Horovod and PyMC . Packaging such projects for PyPI has been, and still is, quite challenging.","title":"Packaging projects with GPU code"},{"location":"key-issues/gpus/#current-state","text":"As of December 2022, PyPI and Python packaging tools are completely unaware of GPUs, and of CUDA. There is no way to mark a package as needing a GPU in sdist or wheel metadata, or as containing GPU-specific code (CUDA or otherwise). A GPU is hardware that may or may not be present in a machine that a Python package is being installed on - pip and other installers are unaware of this. If wheels contain CUDA code, they require CUDA Toolkit (a specific version of it at that) to be installed. Again, installers do not know this and there is no way to express this dependency. The same will be true for ROCm and other types of GPU hardware and languages. NVIDIA has made steps towards better support for CUDA on PyPI, through CUDA Python ( website , PyPI package ), however this is quite new and not used by other projects (nor does it target large projects like PyTorch and TensorFlow). For most of its own projects, it uses a Private PyPI Index - and that also includes rebuilds of TensorFlow and other packages. A single CUDA version supports a reasonable range of GPU architectures. New CUDA versions get released regularly, and - because they come with increased performance or new functionality - it may be necessary or desirable to build new wheels for that CUDA version. If only the supported CUDA version is different between two wheels, the wheel tags and filename will be identical. Hence it is not possible to upload more than one of those wheels under the same package name. To work around that, a project may either support only one CUDA version on PyPI, or create different packages. PyTorch and TensorFlow do the former, with TensorFlow supporting only a single CUDA version, and PyTorch providing more wheels for other CUDA versions and a CPU-only version in a separate wheelhouse (see pytorch.org/get-started ). CuPy provides a number of packages: cupy , cupy-cuda102 , cupy-cuda110 , cupy-cuda111 , cupy-cuda11x , cupy-rocm-4-3 , cupy-rocm-5-0 . Other projects do similar things - none of it works very well. GPU packages tend to result in very large wheels. This is true in particular for deep learning packages, because they link in cuDNN . For example, the most recent manylinux2014 wheels for TensorFlow are 588 MB ( 2.11.0 files ), and for PyTorch those are 890 MB ( 1.13.0 files ). The problems around and causes of GPU wheel sizes were discussed in depth in this Packaging thread on Discourse . So far we have only discussed individual projects containing GPU code. Those projects are the most fundamental libraries in larger stacks of packages (perhaps even whole ecosystems). Hence, other projects will want to declare a dependency on them. This is currently quite difficult, because of the implicit coupling through a shared CUDA version. If a project like PyTorch releases a new version and bumps the default CUDA version used in the torch wheels, then any downstream package which also contains CUDA code will break unless it has an exact == pin on the older torch version, and then releases a new version of its own for the new CUDA version. Such synchronized releases are hard to do. If there where a way to declare a dependency on CUDA version (e.g., through a metapackage on PyPI), that strong coupling between packages would not be necessary. Other package managers typically do have support for CUDA: Conda: provides all CUDA versions through a cudatoolkit conda-forge package and a virtual __cuda package , Spack: supports building with or without CUDA, and allows specifying supported GPU architectures: docs . CUDA itself can be specified as externally provided, and is recommended to be installed directly from NVIDIA: docs . ROCm is supported in a similar fashion , Ubuntu: provides one CUDA version per Ubuntu release: nvidia-cuda-toolkit package , Arch Linux: provides one CUDA version: cuda package . Those package managers typically also provide CUDA-related development tools, and build all the most popular deep learning and numerical computing packages for the CUDA version they ship.","title":"Current state"},{"location":"key-issues/gpus/#problems","text":"The problems around GPU packages include: User-friendliness: Installs depend on a specific CUDA or ROCm version, and pip does not know about this. Hence installs may succeed, followed by errors at runtime, CUDA or ROCm must be installed through another package manager or a direct download from the vendor. And the other package manager upgrading CUDA or ROCm may silently break the installed Python package, Wheels may have to come from a separate wheelhouse, requiring install commands like python -m pip install torch --extra-index-url https://download.pytorch.org/whl/cu116 which are easy to get wrong, The very large download sizes are problematic for users on slow network connections or plans with a maximum amount of bandwidth usage for a given month ( pip potentially downloading multiple wheels because of backtracking in the resolver is extra painful here). Maintainer effort: Keeping wheel sizes below either the 1 GB hard limit or the current PyPI file size or total project size limits can be a lot of work (or even impossible), Hosting your own wheelhouse to support multiple CUDA or ROCm versions is a lot of work, Depending on another GPU package is difficult, and likely requires a == pin, A dependendency on CUDA, ROCm, or a specific version of them cannot be expressed in metadata, hence maintaining build environments is more error-prone than it has to be. For PyPI itself: The large amount of space and bandwidth consumed by GPU packages. pypi.org/stats shows under \"top projects by total package size\" that many of the largest package are GPU ones, and that together they consume a significant fraction (estimated at ~20% for the ones listed in the top 100) of the total size for all of PyPI.","title":"Problems"},{"location":"key-issues/gpus/#history","text":"Support for GPUs and CUDA has been discussed on and off on distutils-sig and the Packaging Discourse: Environment markers for GPU/CUDA availibility thread on distutils-sig (2018), The next manylinux specification thread on Discourse (2019), with a specific comment about presence/absence of GPU hardware and CUDA libraries being out of scope, What to do about GPUs? (and the built distributions that support them) on a Packaging thread on Discourse (2021), None of the suggested ideas in those threads gained traction, mostly due to a combination of the complexity of the problem, difficulty of implementing support in packaging tools, and lack of people to work on a solution.","title":"History"},{"location":"key-issues/gpus/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"key-issues/gpus/#potential-solutions-or-mitigations","text":"Potential solutions on the PyPI side include: add specific wheel tags or metadata for the most popular libraries, make an environment marker or selector package approach work, improve interoperability with other package managers, in order to be able to declare a dependency on a CUDA or ROCm version as externally provided,","title":"Potential solutions or mitigations"},{"location":"key-issues/pypi_metadata_handling/","text":"Metadata handling on PyPI Metadata for sdists and wheels on PyPI is contained in those sdists and wheels themselves, in a form as prescribed by the Core metadata specifications . That metadata format is typically produced by build backends. The source of that metadata is information that package authors maintain within their project. It historically lived in setup.py , and may still live there today - however it now preferably should be specified in pyproject.toml (see PEP 621 - Storing project metadata in pyproject.toml ). Specifying that metadata is relatively straightforward, and is mostly in decent shape (even though there are some loose ends 1 ) - with the exception of native dependencies, as discussed on this key issue page . Can package metadata be queried for PyPI? From this Discourse thread (Dec 2022): dist-info-metadata is available in the simple API for wheels (and sdists if they follow PEP 643). But PyPI doesn\u2019t expose that data (yet). The PyPI JSON API includes metadata, but it is unreliable as it is at the project release level, so it doesn\u2019t take into account the possibility of different wheels having different metadata. But it\u2019s the nearest you can get right now. See also the Project Metadata Table in BigQuery . Similarly, metadata about the package can be queried. For example, Libraries.io provides an easy to use overview of projects including reverse dependencies. And there are a number of ways of querying package download statistics, see for example pypistats.org for a UI with quick numbers, and the Analyzing PyPI package downloads page of the Python Packaging User Guide which lists a number of tools to enable more in-depth analysis. While specifying metadata for a package is relatively straightforward in most cases 2 , the same cannot be said for the workflows around dealing with problems in metadata. Current state By design, metadata in artifacts on PyPI is (a) immutable and (b) contained within the artifact itself rather than available separately. Both of those design aspects can be problematic. Impact of immutable metadata When a package author discovers an issue with their release or with a particular artifact in a release, there are good solutions. In the former case, a release can be yanked . In the latter case, a new artifact with a higher build number can be uploaded. This blog post by Brett Cannon (2021) explains what to do in those cases in detail. Where immutability becomes a problem is when the issue is in the metadata - in particular, when a build or runtime dependency changes something that breaks a package. It's a problem because: Doing a new release for complex packages with native code is very expensive. It may be days of full-time work (builds may have to run on multiple CI systems, and those configs tend to degrade pretty quickly for older releases), and therefore it may not be feasible to do on short notice. Having affected users deal with the situation by themselves is also very expensive. The most popular packages have hundreds of thousands or even millions of users, so even if only 1% of users 3 are affected by a problem with a dependency, that is still an unacceptably large amount of work (and probably lots of complaints on the issue tracker). (2) is often advocated for by Python packaging experts, in particular by having users apply post-hoc constraints through a constraints file . (2) is the worst solution though in the case of large-scale breakage, both because of the large numbers of users that each need to take action and because users are, more often than not, not developers . Instead, they're (data) scientists, engineers, business analysts and so on. They don't want to, and shouldn't need to, understand things like constraints files. If the metadata needs patching, the far better solution would be to patch them on PyPI. And this is not possible, because artifacts are immutable. Depending on the situation, these are the most common ways that an issue with a dependency gets dealt with: Bite the bullet and do a new release of the affected package, Convince the authors of the dependency that broke things to unbreak them again (e.g., undo removal of a deprecated or private API), Or even temporarily yank the dependency that broke things. Because this kind of situation happens frequently, it may also be a good idea to add upper bounds on version specifiers of dependencies. No one likes upper bounds, because they result in incompatibilities and make dependency resolution more difficult. A lot of effort has been spent discussing the issues with upper bounds (e.g., see this blog post and this Discourse thread ); package authors are caught between a rock and a hard place though - the problem is immutability of metadata. On upper bounds - Matthias Bussonnier I echo many sentiments here that 1) I hate that some projects have to put an upper bound [in their metadata], but 2) they do it because removing the upper bound is worse. Experience with other package managers that are able to patch metadata shows that this is a much nicer experience. For example, conda-forge uses \"repo data patching\" , while Spack and Nix build from source (with a binary cache providing many common build configs) and the Spack repo and Nixpgs repo contain metadata for all packages and can therefore be updated via a PR. As a result, upper bounds that are present on PyPI can typically be left out safely in these package managers; applying new constraints later is cheap. Managing the necessary upper bounds itself is an exercise that may have to be repeated for each release, and takes time and effort. See for example this part of the SciPy developer guide . Metadata contained within artifacts Each wheel has its own metadata contained within the artifact. It can be different metadata than that for the sdist for which it came - and this is more likely to happen for packages with native code. Wheels for packages with native code also tend to be larger -from tens of MBs for the likes of NumPy, SciPy, Pandas and PyArrow to many hundreds of MBs for deep learning packages. Downloading such large packages in order to access the metadata is clearly suboptimal. Especially if that metadata then shows a conflict and Pip has to backtrack. Also during debugging install issues this is a significant problem - when one wants to go through a number of wheels and compare differences with for example the METADATA or RECORD files, the current process is slow and bandwidth-intensive. The solution seems obvious: make metadata separately accessible from wheels. Luckily, the solution for this is currently in progress: PEP 568 - Serve Distribution Metadata in the Simple Repository API PEP 568 (accepted) proposes to make the metadata file in the .dist-info directory of a wheel separately available. This should solve the problems identified in this section. Support is already implemented in pip . Implementation in PyPI is still pending, see warehouse#8254 . There are also issues around packages who don't yet use static metadata in pyproject.toml , and reliable metadata for sdists being only relatively recently available ( PEP 643, Nov 2020 ). With dynamic metadata or setup.py usage, sdists have to be built in order to obtain the metadata. This is a general packaging issue however, not specific to packages with native code, and not nearly as much of a problem as the other issues discussed higher up. See, e.g., pip#1884 and this thread for details. Problems The most important problem is the need to add upper bounds on version specifications of dependencies. History Older analyses of PyPI dependencies include this one from Olivier Girardot (2013), this one from Martin Thoma (2015), and this one from Kevin Gullikson (2016). The Requires-Python upper limits Discourse thread (2021) went into detail on issues around specifying the upper bound of supported Python versions. TODO: add more history Relevant resources TODO Potential solutions or mitigations Making metadata editable. This would require a PEP and be a large effort. The impact of issues with build dependencies, and hence the need to add upper bounds, would be much less if Pip did not install from sdist by default, as discussed in Unsuspecting users getting failing from-source builds . Fix issues in installers. E.g., Poetry and PDM should not propagate upper bounds the way they currently do, as discussed in this thread . Pip also needs to continue reducing the amount of excessive backtracking, and use the separate metadata available soon with PEP 568 to reduce the impact of that backtracking. See Possible ways to reduce backtracking in the Pip docs for current mitigation options available to users. Example of a not unimportant loose end: PEP 639 - Improving License Clarity with Better Package Metadata is still in Draft status and not supported by PyPI as of Dec 2022. \u21a9 See Example: Using the NumPy C API on this page for a case where getting the right metadata into a wheel is very difficult. \u21a9 The number of users on platforms without wheel support on PyPI is on the order of 1%, and that is a set of users that is frequently affected by issues with build dependencies. \u21a9","title":"Metadata handling on PyPI"},{"location":"key-issues/pypi_metadata_handling/#metadata-handling-on-pypi","text":"Metadata for sdists and wheels on PyPI is contained in those sdists and wheels themselves, in a form as prescribed by the Core metadata specifications . That metadata format is typically produced by build backends. The source of that metadata is information that package authors maintain within their project. It historically lived in setup.py , and may still live there today - however it now preferably should be specified in pyproject.toml (see PEP 621 - Storing project metadata in pyproject.toml ). Specifying that metadata is relatively straightforward, and is mostly in decent shape (even though there are some loose ends 1 ) - with the exception of native dependencies, as discussed on this key issue page . Can package metadata be queried for PyPI? From this Discourse thread (Dec 2022): dist-info-metadata is available in the simple API for wheels (and sdists if they follow PEP 643). But PyPI doesn\u2019t expose that data (yet). The PyPI JSON API includes metadata, but it is unreliable as it is at the project release level, so it doesn\u2019t take into account the possibility of different wheels having different metadata. But it\u2019s the nearest you can get right now. See also the Project Metadata Table in BigQuery . Similarly, metadata about the package can be queried. For example, Libraries.io provides an easy to use overview of projects including reverse dependencies. And there are a number of ways of querying package download statistics, see for example pypistats.org for a UI with quick numbers, and the Analyzing PyPI package downloads page of the Python Packaging User Guide which lists a number of tools to enable more in-depth analysis. While specifying metadata for a package is relatively straightforward in most cases 2 , the same cannot be said for the workflows around dealing with problems in metadata.","title":"Metadata handling on PyPI"},{"location":"key-issues/pypi_metadata_handling/#current-state","text":"By design, metadata in artifacts on PyPI is (a) immutable and (b) contained within the artifact itself rather than available separately. Both of those design aspects can be problematic.","title":"Current state"},{"location":"key-issues/pypi_metadata_handling/#impact-of-immutable-metadata","text":"When a package author discovers an issue with their release or with a particular artifact in a release, there are good solutions. In the former case, a release can be yanked . In the latter case, a new artifact with a higher build number can be uploaded. This blog post by Brett Cannon (2021) explains what to do in those cases in detail. Where immutability becomes a problem is when the issue is in the metadata - in particular, when a build or runtime dependency changes something that breaks a package. It's a problem because: Doing a new release for complex packages with native code is very expensive. It may be days of full-time work (builds may have to run on multiple CI systems, and those configs tend to degrade pretty quickly for older releases), and therefore it may not be feasible to do on short notice. Having affected users deal with the situation by themselves is also very expensive. The most popular packages have hundreds of thousands or even millions of users, so even if only 1% of users 3 are affected by a problem with a dependency, that is still an unacceptably large amount of work (and probably lots of complaints on the issue tracker). (2) is often advocated for by Python packaging experts, in particular by having users apply post-hoc constraints through a constraints file . (2) is the worst solution though in the case of large-scale breakage, both because of the large numbers of users that each need to take action and because users are, more often than not, not developers . Instead, they're (data) scientists, engineers, business analysts and so on. They don't want to, and shouldn't need to, understand things like constraints files. If the metadata needs patching, the far better solution would be to patch them on PyPI. And this is not possible, because artifacts are immutable. Depending on the situation, these are the most common ways that an issue with a dependency gets dealt with: Bite the bullet and do a new release of the affected package, Convince the authors of the dependency that broke things to unbreak them again (e.g., undo removal of a deprecated or private API), Or even temporarily yank the dependency that broke things. Because this kind of situation happens frequently, it may also be a good idea to add upper bounds on version specifiers of dependencies. No one likes upper bounds, because they result in incompatibilities and make dependency resolution more difficult. A lot of effort has been spent discussing the issues with upper bounds (e.g., see this blog post and this Discourse thread ); package authors are caught between a rock and a hard place though - the problem is immutability of metadata. On upper bounds - Matthias Bussonnier I echo many sentiments here that 1) I hate that some projects have to put an upper bound [in their metadata], but 2) they do it because removing the upper bound is worse. Experience with other package managers that are able to patch metadata shows that this is a much nicer experience. For example, conda-forge uses \"repo data patching\" , while Spack and Nix build from source (with a binary cache providing many common build configs) and the Spack repo and Nixpgs repo contain metadata for all packages and can therefore be updated via a PR. As a result, upper bounds that are present on PyPI can typically be left out safely in these package managers; applying new constraints later is cheap. Managing the necessary upper bounds itself is an exercise that may have to be repeated for each release, and takes time and effort. See for example this part of the SciPy developer guide .","title":"Impact of immutable metadata"},{"location":"key-issues/pypi_metadata_handling/#metadata-contained-within-artifacts","text":"Each wheel has its own metadata contained within the artifact. It can be different metadata than that for the sdist for which it came - and this is more likely to happen for packages with native code. Wheels for packages with native code also tend to be larger -from tens of MBs for the likes of NumPy, SciPy, Pandas and PyArrow to many hundreds of MBs for deep learning packages. Downloading such large packages in order to access the metadata is clearly suboptimal. Especially if that metadata then shows a conflict and Pip has to backtrack. Also during debugging install issues this is a significant problem - when one wants to go through a number of wheels and compare differences with for example the METADATA or RECORD files, the current process is slow and bandwidth-intensive. The solution seems obvious: make metadata separately accessible from wheels. Luckily, the solution for this is currently in progress: PEP 568 - Serve Distribution Metadata in the Simple Repository API PEP 568 (accepted) proposes to make the metadata file in the .dist-info directory of a wheel separately available. This should solve the problems identified in this section. Support is already implemented in pip . Implementation in PyPI is still pending, see warehouse#8254 . There are also issues around packages who don't yet use static metadata in pyproject.toml , and reliable metadata for sdists being only relatively recently available ( PEP 643, Nov 2020 ). With dynamic metadata or setup.py usage, sdists have to be built in order to obtain the metadata. This is a general packaging issue however, not specific to packages with native code, and not nearly as much of a problem as the other issues discussed higher up. See, e.g., pip#1884 and this thread for details.","title":"Metadata contained within artifacts"},{"location":"key-issues/pypi_metadata_handling/#problems","text":"The most important problem is the need to add upper bounds on version specifications of dependencies.","title":"Problems"},{"location":"key-issues/pypi_metadata_handling/#history","text":"Older analyses of PyPI dependencies include this one from Olivier Girardot (2013), this one from Martin Thoma (2015), and this one from Kevin Gullikson (2016). The Requires-Python upper limits Discourse thread (2021) went into detail on issues around specifying the upper bound of supported Python versions. TODO: add more history","title":"History"},{"location":"key-issues/pypi_metadata_handling/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"key-issues/pypi_metadata_handling/#potential-solutions-or-mitigations","text":"Making metadata editable. This would require a PEP and be a large effort. The impact of issues with build dependencies, and hence the need to add upper bounds, would be much less if Pip did not install from sdist by default, as discussed in Unsuspecting users getting failing from-source builds . Fix issues in installers. E.g., Poetry and PDM should not propagate upper bounds the way they currently do, as discussed in this thread . Pip also needs to continue reducing the amount of excessive backtracking, and use the separate metadata available soon with PEP 568 to reduce the impact of that backtracking. See Possible ways to reduce backtracking in the Pip docs for current mitigation options available to users. Example of a not unimportant loose end: PEP 639 - Improving License Clarity with Better Package Metadata is still in Draft status and not supported by PyPI as of Dec 2022. \u21a9 See Example: Using the NumPy C API on this page for a case where getting the right metadata into a wheel is very difficult. \u21a9 The number of users on platforms without wheel support on PyPI is on the order of 1%, and that is a set of users that is frequently affected by issues with build dependencies. \u21a9","title":"Potential solutions or mitigations"},{"location":"key-issues/simd_support/","text":"Distributing a package containing SIMD code Single Instruction, Multiple Data (SIMD) instructions are instructions that are CPU-specific, and can yield significant performance gains compared to regular, portable C/C++ code. Each popular modern CPU architecture has its own SIMD instruction sets. Using SIMD instructions in a Python package is quite difficult, because there is no way to specify, in either metadata or wheel tags, what CPU features are needed on the target machine in order to use a given wheel. What does code containing SIMD instructions look like? This code fragment shows how to use a single SSE2 instruction on an x86-64 CPU. It defines a mul function which multiplies two double precision floating point vectors: #include <immintrin.h> __m128d mul ( __m128d a , __m128d b ) { return _mm_mul_pd ( a , b ); } If the CPU supports the instruction and the code gets compiled with the needed compiler flag ( -msse2 ), the mul function will work and will be faster than using regular multiplication in C/C++. As a more real-world example, here is a code fragment from a sin function for 32-bit float data from NumPy code: #if NPY_SIMD_F32 && NPY_SIMD_FMA3 if ( is_mem_overlap ( src , steps [ 0 ], dst , steps [ 1 ], len ) || ! npyv_loadable_stride_f32 ( ssrc ) || ! npyv_storable_stride_f32 ( sdst ) ) { for (; len > 0 ; -- len , src += ssrc , dst += sdst ) { simd_sincos_f32 ( src , 1 , dst , 1 , 1 , SIMD_COMPUTE_SIN ); } } else { simd_sincos_f32 ( src , ssrc , dst , sdst , len , SIMD_COMPUTE_SIN ); } #else for (; len > 0 ; -- len , src += ssrc , dst += sdst ) { const float src0 = * src ; * dst = npy_sinf ( src0 ); } #endif How important is use of SIMD code? Code with SIMD instructions is typically a lot more difficult to read and maintain than regular C or C++ code. The speedups can be large however, so the implementation effort and the maintenance burden may be worth it. For basic and heavily used functionality like element-wise math functions ( abs , sqrt , multiply , etc.), typical gains are in the 1.x - 10 range, and sometimes even >10 ). Here are a few benchmark results for: OpenCV color conversion functionality, ~25x faster on ARM CPUs with NEON: opencv#19883 NumPy's absolute , reciprocal , sqrt , square functions, for SSE/AVX2 (x86-64), NEON (aarch64/arm64), and VSX (ppc64le): numpy#16247 PyTorch softmax , min and max 3x-4x faster for bfloat16 with AVX2/AVX512 on x86-64: pytorch#55202 , and up to 2x-10x with uint8 for + , >> , min : pytorch#89284 Using AVX2 instead of SSE in SciPy's 2-D Fourier transforms: scipy#16984 It is safe to say that performance gains that large, for single-threaded execution in libraries that are so widely used, are extremely important. Current state As of December 2022, there is no support on PyPI, in the wheel spec, or in any widely used packaging tool for binaries containing SIMD instructions. Nor a plan to implement such support. The only relevant metadata is the \"platform compatibility tag\" in a wheel name, first defined in PEP 425 and now maintained under PyPA specifications in the Python Packaging User Guide . A platform tag defines a CPU family, for example x86_64 for 64-bit x86 CPUs and aarch64 for 64-bit ARM CPUs. Projects that want to distribute wheels containing SIMD instructions have effectively three choices: Make a single choice of SIMD instructions to include. Build extension modules with multiple SIMD flavors inside, detect CPU capabilities at runtime, and then dynamically choose the optimal binary code. Create separate packages on PyPI with a different package name but the same import name, and containing wheels with newer instructions. Then let users manually install those alternative packages. Choice (1) implicitly defines what CPUs are supported by their package. Given that unsupported instructions result in very obscure errors, this means targeting SIMD instruction sets that are at least 10 years old (sometimes more). Choice (2) results in improved performance, because newer SIMD instructions can be used. However, this comes at the cost of a large amount of code complexity and larger wheel sizes. In practice, only the largest and most widely used projects are able to make choice (2). And they indeed do so - TensorFlow, PyTorch, OpenCV, NumPy, and MXNet all have their own machinery and methods to work with SIMD instructions. There are not many examples of choice (3). The ones that do exist, e.g. Pillow-SIMD and Intel(R) Extension for scikit-learn , tend to be forks by a third party rather than packages created by the original development team. Distributing binaries with SIMD instructions is not something many other packaging systems have an answer for. The exception is Spack, which has builtin capabilities through archspec for installing optimized binaries. This will even be surfaced in its resolver; individual package entries will contain a tag like -skylake_avx512 (microarchitecture + highest supported instruction set). The archspec paper is worth reading for a thorough discussion of the design aspects of integrating support for SIMD instructions, and dealing with CPU compatibility in a packaging system in a more granular fashion. Problems Writing SIMD instructions is a specialized skill, however it can be effective to do so in only a few performance hotspots of the code. So it is often worthwhile, if it weren't for the problems around distributing wheels on PyPI. To illustrate how prohibitively expensive in terms of developer time the dynamic dispatch solution is: NumPy only gained support for it in 2020, and SciPy still does not have it (it chooses SSE3 instructions, first released in 2005, as the most recent instructions that are allowed to be used on x86). Less sophisticated methods employed in the wild are compiling the project twice (e.g. once with SSE3 and once with AVX2), and defaulting to importing the latter while falling back to the former. Obviously this doubles binary size. The \"distribute separate wheels under a different package name (choice 3 above) is so user-unfriendly, and also fairly labor-intensive, that we cannot think of a single open source project that does this on PyPI. The \"choose a baseline and compile only for that\" (choice 1 above) is the easiest choice that still allows using some SIMD instructions - and the difference between some (e.g. up to SSE3) and none at all can still be very large in terms of performance gain. However, this still leaves some users with old or nonstandard CPUs out in the cold, and it forces package authors to come up with a method for choosing that maximum feature set. The rule of thumb that NumPy and SciPy came up with is: if the number of users with incompatible CPUs stays below 0.5% (as determined by some publicly available data from browser and gaming vendors), then it's okay to use a particular feature. This is not ideal, but tends to lead to few complaints in practice. History Distributing packages containing SIMD code on PyPI came up a number of times on the distutils-sig mailing list, as well as more recently on Discourse: Handling the binary dependency management problem thread on distutils-sig (2013) Warning about potential problems for wheels thread on distutils-sig (2015) Status update on the NumPy & SciPy vs SSE problem? thread on distutils-sig (2016) Archspec: a library for labeling optimized binaries on a Packaging thread on Discourse (2020) Idea: selector packages on a Packaging thread on Discourse (2020) Even before wheels existed, NumPy and SciPy were already distributing .exe Windows installers for three SIMD flavors (no SIMD, up to SSE2, and up to SSE3, see for example pypi.org/project/numpy/1.5.1/#files . Relevant resources Links to key issues, forum discussions, PEPs, blog posts, etc. NEP 38 - Using SIMD optimization instructions for performance (see also the \"Related Work\" section in that NEP for more relevant projects) archspec - a library for detecting, labeling, and reasoning about microarchitectures: GitHub repo , paper pytorch/cpuinfo - CPU INFOrmation library: GitHub repo xsimd - C++ wrappers for SIMD intrinsics and parallelized, optimized mathematical functions: GitHub repo Spack's docs on support for specific microarchitectures Potential solutions or mitigations There are few potential solutions on the Python packaging side that look promising: New wheel tags for specific microarchitectures is a blunt instrument, and there are too many microarchitectures to consider for this to work well, Using a library like archspec by packaging tools is very likely too complicated, The selector packages idea seemed promising at first, but seems to have fallen out of favor now. The most likely path forward to improve the current situation is to make it easier to share and reuse infrastructure for CPU feature detection and runtime dispatch. With archspec and pytorch/cpuinfo there are two solid libraries available for feature detection. The NumPy and Meson projects are planning to collaborate to make the \"multiple compilation for different CPU capabilities\" part available as a build system feature. If the runtime dispatch part could be implemented as a standalone, vendorable component, perhaps it will become easier for other projects to go this route.","title":"Distributing a package containing SIMD code"},{"location":"key-issues/simd_support/#distributing-a-package-containing-simd-code","text":"Single Instruction, Multiple Data (SIMD) instructions are instructions that are CPU-specific, and can yield significant performance gains compared to regular, portable C/C++ code. Each popular modern CPU architecture has its own SIMD instruction sets. Using SIMD instructions in a Python package is quite difficult, because there is no way to specify, in either metadata or wheel tags, what CPU features are needed on the target machine in order to use a given wheel. What does code containing SIMD instructions look like? This code fragment shows how to use a single SSE2 instruction on an x86-64 CPU. It defines a mul function which multiplies two double precision floating point vectors: #include <immintrin.h> __m128d mul ( __m128d a , __m128d b ) { return _mm_mul_pd ( a , b ); } If the CPU supports the instruction and the code gets compiled with the needed compiler flag ( -msse2 ), the mul function will work and will be faster than using regular multiplication in C/C++. As a more real-world example, here is a code fragment from a sin function for 32-bit float data from NumPy code: #if NPY_SIMD_F32 && NPY_SIMD_FMA3 if ( is_mem_overlap ( src , steps [ 0 ], dst , steps [ 1 ], len ) || ! npyv_loadable_stride_f32 ( ssrc ) || ! npyv_storable_stride_f32 ( sdst ) ) { for (; len > 0 ; -- len , src += ssrc , dst += sdst ) { simd_sincos_f32 ( src , 1 , dst , 1 , 1 , SIMD_COMPUTE_SIN ); } } else { simd_sincos_f32 ( src , ssrc , dst , sdst , len , SIMD_COMPUTE_SIN ); } #else for (; len > 0 ; -- len , src += ssrc , dst += sdst ) { const float src0 = * src ; * dst = npy_sinf ( src0 ); } #endif How important is use of SIMD code? Code with SIMD instructions is typically a lot more difficult to read and maintain than regular C or C++ code. The speedups can be large however, so the implementation effort and the maintenance burden may be worth it. For basic and heavily used functionality like element-wise math functions ( abs , sqrt , multiply , etc.), typical gains are in the 1.x - 10 range, and sometimes even >10 ). Here are a few benchmark results for: OpenCV color conversion functionality, ~25x faster on ARM CPUs with NEON: opencv#19883 NumPy's absolute , reciprocal , sqrt , square functions, for SSE/AVX2 (x86-64), NEON (aarch64/arm64), and VSX (ppc64le): numpy#16247 PyTorch softmax , min and max 3x-4x faster for bfloat16 with AVX2/AVX512 on x86-64: pytorch#55202 , and up to 2x-10x with uint8 for + , >> , min : pytorch#89284 Using AVX2 instead of SSE in SciPy's 2-D Fourier transforms: scipy#16984 It is safe to say that performance gains that large, for single-threaded execution in libraries that are so widely used, are extremely important.","title":"Distributing a package containing SIMD code"},{"location":"key-issues/simd_support/#current-state","text":"As of December 2022, there is no support on PyPI, in the wheel spec, or in any widely used packaging tool for binaries containing SIMD instructions. Nor a plan to implement such support. The only relevant metadata is the \"platform compatibility tag\" in a wheel name, first defined in PEP 425 and now maintained under PyPA specifications in the Python Packaging User Guide . A platform tag defines a CPU family, for example x86_64 for 64-bit x86 CPUs and aarch64 for 64-bit ARM CPUs. Projects that want to distribute wheels containing SIMD instructions have effectively three choices: Make a single choice of SIMD instructions to include. Build extension modules with multiple SIMD flavors inside, detect CPU capabilities at runtime, and then dynamically choose the optimal binary code. Create separate packages on PyPI with a different package name but the same import name, and containing wheels with newer instructions. Then let users manually install those alternative packages. Choice (1) implicitly defines what CPUs are supported by their package. Given that unsupported instructions result in very obscure errors, this means targeting SIMD instruction sets that are at least 10 years old (sometimes more). Choice (2) results in improved performance, because newer SIMD instructions can be used. However, this comes at the cost of a large amount of code complexity and larger wheel sizes. In practice, only the largest and most widely used projects are able to make choice (2). And they indeed do so - TensorFlow, PyTorch, OpenCV, NumPy, and MXNet all have their own machinery and methods to work with SIMD instructions. There are not many examples of choice (3). The ones that do exist, e.g. Pillow-SIMD and Intel(R) Extension for scikit-learn , tend to be forks by a third party rather than packages created by the original development team. Distributing binaries with SIMD instructions is not something many other packaging systems have an answer for. The exception is Spack, which has builtin capabilities through archspec for installing optimized binaries. This will even be surfaced in its resolver; individual package entries will contain a tag like -skylake_avx512 (microarchitecture + highest supported instruction set). The archspec paper is worth reading for a thorough discussion of the design aspects of integrating support for SIMD instructions, and dealing with CPU compatibility in a packaging system in a more granular fashion.","title":"Current state"},{"location":"key-issues/simd_support/#problems","text":"Writing SIMD instructions is a specialized skill, however it can be effective to do so in only a few performance hotspots of the code. So it is often worthwhile, if it weren't for the problems around distributing wheels on PyPI. To illustrate how prohibitively expensive in terms of developer time the dynamic dispatch solution is: NumPy only gained support for it in 2020, and SciPy still does not have it (it chooses SSE3 instructions, first released in 2005, as the most recent instructions that are allowed to be used on x86). Less sophisticated methods employed in the wild are compiling the project twice (e.g. once with SSE3 and once with AVX2), and defaulting to importing the latter while falling back to the former. Obviously this doubles binary size. The \"distribute separate wheels under a different package name (choice 3 above) is so user-unfriendly, and also fairly labor-intensive, that we cannot think of a single open source project that does this on PyPI. The \"choose a baseline and compile only for that\" (choice 1 above) is the easiest choice that still allows using some SIMD instructions - and the difference between some (e.g. up to SSE3) and none at all can still be very large in terms of performance gain. However, this still leaves some users with old or nonstandard CPUs out in the cold, and it forces package authors to come up with a method for choosing that maximum feature set. The rule of thumb that NumPy and SciPy came up with is: if the number of users with incompatible CPUs stays below 0.5% (as determined by some publicly available data from browser and gaming vendors), then it's okay to use a particular feature. This is not ideal, but tends to lead to few complaints in practice.","title":"Problems"},{"location":"key-issues/simd_support/#history","text":"Distributing packages containing SIMD code on PyPI came up a number of times on the distutils-sig mailing list, as well as more recently on Discourse: Handling the binary dependency management problem thread on distutils-sig (2013) Warning about potential problems for wheels thread on distutils-sig (2015) Status update on the NumPy & SciPy vs SSE problem? thread on distutils-sig (2016) Archspec: a library for labeling optimized binaries on a Packaging thread on Discourse (2020) Idea: selector packages on a Packaging thread on Discourse (2020) Even before wheels existed, NumPy and SciPy were already distributing .exe Windows installers for three SIMD flavors (no SIMD, up to SSE2, and up to SSE3, see for example pypi.org/project/numpy/1.5.1/#files .","title":"History"},{"location":"key-issues/simd_support/#relevant-resources","text":"Links to key issues, forum discussions, PEPs, blog posts, etc. NEP 38 - Using SIMD optimization instructions for performance (see also the \"Related Work\" section in that NEP for more relevant projects) archspec - a library for detecting, labeling, and reasoning about microarchitectures: GitHub repo , paper pytorch/cpuinfo - CPU INFOrmation library: GitHub repo xsimd - C++ wrappers for SIMD intrinsics and parallelized, optimized mathematical functions: GitHub repo Spack's docs on support for specific microarchitectures","title":"Relevant resources"},{"location":"key-issues/simd_support/#potential-solutions-or-mitigations","text":"There are few potential solutions on the Python packaging side that look promising: New wheel tags for specific microarchitectures is a blunt instrument, and there are too many microarchitectures to consider for this to work well, Using a library like archspec by packaging tools is very likely too complicated, The selector packages idea seemed promising at first, but seems to have fallen out of favor now. The most likely path forward to improve the current situation is to make it easier to share and reuse infrastructure for CPU feature detection and runtime dispatch. With archspec and pytorch/cpuinfo there are two solid libraries available for feature detection. The NumPy and Meson projects are planning to collaborate to make the \"multiple compilation for different CPU capabilities\" part available as a build system feature. If the runtime dispatch part could be implemented as a standalone, vendorable component, perhaps it will become easier for other projects to go this route.","title":"Potential solutions or mitigations"},{"location":"key-issues/unexpected_fromsource_builds/","text":"Unsuspecting users getting failing from-source builds Current state When a project makes a release, it typically uploads one sdist (a source distribution) and multiple wheels (binary installers). Wheels are primarily meant to make the installation experience better and faster. For projects which contain code that needs to be compiled (e.g., C/C++/Cython), installing from the sdist is challenging. The sdist metadata does not even allow expression the required dependencies (e.g., a compiler - see native dependencies ). Hence installing from an sdist often goes wrong. Why does a user get an sdist when they didn't expect one? This can happen in quite a few circumstances: Shortly after the release of a new Python version, most projects will not yet have wheels for that new Python version uploaded to PyPI. So when a user installs the \"latest and greatest\" Python and type pip install somepackage , they are likely to see pip try to install the sdist of the highest version of somepackage . In case new hardware becomes available. A recent example is macOS arm64: it took many scientific projects over a year before they were able to build arm64 or universal2 wheels and upload them to PyPI. All users which used a native arm64 Python were getting builds from sdist. Users who use an old pip version (e.g. the pip shipped with their distro on a typical HPC cluster) which does not have support for recent manylinux versions may see pip try to install from an sdist even though there are (say) manylinux2014 wheels for the package. Installs from sdist may happen if a project tags a release but there's a problem on a particular platform for which it normally uploads wheels. Especially if this is a less popular platform (e.g., ppc64le ) the release manager may just go ahead with the release, and aim to upload the missing wheels later. If a project is uploading a new version and the person doing the release isn't careful to upload all wheels first and the sdist afterwards, then users on any platform may see installers try to install from the sdist. This mistake is easy to make, and can lead to a lot of failed installs quickly if a package is popular (e.g., the download rate for numpy is ~2000/minute). Many years ago, users were expecting to build from source. Today, in 2022, the scientific Python ecosystem has tens of millions of users. The vast majority of those users are not expecting, and are often unable, to build from source when they type a command like pip install scikit-learn . Problems There clearly are a lot of issues due to installing from an sdist when the user did not intend to do that. For users: Failed installs, often with confusing error messages and after a possibly time-consuming build step. Installs that appear to succeed but have issues that show up at runtime as a result of building against incorrect or mismatching libraries. This ranges from import errors due to missing symbols in shared libraries to segfaults and silently wrong numerical results. For pip maintainers: a lot of bug reports they have to deal with because the user thinks pip is the cause rather than the package they tried to install. For maintainers of projects with compiled code: A lot of bug reports that are very time-consuming to address. Issues are often not reproducible, and bug reports typically do not contain enough information to be able to understand if the problem is user error or an actual bug in the project. A lot of time spent carefully managing build dependencies and their versions in pyproject.toml (see, e.g., the oldest-supported-numpy metapackage ) which has as its only purpose to serve as a build dependency which pins numpy to the correct version (typically the lowest version for which there are wheels on PyPI) per platform and Python version/interpreter. Being forced to support platforms that are already end of life, because the project does not have a good way of dropping support for older manylinux flavors (see, e.g., numpy#19192 ). History TODO Relevant resources TODO Potential solutions or mitigations Do not upload sdists to PyPI at all. This is the approach taken by many of the projects with the most complex builds - for example PyTorch, TensorFlow, MXNet, jaxlib, and Ray. It is necessary to then delete every single sdist for any version of the package from PyPI - if there was a single sdist for version 0.1.0, even yanking that is not enough (PyTorch found this out the hard way, some long-running issues were closed when deleting old yanked sdists). Change the behavior of installers to not use sdists by default. Make it easy for users to opt in to installing from source, but by default only look for wheels and error out with a clear message if no wheels matching the users' platform and Python interpreter are found. The pip maintainers recently agreed to take this direction, see pypa/pip#9140 . Also note that it's recommended to upload wheels even for projects that are pure Python, because installs are faster (metadata in a wheel is static, no need to run `setup.py) - TODO: find reference (e.g. podcast Brett Cannon). There are very few packages which would be unable to upload a wheel. Let individual packages determine the behavior of installers (try to install from sdist, or error out) via metadata on PyPI somehow. Individual solutions for some of the separate issues. For example, reduce the load on pip maintainers via better error messages, and let projects who want to drop support for old manylinux versions detect the pip version in their build scripts/files, and error out if a too old version is detected.","title":"Unsuspecting users getting failing from-source builds"},{"location":"key-issues/unexpected_fromsource_builds/#unsuspecting-users-getting-failing-from-source-builds","text":"","title":"Unsuspecting users getting failing from-source builds"},{"location":"key-issues/unexpected_fromsource_builds/#current-state","text":"When a project makes a release, it typically uploads one sdist (a source distribution) and multiple wheels (binary installers). Wheels are primarily meant to make the installation experience better and faster. For projects which contain code that needs to be compiled (e.g., C/C++/Cython), installing from the sdist is challenging. The sdist metadata does not even allow expression the required dependencies (e.g., a compiler - see native dependencies ). Hence installing from an sdist often goes wrong. Why does a user get an sdist when they didn't expect one? This can happen in quite a few circumstances: Shortly after the release of a new Python version, most projects will not yet have wheels for that new Python version uploaded to PyPI. So when a user installs the \"latest and greatest\" Python and type pip install somepackage , they are likely to see pip try to install the sdist of the highest version of somepackage . In case new hardware becomes available. A recent example is macOS arm64: it took many scientific projects over a year before they were able to build arm64 or universal2 wheels and upload them to PyPI. All users which used a native arm64 Python were getting builds from sdist. Users who use an old pip version (e.g. the pip shipped with their distro on a typical HPC cluster) which does not have support for recent manylinux versions may see pip try to install from an sdist even though there are (say) manylinux2014 wheels for the package. Installs from sdist may happen if a project tags a release but there's a problem on a particular platform for which it normally uploads wheels. Especially if this is a less popular platform (e.g., ppc64le ) the release manager may just go ahead with the release, and aim to upload the missing wheels later. If a project is uploading a new version and the person doing the release isn't careful to upload all wheels first and the sdist afterwards, then users on any platform may see installers try to install from the sdist. This mistake is easy to make, and can lead to a lot of failed installs quickly if a package is popular (e.g., the download rate for numpy is ~2000/minute). Many years ago, users were expecting to build from source. Today, in 2022, the scientific Python ecosystem has tens of millions of users. The vast majority of those users are not expecting, and are often unable, to build from source when they type a command like pip install scikit-learn .","title":"Current state"},{"location":"key-issues/unexpected_fromsource_builds/#problems","text":"There clearly are a lot of issues due to installing from an sdist when the user did not intend to do that. For users: Failed installs, often with confusing error messages and after a possibly time-consuming build step. Installs that appear to succeed but have issues that show up at runtime as a result of building against incorrect or mismatching libraries. This ranges from import errors due to missing symbols in shared libraries to segfaults and silently wrong numerical results. For pip maintainers: a lot of bug reports they have to deal with because the user thinks pip is the cause rather than the package they tried to install. For maintainers of projects with compiled code: A lot of bug reports that are very time-consuming to address. Issues are often not reproducible, and bug reports typically do not contain enough information to be able to understand if the problem is user error or an actual bug in the project. A lot of time spent carefully managing build dependencies and their versions in pyproject.toml (see, e.g., the oldest-supported-numpy metapackage ) which has as its only purpose to serve as a build dependency which pins numpy to the correct version (typically the lowest version for which there are wheels on PyPI) per platform and Python version/interpreter. Being forced to support platforms that are already end of life, because the project does not have a good way of dropping support for older manylinux flavors (see, e.g., numpy#19192 ).","title":"Problems"},{"location":"key-issues/unexpected_fromsource_builds/#history","text":"TODO","title":"History"},{"location":"key-issues/unexpected_fromsource_builds/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"key-issues/unexpected_fromsource_builds/#potential-solutions-or-mitigations","text":"Do not upload sdists to PyPI at all. This is the approach taken by many of the projects with the most complex builds - for example PyTorch, TensorFlow, MXNet, jaxlib, and Ray. It is necessary to then delete every single sdist for any version of the package from PyPI - if there was a single sdist for version 0.1.0, even yanking that is not enough (PyTorch found this out the hard way, some long-running issues were closed when deleting old yanked sdists). Change the behavior of installers to not use sdists by default. Make it easy for users to opt in to installing from source, but by default only look for wheels and error out with a clear message if no wheels matching the users' platform and Python interpreter are found. The pip maintainers recently agreed to take this direction, see pypa/pip#9140 . Also note that it's recommended to upload wheels even for projects that are pure Python, because installs are faster (metadata in a wheel is static, no need to run `setup.py) - TODO: find reference (e.g. podcast Brett Cannon). There are very few packages which would be unable to upload a wheel. Let individual packages determine the behavior of installers (try to install from sdist, or error out) via metadata on PyPI somehow. Individual solutions for some of the separate issues. For example, reduce the load on pip maintainers via better error messages, and let projects who want to drop support for old manylinux versions detect the pip version in their build scripts/files, and error out if a too old version is detected.","title":"Potential solutions or mitigations"},{"location":"key-issues/native-dependencies/","text":"Native dependencies Depending on non-Python compiled dependencies (\"native dependencies\") is very tricky. Python packaging works reasonably well when a project contains some self-contained C, C++ or Cython code. Even then, issues can occur though - for example because the dependency on a C or C++ compiler cannot be expressed in package metadata. So any constraints on versions, compiler types, etc. can only be documented and not enforced 1 . C, C++ and Cython are not the only languages that need native dependencies - Fortran, CUDA, and Rust are other commonly used languages, and there are more languages that one may want to use (e.g., in the context of scientific computing and GPUs, OpenCL, HIP and SYCL are of increasing interest). Once such code starts to depend on APIs from non-Python projects, more problems show up. Current state One obvious and hard to deal with problem is that dependencies on libraries that are not on PyPI cannot be expressed. When one types pip install somepkg for a somepkg that has such dependencies on a platform that somepkg doesn't provide wheels for, what is most likely to happen is a build failure halfway through because a dependency is missing or is present but in an unexpected configuration. Example: SciPy's build and runtime dependencies SciPy has a few build-time dependencies and one runtime dependency ( numpy ) listed in its pyproject.toml . As a package with medium build complexity (more complex than projects with self-contained C/Cython extensions, but less than the likes of TensorFlow and PyArrow), it can serve as an example of what metadata can and cannot capture about dependencies. This diagram illustrates those dependencies: Out of those, these are the dependencies declared in pyproject.toml : numpy , Cython , pybind11 , and pythran (also the build system dependencies: meson-python , wheel ). And these are the dependencies that cannot be declared: C/C++ compilers Fortran compiler BLAS and LAPACK libraries *-dev packages for Python, BLAS and LAPACK, if headers are packaged separately pkg-config or system CMake (for dependency resolution of BLAS/LAPACK) Finally, a number of native libraries (Boost, ARPACK, HiGHS, etc.) are vendored into SciPy. Unvendoring those (something system packagers would like) has been deemed infeasible 2 , those dependencies would also not be expressable and therefore make the build more fragile. Some projects do upload sdists but advise users to avoid them (or avoid PyPI completely in favor of other package managers). Some other projects do not upload sdists to PyPI to avoid users filing issues about failing builds. See purposes of PyPI for more on this topic. Building wheels is challenging too. Wheels are required to be self-contained, and therefore must vendor those non-Python dependencies. This means that a project becomes responsible for (often) rebuilding the dependency, dealing with the vendoring process (through auditwheel , delocate , delvewheel , etc. - and sometimes those tools are not enough), and implementing static linking or other ways of slimming down wheel sizes when vendoring large libraries. There may be other issues with vendoring, like EULA's for libraries like CUDA and MKL being ambiguous or outright forbidding redistribution. Even when the vendoring hurdle is successfully taken and working wheels are produced, there may be problems because vendoring may be the wrong solution technically. E.g., runtimes are often designed with the assumption that they're the only runtime on a given system - so having multiple vendored copied of a runtime in different packages leads to conflicts. Native dependencies is a huge topic, so to make the problems more concrete, a number of cases are worked out: BLAS, LAPACK and OpenMP , The Geospatial stack , Complex C++ dependencies . Problems The key problems are (1) not being able to express dependencies in metadata, and (b) the design of Python packaging and the wheel spec forcing vendoring dependencies. For more detailed explanations of problems, see the concrete cases linked above. History PEP 426, section \"Mapping dependencies to development and distribution activities\" (2012, withdrawn). PEP 459 - \"Standard Metadata Extensions for Python Software Packages\" (2013, withdrawn). This is probably the most relevant PEP that has been proposed - it explicitly deals with native dependencies provided by the system. Nathaniel Smith's pynativelib proposal (2016). PEP 668 - Marking Python base environments as \u201cexternally managed\u201d (2021). More history TODO Relevant resources TODO Potential solutions or mitigations From the \"Wanting a singular packaging tool/vision\" Discourse thread (2022): Define \"native requirements\" metadata (even if pip ignores it) Allow (encourage) wheels with binaries to have tighter dependencies than their sdists Encode and expose more information about ABI in package requirements Adding a mechanism to specify system dependencies that are needed by a Python package seems like a tractable first step here. Provide a way for users to get pure Python packages from PyPI, and everything else from a system package manager. ... many other potential improvements (all a lot of work). As a simple example, let's use Pythran - a Python to C++ transpiler that is gaining popularity and is used in SciPy and scikit-image. On Windows it needs Clang-cl rather than MSVC. This often goes wrong, because most users don't have Clang-cl installed. \u21a9 With Meson as the new build system for SciPy, it is becoming possible to query the system for dependencies first, and only fall back to a vendored version if the system is not found. This may be done in the future. \u21a9","title":"Native dependencies"},{"location":"key-issues/native-dependencies/#native-dependencies","text":"Depending on non-Python compiled dependencies (\"native dependencies\") is very tricky. Python packaging works reasonably well when a project contains some self-contained C, C++ or Cython code. Even then, issues can occur though - for example because the dependency on a C or C++ compiler cannot be expressed in package metadata. So any constraints on versions, compiler types, etc. can only be documented and not enforced 1 . C, C++ and Cython are not the only languages that need native dependencies - Fortran, CUDA, and Rust are other commonly used languages, and there are more languages that one may want to use (e.g., in the context of scientific computing and GPUs, OpenCL, HIP and SYCL are of increasing interest). Once such code starts to depend on APIs from non-Python projects, more problems show up.","title":"Native dependencies"},{"location":"key-issues/native-dependencies/#current-state","text":"One obvious and hard to deal with problem is that dependencies on libraries that are not on PyPI cannot be expressed. When one types pip install somepkg for a somepkg that has such dependencies on a platform that somepkg doesn't provide wheels for, what is most likely to happen is a build failure halfway through because a dependency is missing or is present but in an unexpected configuration. Example: SciPy's build and runtime dependencies SciPy has a few build-time dependencies and one runtime dependency ( numpy ) listed in its pyproject.toml . As a package with medium build complexity (more complex than projects with self-contained C/Cython extensions, but less than the likes of TensorFlow and PyArrow), it can serve as an example of what metadata can and cannot capture about dependencies. This diagram illustrates those dependencies: Out of those, these are the dependencies declared in pyproject.toml : numpy , Cython , pybind11 , and pythran (also the build system dependencies: meson-python , wheel ). And these are the dependencies that cannot be declared: C/C++ compilers Fortran compiler BLAS and LAPACK libraries *-dev packages for Python, BLAS and LAPACK, if headers are packaged separately pkg-config or system CMake (for dependency resolution of BLAS/LAPACK) Finally, a number of native libraries (Boost, ARPACK, HiGHS, etc.) are vendored into SciPy. Unvendoring those (something system packagers would like) has been deemed infeasible 2 , those dependencies would also not be expressable and therefore make the build more fragile. Some projects do upload sdists but advise users to avoid them (or avoid PyPI completely in favor of other package managers). Some other projects do not upload sdists to PyPI to avoid users filing issues about failing builds. See purposes of PyPI for more on this topic. Building wheels is challenging too. Wheels are required to be self-contained, and therefore must vendor those non-Python dependencies. This means that a project becomes responsible for (often) rebuilding the dependency, dealing with the vendoring process (through auditwheel , delocate , delvewheel , etc. - and sometimes those tools are not enough), and implementing static linking or other ways of slimming down wheel sizes when vendoring large libraries. There may be other issues with vendoring, like EULA's for libraries like CUDA and MKL being ambiguous or outright forbidding redistribution. Even when the vendoring hurdle is successfully taken and working wheels are produced, there may be problems because vendoring may be the wrong solution technically. E.g., runtimes are often designed with the assumption that they're the only runtime on a given system - so having multiple vendored copied of a runtime in different packages leads to conflicts. Native dependencies is a huge topic, so to make the problems more concrete, a number of cases are worked out: BLAS, LAPACK and OpenMP , The Geospatial stack , Complex C++ dependencies .","title":"Current state"},{"location":"key-issues/native-dependencies/#problems","text":"The key problems are (1) not being able to express dependencies in metadata, and (b) the design of Python packaging and the wheel spec forcing vendoring dependencies. For more detailed explanations of problems, see the concrete cases linked above.","title":"Problems"},{"location":"key-issues/native-dependencies/#history","text":"PEP 426, section \"Mapping dependencies to development and distribution activities\" (2012, withdrawn). PEP 459 - \"Standard Metadata Extensions for Python Software Packages\" (2013, withdrawn). This is probably the most relevant PEP that has been proposed - it explicitly deals with native dependencies provided by the system. Nathaniel Smith's pynativelib proposal (2016). PEP 668 - Marking Python base environments as \u201cexternally managed\u201d (2021). More history TODO","title":"History"},{"location":"key-issues/native-dependencies/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"key-issues/native-dependencies/#potential-solutions-or-mitigations","text":"From the \"Wanting a singular packaging tool/vision\" Discourse thread (2022): Define \"native requirements\" metadata (even if pip ignores it) Allow (encourage) wheels with binaries to have tighter dependencies than their sdists Encode and expose more information about ABI in package requirements Adding a mechanism to specify system dependencies that are needed by a Python package seems like a tractable first step here. Provide a way for users to get pure Python packages from PyPI, and everything else from a system package manager. ... many other potential improvements (all a lot of work). As a simple example, let's use Pythran - a Python to C++ transpiler that is gaining popularity and is used in SciPy and scikit-image. On Windows it needs Clang-cl rather than MSVC. This often goes wrong, because most users don't have Clang-cl installed. \u21a9 With Meson as the new build system for SciPy, it is becoming possible to query the system for dependencies first, and only fall back to a vendored version if the system is not found. This may be done in the future. \u21a9","title":"Potential solutions or mitigations"},{"location":"key-issues/native-dependencies/blas_openmp/","text":"BLAS, LAPACK and OpenMP BLAS, LAPACK and OpenMP are key libraries for scientific computing. BLAS and LAPACK provide linear algebra functionality, and OpenMP provides primitives for parallel computing on shared memory machines. They're written in C, C++, Fortran, and even some assembly code. They are typically not packaged on PyPI (MKL is the exception). The most popular libraries for BLAS and LAPACK are OpenBLAS and MKL; they provide both BLAS and LAPACK 1 . NumPy and SciPy depend on BLAS and LAPACK; scikit-learn depends on OpenMP. BLAS and LAPACK libraries themselves can be built to use OpenMP or pthreads. Two things make dealing with these native dependencies extra challenging: There are multiple independent implementations of the BLAS, LAPACK and OpenMP API specifications. They adhere to the same API, but may be built with different ABI, make different choices for symbol names for 64-bit builds, etc. All packages in an environment should use the same library for each of BLAS, LAPACK and OpenMP, because otherwise issues with threading control will occur. This is difficult to guarantee if wheels must be self-contained; it's clearly at odds with vendoring. What are BLAS and LAPACK exactly? BLAS (Basic Linear Algebra Subprograms) are routines for basic vector and matrix operations. LAPACK\u2009(Linear Algebra PACKage) builds on top of BLAS and provides more complex linear algebra routines. Both have a reference implementation in the Netlib repository , written in Fortran: BLAS , LAPACK . BLAS also has a reference C API, named CBLAS which is widely implemented; LAPACK correspondingly has a C API named LAPACKE , however that is less widely implemented. A lot of libraries implement the BLAS and LAPACK interfaces. This is typically done to obtain maximum performance, and optimized for particular hardware. The performance improvements over the Netlib version are often in the 10x-100x range, and given how critical linear algebra is to scientific computing and deep learning, these libraries and their performance characteristics are of major importance. Well-known implementations include OpenBLAS , Intel MKL , Apple Accelerate , ATLAS (no longer updated, but still shipped especially by Linux distros), BLIS (BLAS-only) and libflame (LAPACK-only, builds on BLIS), AMD's AOCL-BLIS and AOCL-libFLAME , and ARM Performance Libraries . Those are all for CPU; there are more libraries for GPUs and other accelerators, as well as libraries supporting sparse and graph data structures. What is OpenMP exactly? OpenMP (Open Multi-Processing) is an API that supports shared-memory parallel programming in C, C++ and Fortran. It is widely used, from regular laptop/desktop to large supercomputers. There are many implementations, often shipped together with compilers. Commonly used implementations (which we may find vendored in wheels on PyPI) include the GCC implementation ( libgomp ), the LLVM implementation ( libomp ), and the Intel implementation ( libiomp ). For a comprehensive overview, see this overview . Current state NumPy vendors OpenBLAS as a shared library in the wheels it uploads to PyPI. SciPy also vendors OpenBLAS, in the same manner as NumPy - just not the same version, NumPy moved to ILP64 (64-bit) OpenBLAS while SciPy uses the default 32-bit build. Those OpenBLAS libraries are built with pthreads. In conda-forge and Homebrew on the other hand, OpenBLAS is built with OpenMP. The OpenBLAS build for wheels requires a separate repository with CI - see macPython/openblas-libs . The build is maintained mostly by NumPy and SciPy maintainers - and as can be seen from the history in the repository, it's a lot of work. To make matters worse, NumPy and SciPy want tight control over the version of OpenBLAS they ship with, because another version may cause segfaults or wildly incorrect numerical results. Hence upgrading is very hard. Things have gotten a little better recently though, and it may be an option to start depending on OpenBLAS as a separate package with a version range rather than an exact pin. openblas-libs#86 discussed in-progress work to create a separate wheel for OpenBLAS; not ideal to have to maintain that, but an improvement over vendoring separate versions in NumPy and SciPy wheels. Scikit-learn depends on OpenMP directly for an increasing amount of its parallel execution, and vendors libomp / libgomp in its wheels. It depends on SciPy for linear algebra functionality - in particular, the Cython interface to BLAS and LAPACK that SciPy provides. The scikit-learn team also created threadpoolctl , a separate package specifically to control the parallelism across NumPy, SciPy, scikit-learn, and any installed BLAS, LAPACK and OpenMP libraries. PyTorch statically links MKL (except on macOS) and vendors OpenMP ( libiomp on Windows/macOS, libgomp on Linux) in its wheels. TensorFlow uses Eigen for linear algebra functionality. This is a header-only C++ library, different from BLAS/LAPACK implementations and easier for distribution. For parallel execution, TensorFlow does use OpenMP. It is statically linked rather than vendored. Deep learning libraries are often their own \"silo\", providing all functionality in one coherent framework. When they are installed side-by-side with other packages, there are issues though with conflicting libraries. However, the problem is worse in the PyData stack. Here is what the situation look for just NumPy, SciPy and scikit-learn: Build and runtime dependencies for scikit-learn - showing how BLAS, LAPACK and OpenMP are included in an environment when installing either from wheels on PyPI, or from conda-forge or a similar such system that includes all required dependencies as separate packages. As we can see, there are multiple issues that are unique to PyPI: We have multiple copies of OpenBLAS rather than just one. Also different versions, breaking the \"one version rule\", No runtime switching of BLAS libraries, because of vendoring, Mixing pthreads and OpenMP parallelism, resulting in oversubscription issues, A diamond dependency, where NumPy and SciPy both depend on OpenBLAS. Making upgrades that not synchronized harder, even if those projects will be able to release a single OpenBLAS wheel in the future. When for example PyTorch is installed, the problems multiply - now we also have a second vendored OpenMP library in addition to two BLAS/LAPACK libraries. Example: PyTorch & OpenMP PyTorch makes heavy use of OpenMP. And has a lot of issues with it - partly because OpenMP is inherently complex, and partly because of packaging limitations. One recurring problem is deadlocks in combination with multiprocessing (see, e.g., pytorch#17199 ). These are caused by the GCC implementation of OpenMP ( libgomp ) not being fork-safe, and multiprocessing defaulting to 'fork' . The LLVM ( libomp ) and Intel ( libiomp ) implementations are fork-safe, therefore if the dependency could be expressed in package metadata, it would be easier to avoid the problem. The dependency of PyTorch on OpenMP is completely implicit in pyproject.toml / setup.py metadata however. Here is another example issue, where a PyTorch build inside a virtualenv picks up an OpenMP implementation in an uncontrolled fashion, because it's an implicit dependency and hence build results get affected by another package pulling in an OpenMP library: pytorch#18398 . Note: there is an active discussion ( cpython#84559 , Discourse thread - Dec'22) to change the default multiprocessing context away from 'fork' . System package managers usually have a way of dealing with multiple implementations of an API, through building against a reference package with stubs as a \"virtual dependency\". This can be a generic mechanism, or specific to the dependency type. Conda manages BLAS, LAPACK and OpenMP (and other such complex dependencies, like MPI) through mutex metapackages , which ensure a single version of an implementation is installed and allow users to switch between implementations . Spack uses virtual dependencies which work in a similar fashion. Linux package managers may do something similar (e.g., Debian uses update-alternatives applied to libblas and liblapack ), or they may use a more a more specific way of managing BLAS and LAPACK, such as Fedora using Flexiblas . Problems Building from source is quite difficult. Dependencies on BLAS, LAPACK and OpenMP cannot be expressed, and end users will typically not have it installed, or they do have it but it's not found, or it's the wrong version. In addition, variations between system package managers are not easy to handle (e.g., Fedora fails to ship pkg-config files for OpenBLAS, Arch Linux fails to include LAPACK in OpenBLAS, etc.). Building wheels is way too much work. Maintainers of each project using BLAS and LAPACK are responsible for building it themselves (which is painful) and then vendoring it. In addition, in the case of OpenBLAS, it requires vendoring libgfortran . Deadlocks due to use of multiprocessing in combination with libgomp . Issues due to using more than one library of the same type. See for example scipy#15050 ( \"Performance issue on macOS arm64 (M1) when installing from wheels (2x libopenblas)\" ) and openblas#3187 ( \"Slowdown when using openblas-pthreads alongside openmp based parallel code\" ). As a result of the above-mentioned problems with OpenMP, SciPy bans usage of OpenMP completely even though it would be of significant interest to use OpenMP. Scikit-learn is gradually expanding its usage of OpenMP, but is running into problems. No runtime switching of implementations when installing from wheels. This makes testing, benchmarking and development work more difficult. MKL does provide wheels but cannot be linked to due to the wheel spec. And MKL itself is >100 MB in size, so vendoring it isn't reasonable. History TODO Relevant resources scipy#10239 - can OpenMP be used portably array-api#4 - Parallelism - what do libraries offer, and is there an API aspect to it OpenBLAS build config for vendoring into wheels: MacPython/openblas-libs Potential solutions or mitigations A mitigation to not make things worse: stay with the status quo, do not use more OpenMP. Not a very satisfactory one though. Build wheels for OpenBLAS and perhaps also OpenMP and maintain those on PyPI. Issue: who takes responsibility for these, and decides on changes over time (possibly breaking ones)? See this gradual evolution plan discussed between scikit-learn and SciPy maintainers: scipy#15050 Larger changes to PyPI/wheels to get to parity with system package managers. This will require dealing with several \"meta topics\", like a build farm and PyPI's author-led social model , in addition to implementing something like virtual packages. Making it possible to express dependencies on libraries outside of PyPI. Making a distinction on PyPI between pure Python packages and other packages. With the latter set of packages all being provided by a system package manager. Note that it is possible to build OpenBLAS without LAPACK support. This is a bad idea, however Arch Linux does do this anyway (as of Dec 2022, see scipy#17465 ). Accounting for such nonstandard choices by individual packagers of system dependencies makes dealing with native dependencies extra difficult. \u21a9","title":"BLAS, LAPACK and OpenMP"},{"location":"key-issues/native-dependencies/blas_openmp/#blas-lapack-and-openmp","text":"BLAS, LAPACK and OpenMP are key libraries for scientific computing. BLAS and LAPACK provide linear algebra functionality, and OpenMP provides primitives for parallel computing on shared memory machines. They're written in C, C++, Fortran, and even some assembly code. They are typically not packaged on PyPI (MKL is the exception). The most popular libraries for BLAS and LAPACK are OpenBLAS and MKL; they provide both BLAS and LAPACK 1 . NumPy and SciPy depend on BLAS and LAPACK; scikit-learn depends on OpenMP. BLAS and LAPACK libraries themselves can be built to use OpenMP or pthreads. Two things make dealing with these native dependencies extra challenging: There are multiple independent implementations of the BLAS, LAPACK and OpenMP API specifications. They adhere to the same API, but may be built with different ABI, make different choices for symbol names for 64-bit builds, etc. All packages in an environment should use the same library for each of BLAS, LAPACK and OpenMP, because otherwise issues with threading control will occur. This is difficult to guarantee if wheels must be self-contained; it's clearly at odds with vendoring. What are BLAS and LAPACK exactly? BLAS (Basic Linear Algebra Subprograms) are routines for basic vector and matrix operations. LAPACK\u2009(Linear Algebra PACKage) builds on top of BLAS and provides more complex linear algebra routines. Both have a reference implementation in the Netlib repository , written in Fortran: BLAS , LAPACK . BLAS also has a reference C API, named CBLAS which is widely implemented; LAPACK correspondingly has a C API named LAPACKE , however that is less widely implemented. A lot of libraries implement the BLAS and LAPACK interfaces. This is typically done to obtain maximum performance, and optimized for particular hardware. The performance improvements over the Netlib version are often in the 10x-100x range, and given how critical linear algebra is to scientific computing and deep learning, these libraries and their performance characteristics are of major importance. Well-known implementations include OpenBLAS , Intel MKL , Apple Accelerate , ATLAS (no longer updated, but still shipped especially by Linux distros), BLIS (BLAS-only) and libflame (LAPACK-only, builds on BLIS), AMD's AOCL-BLIS and AOCL-libFLAME , and ARM Performance Libraries . Those are all for CPU; there are more libraries for GPUs and other accelerators, as well as libraries supporting sparse and graph data structures. What is OpenMP exactly? OpenMP (Open Multi-Processing) is an API that supports shared-memory parallel programming in C, C++ and Fortran. It is widely used, from regular laptop/desktop to large supercomputers. There are many implementations, often shipped together with compilers. Commonly used implementations (which we may find vendored in wheels on PyPI) include the GCC implementation ( libgomp ), the LLVM implementation ( libomp ), and the Intel implementation ( libiomp ). For a comprehensive overview, see this overview .","title":"BLAS, LAPACK and OpenMP"},{"location":"key-issues/native-dependencies/blas_openmp/#current-state","text":"NumPy vendors OpenBLAS as a shared library in the wheels it uploads to PyPI. SciPy also vendors OpenBLAS, in the same manner as NumPy - just not the same version, NumPy moved to ILP64 (64-bit) OpenBLAS while SciPy uses the default 32-bit build. Those OpenBLAS libraries are built with pthreads. In conda-forge and Homebrew on the other hand, OpenBLAS is built with OpenMP. The OpenBLAS build for wheels requires a separate repository with CI - see macPython/openblas-libs . The build is maintained mostly by NumPy and SciPy maintainers - and as can be seen from the history in the repository, it's a lot of work. To make matters worse, NumPy and SciPy want tight control over the version of OpenBLAS they ship with, because another version may cause segfaults or wildly incorrect numerical results. Hence upgrading is very hard. Things have gotten a little better recently though, and it may be an option to start depending on OpenBLAS as a separate package with a version range rather than an exact pin. openblas-libs#86 discussed in-progress work to create a separate wheel for OpenBLAS; not ideal to have to maintain that, but an improvement over vendoring separate versions in NumPy and SciPy wheels. Scikit-learn depends on OpenMP directly for an increasing amount of its parallel execution, and vendors libomp / libgomp in its wheels. It depends on SciPy for linear algebra functionality - in particular, the Cython interface to BLAS and LAPACK that SciPy provides. The scikit-learn team also created threadpoolctl , a separate package specifically to control the parallelism across NumPy, SciPy, scikit-learn, and any installed BLAS, LAPACK and OpenMP libraries. PyTorch statically links MKL (except on macOS) and vendors OpenMP ( libiomp on Windows/macOS, libgomp on Linux) in its wheels. TensorFlow uses Eigen for linear algebra functionality. This is a header-only C++ library, different from BLAS/LAPACK implementations and easier for distribution. For parallel execution, TensorFlow does use OpenMP. It is statically linked rather than vendored. Deep learning libraries are often their own \"silo\", providing all functionality in one coherent framework. When they are installed side-by-side with other packages, there are issues though with conflicting libraries. However, the problem is worse in the PyData stack. Here is what the situation look for just NumPy, SciPy and scikit-learn: Build and runtime dependencies for scikit-learn - showing how BLAS, LAPACK and OpenMP are included in an environment when installing either from wheels on PyPI, or from conda-forge or a similar such system that includes all required dependencies as separate packages. As we can see, there are multiple issues that are unique to PyPI: We have multiple copies of OpenBLAS rather than just one. Also different versions, breaking the \"one version rule\", No runtime switching of BLAS libraries, because of vendoring, Mixing pthreads and OpenMP parallelism, resulting in oversubscription issues, A diamond dependency, where NumPy and SciPy both depend on OpenBLAS. Making upgrades that not synchronized harder, even if those projects will be able to release a single OpenBLAS wheel in the future. When for example PyTorch is installed, the problems multiply - now we also have a second vendored OpenMP library in addition to two BLAS/LAPACK libraries. Example: PyTorch & OpenMP PyTorch makes heavy use of OpenMP. And has a lot of issues with it - partly because OpenMP is inherently complex, and partly because of packaging limitations. One recurring problem is deadlocks in combination with multiprocessing (see, e.g., pytorch#17199 ). These are caused by the GCC implementation of OpenMP ( libgomp ) not being fork-safe, and multiprocessing defaulting to 'fork' . The LLVM ( libomp ) and Intel ( libiomp ) implementations are fork-safe, therefore if the dependency could be expressed in package metadata, it would be easier to avoid the problem. The dependency of PyTorch on OpenMP is completely implicit in pyproject.toml / setup.py metadata however. Here is another example issue, where a PyTorch build inside a virtualenv picks up an OpenMP implementation in an uncontrolled fashion, because it's an implicit dependency and hence build results get affected by another package pulling in an OpenMP library: pytorch#18398 . Note: there is an active discussion ( cpython#84559 , Discourse thread - Dec'22) to change the default multiprocessing context away from 'fork' . System package managers usually have a way of dealing with multiple implementations of an API, through building against a reference package with stubs as a \"virtual dependency\". This can be a generic mechanism, or specific to the dependency type. Conda manages BLAS, LAPACK and OpenMP (and other such complex dependencies, like MPI) through mutex metapackages , which ensure a single version of an implementation is installed and allow users to switch between implementations . Spack uses virtual dependencies which work in a similar fashion. Linux package managers may do something similar (e.g., Debian uses update-alternatives applied to libblas and liblapack ), or they may use a more a more specific way of managing BLAS and LAPACK, such as Fedora using Flexiblas .","title":"Current state"},{"location":"key-issues/native-dependencies/blas_openmp/#problems","text":"Building from source is quite difficult. Dependencies on BLAS, LAPACK and OpenMP cannot be expressed, and end users will typically not have it installed, or they do have it but it's not found, or it's the wrong version. In addition, variations between system package managers are not easy to handle (e.g., Fedora fails to ship pkg-config files for OpenBLAS, Arch Linux fails to include LAPACK in OpenBLAS, etc.). Building wheels is way too much work. Maintainers of each project using BLAS and LAPACK are responsible for building it themselves (which is painful) and then vendoring it. In addition, in the case of OpenBLAS, it requires vendoring libgfortran . Deadlocks due to use of multiprocessing in combination with libgomp . Issues due to using more than one library of the same type. See for example scipy#15050 ( \"Performance issue on macOS arm64 (M1) when installing from wheels (2x libopenblas)\" ) and openblas#3187 ( \"Slowdown when using openblas-pthreads alongside openmp based parallel code\" ). As a result of the above-mentioned problems with OpenMP, SciPy bans usage of OpenMP completely even though it would be of significant interest to use OpenMP. Scikit-learn is gradually expanding its usage of OpenMP, but is running into problems. No runtime switching of implementations when installing from wheels. This makes testing, benchmarking and development work more difficult. MKL does provide wheels but cannot be linked to due to the wheel spec. And MKL itself is >100 MB in size, so vendoring it isn't reasonable.","title":"Problems"},{"location":"key-issues/native-dependencies/blas_openmp/#history","text":"TODO","title":"History"},{"location":"key-issues/native-dependencies/blas_openmp/#relevant-resources","text":"scipy#10239 - can OpenMP be used portably array-api#4 - Parallelism - what do libraries offer, and is there an API aspect to it OpenBLAS build config for vendoring into wheels: MacPython/openblas-libs","title":"Relevant resources"},{"location":"key-issues/native-dependencies/blas_openmp/#potential-solutions-or-mitigations","text":"A mitigation to not make things worse: stay with the status quo, do not use more OpenMP. Not a very satisfactory one though. Build wheels for OpenBLAS and perhaps also OpenMP and maintain those on PyPI. Issue: who takes responsibility for these, and decides on changes over time (possibly breaking ones)? See this gradual evolution plan discussed between scikit-learn and SciPy maintainers: scipy#15050 Larger changes to PyPI/wheels to get to parity with system package managers. This will require dealing with several \"meta topics\", like a build farm and PyPI's author-led social model , in addition to implementing something like virtual packages. Making it possible to express dependencies on libraries outside of PyPI. Making a distinction on PyPI between pure Python packages and other packages. With the latter set of packages all being provided by a system package manager. Note that it is possible to build OpenBLAS without LAPACK support. This is a bad idea, however Arch Linux does do this anyway (as of Dec 2022, see scipy#17465 ). Accounting for such nonstandard choices by individual packagers of system dependencies makes dealing with native dependencies extra difficult. \u21a9","title":"Potential solutions or mitigations"},{"location":"key-issues/native-dependencies/cpp_deps/","text":"Complex C++ dependencies Projects with a large amount of C++ code and dependencies include TensorFlow , PyTorch , Apache Arrow , Apache MXNet , cuDF , Ray , ROOT , and Vaex . Many other projects include some C++ components, like: scikit-learn , SciPy , NumPy , CuPy , and Awkward Array . To get an impression of the number and type of external dependencies that some of these projects have, outside of their own large code bases, one can look at the \"third party\" directories in their repositories: pytorch/third_party , tensorflow/third_party , mxnet/3rdparty . There are usually more build time dependencies that are specified elsewhere - some on PyPI, some only listed in the documentation. Projects with dependencies like that tend to not upload any sdist's to PyPI (users will see too many build failures, see purposes of PyPI for context), and have a hard time building wheels because of the requirement that a wheel must be self-contained - which means a lot of vendoring and potential issues. Current state Wheels tend to work fine for packages that have some C, C++ or Cython code but no external dependencies other than Python packages. Once a project has dependencies on other C++ libraries, it has to build that other library and vendor it as part of building its own wheel. Tools like auditwheel , delocate and delvewheel help with the vendoring part, but building everything in a consistent fashion is (a) a lot of work, and (b) may be very difficult. Why are wheels hard - Wes McKinney The Python wheel binary standard was optimized for easy installation of packages with C extensions, but where those C extensions are simple to build. The best case scenario is that the C code is completely self-contained. If things are more complicated, things get messy: Third party C libraries Third party C++ libraries Differing C++ ABI versions The constraint of wheels is that a package must generally be entirely self-contained, including all C/C++ symbols included via static linking or by including the shared library bundled in the wheel -- which style of bundling works best may be different for Linux, macOS, and Windows. Only very recently (Oct '22) was the requirement for wheels to be fully self-contained loosened a little, allowing package authors to take responsibility for the quality of their own wheels and avoiding vendoring of libraries that are very large or had to be vendored into multiple wheels that they have control over: auditwheel#368 . That change is an improvement, but doesn't change the big picture - package authors still cannot rely on other native dependencies on the system easily, and may have to maintain their own separate wheels (e.g., if SciPy and NumPy want to rely on a shared libopenblas wheel, they are the ones who have to do all the work for that). Example: PyArrow PyArrow is the Python API of Apache Arrow. Apache Arrow is a C++ project with many C and C++ dependencies. Large and complicated ones. In 2019 the need to vendor the likes of OpenSLL, gRPC, and Gandiva (which in turn relies on LLVM) made it too hard to build wheels for PyPI at all - because all dependencies must be built from source and vendored into a wheel, which is a major endeavour. As of Dec 2022, there are wheels again, without OpenSSL and Gandiva in them. TODO: figure out in more detail what changed here To understand the C/C++ dependencies of PyArrow, let's look at the dependency tree in conda-forge and at the libraries vendored into a manylinux wheel on PyPI: PyArrow dependency tree (conda-forge) $ # Note: output edited to remove duplicate packages and python/numpy dependencies $ mamba repoquery depends pyarrow --tree pyarrow [ 9 .0.0 ] \u251c\u2500 libgcc-ng [ 12 .2.0 ] \u2502 \u251c\u2500 _openmp_mutex [ 4 .5 ] \u2502 \u2502 \u251c\u2500 _libgcc_mutex [ 0 .1 ] \u2502 \u2502 \u2514\u2500 libgomp [ 12 .2.0 ] \u251c\u2500 libstdcxx-ng [ 12 .2.0 ] \u251c\u2500 numpy [ 1 .23.5 ] \u251c\u2500 parquet-cpp [ 1 .5.1 ] \u2502 \u2514\u2500 arrow-cpp [ 9 .0.0 ] \u2502 \u251c\u2500 gflags [ 2 .2.2 ] \u2502 \u251c\u2500 c-ares [ 1 .18.1 ] \u2502 \u251c\u2500 libbrotlienc [ 1 .0.9 ] \u2502 \u2502 \u2514\u2500 libbrotlicommon [ 1 .0.9 ] \u2502 \u251c\u2500 libbrotlidec [ 1 .0.9 ] \u2502 \u251c\u2500 zstd [ 1 .5.2 ] \u2502 \u251c\u2500 aws-sdk-cpp [ 1 .8.186 ] \u2502 \u2502 \u251c\u2500 aws-c-event-stream [ 0 .2.7 ] \u2502 \u2502 \u2502 \u251c\u2500 aws-c-common [ 0 .6.2 ] \u2502 \u2502 \u2502 \u251c\u2500 aws-checksums [ 0 .1.11 ] \u2502 \u2502 \u2502 \u2514\u2500 aws-c-io [ 0 .10.5 ] \u2502 \u2502 \u2502 \u251c\u2500 aws-c-cal [ 0 .5.11 ] \u2502 \u2502 \u2502 \u2514\u2500 s2n [ 1 .0.10 ] \u2502 \u2502 \u251c\u2500 libcurl [ 7 .86.0 ] \u2502 \u2502 \u2502 \u251c\u2500 krb5 [ 1 .19.3 ] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libedit [ 3 .1.20191231 ] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 keyutils [ 1 .6.1 ] \u2502 \u2502 \u2502 \u251c\u2500 libssh2 [ 1 .10.0 ] \u2502 \u2502 \u2502 \u2514\u2500 libnghttp2 [ 1 .47.0 ] \u2502 \u2502 \u2502 \u2514\u2500 libev [ 4 .33 ] \u2502 \u251c\u2500 lz4-c [ 1 .9.3 ] \u2502 \u251c\u2500 libthrift [ 0 .16.0 ] \u2502 \u2502 \u2514\u2500 libevent [ 2 .1.10 ] \u2502 \u251c\u2500 libutf8proc [ 2 .8.0 ] \u2502 \u251c\u2500 snappy [ 1 .1.9 ] \u2502 \u251c\u2500 re2 [ 2022 .06.01 ] \u2502 \u251c\u2500 glog [ 0 .6.0 ] \u2502 \u251c\u2500 libabseil [ 20220623 .0 ] \u2502 \u251c\u2500 libprotobuf [ 3 .21.10 ] \u2502 \u251c\u2500 orc [ 1 .8.0 ] \u2502 \u251c\u2500 libgrpc [ 1 .49.1 ] \u2502 \u2502 \u251c\u2500 zlib [ 1 .2.13 ] \u2502 \u2514\u2500 libgoogle-cloud [ 2 .3.0 ] \u2502 \u251c\u2500 libcrc32c [ 1 .1.2 ] \u251c\u2500 python_abi \u251c\u2500 python \u251c\u2500 numpy \u2514\u2500 arrow-cpp PyArrow vendored libraries (PyPI wheels) $ ls pyarrow/*.so # pyarrow/ is from an unzipped manylinux wheel /home/rgommers/Downloads/pyarrow/_compute.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_csv.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_dataset.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_dataset_orc.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_dataset_parquet.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_exec_plan.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_feather.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_flight.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_fs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_gcsfs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_hdfs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_hdfsio.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_json.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/libarrow_python_flight.so /home/rgommers/Downloads/pyarrow/libarrow_python.so /home/rgommers/Downloads/pyarrow/lib.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_orc.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_parquet.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_parquet_encryption.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_plasma.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_pyarrow_cpp_tests.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_s3fs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_substrait.cpython-311-x86_64-linux-gnu.so We see that Apache Parquet and Substrait , two separate projects, have to be vendored, and a lot of Apache Arrow C++ components are packed into a single wheel. What can't be seen from the wheel contents is that some of the extension modules were built with other complex dependencies like protobuf and glog (which therefore also need to be built as part of the wheel build process). Such dependencies raise the possibility of symbol conflicts when other packages are built with different versions of those libraries, or in different ways. This can result in hard to debug crashes or \"undefined symbol\" problems. This blog post by Uwe Korn describes some of the issues in detail, including problems installing PyArrow side by side with TensorFlow and Turbodbc. C++ ABI has been an issue for quite a while. Most C++ developers and projects want to use the new ABI, however due to the old manylinux standard and depending on devtoolset forces the use of the old C++ ABI ( _GLIBCXX_USE_CXX11_ABI=0 ). Projects using modern C++14/17 typically want to use the new ABI. This is still quite difficult. It's now possible with manylinux_2_28 , but requires building duplicate sets of wheels 1 (also manylinux2014 for compatibility with older distros like Ubuntu 18.04 and CentOS 8, that will still use the old ABI). As an example: PyTorch added libtorch builds with the new ABI to its own download server in 2019 already ( pytorch#17492 ), however the issue for matching wheels is still open ( pytorch#51039 ) as of Dec '22. An even thornier issue is the proliferation of Abseil in the API of projects like protobuf , given that Abseil's ABI is sensitive (by default) to the C++ standard version being used to compile it, and this needs to be held consistent (essentially) across the entire ecosystem. For more details, see here . Problems Requirement for a wheel being completely self-contained, forcing vendoring of external C++ dependencies. Building external dependencies is a lot of effort, and error prone. It also prevents splitting up a wheel into multiple dependent ones. This may be desirable because of binary size or maintainability. The old C++ ABI still being the default. Symbol clashes when different libraries vendor the same external dependency. History Early history TODO. The RAPIDS projects was forced to drop wheels completely from May 2019 to Oct 2022, because the manylinux required a too old C++ version, and made it impossible to create compliant wheels with the RAPIDS C++14 code base. See this blog post for details . Apache Arrow's issues with wheels and the amount of effort they take were laid out in detail in this mailing list post by Wes McKinney . This long discussion on pypa/packaging-problems#25 touches on many of the key pain points around wheels and projects with complex C/C++ dependencies (interspersed with some \"packaging politics\"). Relevant resources TODO Potential solutions or mitigations Better interaction/integration between PyPI/wheels/pip and other package managers, where dealing with C++ dependencies is easier. ... ? See manylinux#1332 and pytorch#51039 for details. \u21a9","title":"Complex C++ dependencies"},{"location":"key-issues/native-dependencies/cpp_deps/#complex-c-dependencies","text":"Projects with a large amount of C++ code and dependencies include TensorFlow , PyTorch , Apache Arrow , Apache MXNet , cuDF , Ray , ROOT , and Vaex . Many other projects include some C++ components, like: scikit-learn , SciPy , NumPy , CuPy , and Awkward Array . To get an impression of the number and type of external dependencies that some of these projects have, outside of their own large code bases, one can look at the \"third party\" directories in their repositories: pytorch/third_party , tensorflow/third_party , mxnet/3rdparty . There are usually more build time dependencies that are specified elsewhere - some on PyPI, some only listed in the documentation. Projects with dependencies like that tend to not upload any sdist's to PyPI (users will see too many build failures, see purposes of PyPI for context), and have a hard time building wheels because of the requirement that a wheel must be self-contained - which means a lot of vendoring and potential issues.","title":"Complex C++ dependencies"},{"location":"key-issues/native-dependencies/cpp_deps/#current-state","text":"Wheels tend to work fine for packages that have some C, C++ or Cython code but no external dependencies other than Python packages. Once a project has dependencies on other C++ libraries, it has to build that other library and vendor it as part of building its own wheel. Tools like auditwheel , delocate and delvewheel help with the vendoring part, but building everything in a consistent fashion is (a) a lot of work, and (b) may be very difficult. Why are wheels hard - Wes McKinney The Python wheel binary standard was optimized for easy installation of packages with C extensions, but where those C extensions are simple to build. The best case scenario is that the C code is completely self-contained. If things are more complicated, things get messy: Third party C libraries Third party C++ libraries Differing C++ ABI versions The constraint of wheels is that a package must generally be entirely self-contained, including all C/C++ symbols included via static linking or by including the shared library bundled in the wheel -- which style of bundling works best may be different for Linux, macOS, and Windows. Only very recently (Oct '22) was the requirement for wheels to be fully self-contained loosened a little, allowing package authors to take responsibility for the quality of their own wheels and avoiding vendoring of libraries that are very large or had to be vendored into multiple wheels that they have control over: auditwheel#368 . That change is an improvement, but doesn't change the big picture - package authors still cannot rely on other native dependencies on the system easily, and may have to maintain their own separate wheels (e.g., if SciPy and NumPy want to rely on a shared libopenblas wheel, they are the ones who have to do all the work for that). Example: PyArrow PyArrow is the Python API of Apache Arrow. Apache Arrow is a C++ project with many C and C++ dependencies. Large and complicated ones. In 2019 the need to vendor the likes of OpenSLL, gRPC, and Gandiva (which in turn relies on LLVM) made it too hard to build wheels for PyPI at all - because all dependencies must be built from source and vendored into a wheel, which is a major endeavour. As of Dec 2022, there are wheels again, without OpenSSL and Gandiva in them. TODO: figure out in more detail what changed here To understand the C/C++ dependencies of PyArrow, let's look at the dependency tree in conda-forge and at the libraries vendored into a manylinux wheel on PyPI: PyArrow dependency tree (conda-forge) $ # Note: output edited to remove duplicate packages and python/numpy dependencies $ mamba repoquery depends pyarrow --tree pyarrow [ 9 .0.0 ] \u251c\u2500 libgcc-ng [ 12 .2.0 ] \u2502 \u251c\u2500 _openmp_mutex [ 4 .5 ] \u2502 \u2502 \u251c\u2500 _libgcc_mutex [ 0 .1 ] \u2502 \u2502 \u2514\u2500 libgomp [ 12 .2.0 ] \u251c\u2500 libstdcxx-ng [ 12 .2.0 ] \u251c\u2500 numpy [ 1 .23.5 ] \u251c\u2500 parquet-cpp [ 1 .5.1 ] \u2502 \u2514\u2500 arrow-cpp [ 9 .0.0 ] \u2502 \u251c\u2500 gflags [ 2 .2.2 ] \u2502 \u251c\u2500 c-ares [ 1 .18.1 ] \u2502 \u251c\u2500 libbrotlienc [ 1 .0.9 ] \u2502 \u2502 \u2514\u2500 libbrotlicommon [ 1 .0.9 ] \u2502 \u251c\u2500 libbrotlidec [ 1 .0.9 ] \u2502 \u251c\u2500 zstd [ 1 .5.2 ] \u2502 \u251c\u2500 aws-sdk-cpp [ 1 .8.186 ] \u2502 \u2502 \u251c\u2500 aws-c-event-stream [ 0 .2.7 ] \u2502 \u2502 \u2502 \u251c\u2500 aws-c-common [ 0 .6.2 ] \u2502 \u2502 \u2502 \u251c\u2500 aws-checksums [ 0 .1.11 ] \u2502 \u2502 \u2502 \u2514\u2500 aws-c-io [ 0 .10.5 ] \u2502 \u2502 \u2502 \u251c\u2500 aws-c-cal [ 0 .5.11 ] \u2502 \u2502 \u2502 \u2514\u2500 s2n [ 1 .0.10 ] \u2502 \u2502 \u251c\u2500 libcurl [ 7 .86.0 ] \u2502 \u2502 \u2502 \u251c\u2500 krb5 [ 1 .19.3 ] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libedit [ 3 .1.20191231 ] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 keyutils [ 1 .6.1 ] \u2502 \u2502 \u2502 \u251c\u2500 libssh2 [ 1 .10.0 ] \u2502 \u2502 \u2502 \u2514\u2500 libnghttp2 [ 1 .47.0 ] \u2502 \u2502 \u2502 \u2514\u2500 libev [ 4 .33 ] \u2502 \u251c\u2500 lz4-c [ 1 .9.3 ] \u2502 \u251c\u2500 libthrift [ 0 .16.0 ] \u2502 \u2502 \u2514\u2500 libevent [ 2 .1.10 ] \u2502 \u251c\u2500 libutf8proc [ 2 .8.0 ] \u2502 \u251c\u2500 snappy [ 1 .1.9 ] \u2502 \u251c\u2500 re2 [ 2022 .06.01 ] \u2502 \u251c\u2500 glog [ 0 .6.0 ] \u2502 \u251c\u2500 libabseil [ 20220623 .0 ] \u2502 \u251c\u2500 libprotobuf [ 3 .21.10 ] \u2502 \u251c\u2500 orc [ 1 .8.0 ] \u2502 \u251c\u2500 libgrpc [ 1 .49.1 ] \u2502 \u2502 \u251c\u2500 zlib [ 1 .2.13 ] \u2502 \u2514\u2500 libgoogle-cloud [ 2 .3.0 ] \u2502 \u251c\u2500 libcrc32c [ 1 .1.2 ] \u251c\u2500 python_abi \u251c\u2500 python \u251c\u2500 numpy \u2514\u2500 arrow-cpp PyArrow vendored libraries (PyPI wheels) $ ls pyarrow/*.so # pyarrow/ is from an unzipped manylinux wheel /home/rgommers/Downloads/pyarrow/_compute.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_csv.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_dataset.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_dataset_orc.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_dataset_parquet.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_exec_plan.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_feather.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_flight.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_fs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_gcsfs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_hdfs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_hdfsio.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_json.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/libarrow_python_flight.so /home/rgommers/Downloads/pyarrow/libarrow_python.so /home/rgommers/Downloads/pyarrow/lib.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_orc.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_parquet.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_parquet_encryption.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_plasma.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_pyarrow_cpp_tests.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_s3fs.cpython-311-x86_64-linux-gnu.so /home/rgommers/Downloads/pyarrow/_substrait.cpython-311-x86_64-linux-gnu.so We see that Apache Parquet and Substrait , two separate projects, have to be vendored, and a lot of Apache Arrow C++ components are packed into a single wheel. What can't be seen from the wheel contents is that some of the extension modules were built with other complex dependencies like protobuf and glog (which therefore also need to be built as part of the wheel build process). Such dependencies raise the possibility of symbol conflicts when other packages are built with different versions of those libraries, or in different ways. This can result in hard to debug crashes or \"undefined symbol\" problems. This blog post by Uwe Korn describes some of the issues in detail, including problems installing PyArrow side by side with TensorFlow and Turbodbc. C++ ABI has been an issue for quite a while. Most C++ developers and projects want to use the new ABI, however due to the old manylinux standard and depending on devtoolset forces the use of the old C++ ABI ( _GLIBCXX_USE_CXX11_ABI=0 ). Projects using modern C++14/17 typically want to use the new ABI. This is still quite difficult. It's now possible with manylinux_2_28 , but requires building duplicate sets of wheels 1 (also manylinux2014 for compatibility with older distros like Ubuntu 18.04 and CentOS 8, that will still use the old ABI). As an example: PyTorch added libtorch builds with the new ABI to its own download server in 2019 already ( pytorch#17492 ), however the issue for matching wheels is still open ( pytorch#51039 ) as of Dec '22. An even thornier issue is the proliferation of Abseil in the API of projects like protobuf , given that Abseil's ABI is sensitive (by default) to the C++ standard version being used to compile it, and this needs to be held consistent (essentially) across the entire ecosystem. For more details, see here .","title":"Current state"},{"location":"key-issues/native-dependencies/cpp_deps/#problems","text":"Requirement for a wheel being completely self-contained, forcing vendoring of external C++ dependencies. Building external dependencies is a lot of effort, and error prone. It also prevents splitting up a wheel into multiple dependent ones. This may be desirable because of binary size or maintainability. The old C++ ABI still being the default. Symbol clashes when different libraries vendor the same external dependency.","title":"Problems"},{"location":"key-issues/native-dependencies/cpp_deps/#history","text":"Early history TODO. The RAPIDS projects was forced to drop wheels completely from May 2019 to Oct 2022, because the manylinux required a too old C++ version, and made it impossible to create compliant wheels with the RAPIDS C++14 code base. See this blog post for details . Apache Arrow's issues with wheels and the amount of effort they take were laid out in detail in this mailing list post by Wes McKinney . This long discussion on pypa/packaging-problems#25 touches on many of the key pain points around wheels and projects with complex C/C++ dependencies (interspersed with some \"packaging politics\").","title":"History"},{"location":"key-issues/native-dependencies/cpp_deps/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"key-issues/native-dependencies/cpp_deps/#potential-solutions-or-mitigations","text":"Better interaction/integration between PyPI/wheels/pip and other package managers, where dealing with C++ dependencies is easier. ... ? See manylinux#1332 and pytorch#51039 for details. \u21a9","title":"Potential solutions or mitigations"},{"location":"key-issues/native-dependencies/geospatial_stack/","text":"The Geospatial stack Python users have a rich set of packages for geospatial data I/O, manipulation, analytics and visualization available to them. Those include xarray , Shapely , Geopandas , Rasterio , Fiona , GDAL , pyproj , PySAL , Folium , Geoviews , and descartes . Users typically use multiple of these packages together. It has always been difficult to set up a working Python environment for that. Especially when installing from PyPI. Current state The foundation for all of the Python geospatial packages are a set of native libraries, in particular GDAL (C/C++), PROJ (C++), libspatialindex (C++), and libtiff (C). These libraries are difficult to build, GDAL in particular (it has a long list of other native dependencies , some mandatory and some optional). When there are multiple fundamental C/C++-only libraries and multiple consumers of those libraries, there is a problem. The PyPI/wheels design requires that each Python package rebuilds all those C libraries and vendors them. This is (a) a hard job for any individual package author, (b) requires coordination in order not to end up with different versions of vendored libraries, and (c) a big enough hurdle in practice that Python package authors have not been able to solve the problems. To illustrate that, this is the warning displayed in the Geopandas documentations ( v0.12.2 install page ) for installing with pip : Warning When using pip to install GeoPandas, you need to make sure that all dependencies are installed correctly. fiona provides binary wheels with the dependencies included for Mac and Linux, but not for Windows. Alternatively, you can install pyogrio which does have wheels for Windows. pyproj , rtree , and shapely provide binary wheels with dependencies included for Mac, Linux, and Windows. Depending on your platform, you might need to compile and install their C dependencies manually. We refer to the individual packages for more details on installing those. Using conda (see above) avoids the need to compile the dependencies yourself. The description tells a clear story: there are four dependencies with native code, and those then have other native dependencies that may not be included . Why weren't those dependencies all vendored? Likely because it was simply too hard - building for example GDAL correctly is notoriously difficult. Also, while GDAL is a large C/C++ library, it has a Python API and is present on PyPI but does not provide wheels ( its PyPI project description recommends using conda). That brings up a question - should other packages express a dependency on GDAL, knowing it probably won't build, or try to vendor it in their own wheels? 1 We can use conda-forge to look at the full dependency tree for Geopandas, which shows how many native dependencies this one pure Python package has: Geopandas dependency tree This is the dependency tree when installing only geopandas from conda-forge (duplicate entries removed from tree): $ mamba create -n geo-env geopandas $ mamba activate geo-env $ mamba repoquery depends geopandas --tree geopandas[0.12.2] \u251c\u2500 fiona[1.8.22] \u2502 \u251c\u2500 attrs[22.1.0] \u2502 \u2502 \u2514\u2500 python[3.11.0] \u2502 \u2502 \u251c\u2500 bzip2[1.0.8] \u2502 \u2502 \u2502 \u2514\u2500 libgcc-ng[12.2.0] \u2502 \u2502 \u2502 \u251c\u2500 _libgcc_mutex[0.1] \u2502 \u2502 \u2502 \u2514\u2500 _openmp_mutex[4.5] \u2502 \u2502 \u2502 \u2514\u2500 libgomp[12.2.0] \u2502 \u2502 \u251c\u2500 ld_impl_linux-64[2.39] \u2502 \u2502 \u251c\u2500 libffi[3.4.2] \u2502 \u2502 \u251c\u2500 libnsl[2.0.0] \u2502 \u2502 \u251c\u2500 libsqlite[3.40.0] \u2502 \u2502 \u2502 \u2514\u2500 libzlib[1.2.13] \u2502 \u2502 \u251c\u2500 libuuid[2.32.1] \u2502 \u2502 \u251c\u2500 ncurses[6.3] \u2502 \u2502 \u251c\u2500 openssl[3.0.7] \u2502 \u2502 \u2502 \u251c\u2500 ca-certificates[2022.12.7] \u2502 \u2502 \u251c\u2500 readline[8.1.2] \u2502 \u2502 \u251c\u2500 tk[8.6.12] \u2502 \u2502 \u251c\u2500 tzdata[2022g] \u2502 \u2502 \u251c\u2500 xz[5.2.6] \u2502 \u2502 \u2514\u2500 pip[22.3.1] \u2502 \u2502 \u251c\u2500 setuptools[65.5.1] \u2502 \u2502 \u251c\u2500 wheel[0.38.4] \u2502 \u251c\u2500 click[8.1.3] \u2502 \u251c\u2500 click-plugins[1.1.1] \u2502 \u251c\u2500 cligj[0.7.2] \u2502 \u251c\u2500 gdal[3.6.1] \u2502 \u2502 \u251c\u2500 hdf5[1.12.2] \u2502 \u2502 \u2502 \u251c\u2500 libcurl[7.86.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 krb5[1.20.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 keyutils[1.6.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libedit[3.1.20191231] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libstdcxx-ng[12.2.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libnghttp2[1.47.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 c-ares[1.18.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libev[4.33] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libssh2[1.10.0] \u2502 \u2502 \u2502 \u251c\u2500 libgfortran-ng[12.2.0] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 libgfortran5[12.2.0] \u2502 \u2502 \u251c\u2500 libgdal[3.6.1] \u2502 \u2502 \u2502 \u251c\u2500 blosc[1.21.3] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 lz4-c[1.9.3] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 snappy[1.1.9] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 zstd[1.5.2] \u2502 \u2502 \u2502 \u251c\u2500 cfitsio[4.2.0] \u2502 \u2502 \u2502 \u251c\u2500 expat[2.5.0] \u2502 \u2502 \u2502 \u251c\u2500 freexl[1.0.6] \u2502 \u2502 \u2502 \u251c\u2500 geos[3.11.1] \u2502 \u2502 \u2502 \u251c\u2500 geotiff[1.7.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 jpeg[9e] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libtiff[4.4.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 lerc[4.0.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libdeflate[1.14] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libwebp-base[1.2.4] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 proj[9.1.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 sqlite[3.40.0] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 zlib[1.2.13] \u2502 \u2502 \u2502 \u251c\u2500 giflib[5.2.1] \u2502 \u2502 \u2502 \u251c\u2500 hdf4[4.2.15] \u2502 \u2502 \u2502 \u251c\u2500 icu[70.1] \u2502 \u2502 \u2502 \u251c\u2500 json-c[0.16] \u2502 \u2502 \u2502 \u251c\u2500 kealib[1.5.0] \u2502 \u2502 \u2502 \u251c\u2500 libiconv[1.17] \u2502 \u2502 \u2502 \u251c\u2500 libkml[1.3.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 boost-cpp[1.78.0] \u2502 \u2502 \u2502 \u251c\u2500 libnetcdf[4.8.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 curl[7.86.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libxml2[2.10.3] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libzip[1.9.2] \u2502 \u2502 \u2502 \u251c\u2500 libpng[1.6.39] \u2502 \u2502 \u2502 \u251c\u2500 libpq[15.1] \u2502 \u2502 \u2502 \u251c\u2500 libspatialite[5.0.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 librttopo[1.1.0] \u2502 \u2502 \u2502 \u251c\u2500 openjpeg[2.5.0] \u2502 \u2502 \u2502 \u251c\u2500 pcre2[10.40] \u2502 \u2502 \u2502 \u251c\u2500 poppler[22.12.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 cairo[1.16.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 fontconfig[2.14.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 freetype[2.12.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 fonts-conda-ecosystem[1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 fonts-conda-forge[1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 font-ttf-dejavu-sans-mono[2.37] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 font-ttf-inconsolata[3.000] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 font-ttf-source-code-pro[2.038] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 font-ttf-ubuntu[0.83] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libglib[2.74.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 gettext[0.21.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libxcb[1.13] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 pthread-stubs[0.4] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libxau[1.0.9] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-libxdmcp[1.1.3] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 pixman[0.40.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libice[1.0.10] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libsm[1.2.3] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libx11[1.7.2] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-kbproto[1.0.7] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-xproto[7.0.31] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libxext[1.3.4] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-xextproto[7.3.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libxrender[0.9.10] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-renderproto[0.11.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 lcms2[2.14] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 nss[3.82] \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 nspr[4.35] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 poppler-data[0.4.11] \u2502 \u2502 \u2502 \u251c\u2500 postgresql[15.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 tzcode[2022g] \u2502 \u2502 \u2502 \u251c\u2500 qhull[2020.2] \u2502 \u2502 \u2502 \u251c\u2500 tiledb[2.13.0] \u2502 \u2502 \u2502 \u251c\u2500 xerces-c[3.2.4] \u2502 \u2502 \u251c\u2500 numpy[1.23.5] \u2502 \u2502 \u2502 \u251c\u2500 libblas[3.9.0] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 libopenblas[0.3.21] \u2502 \u2502 \u2502 \u251c\u2500 libcblas[3.9.0] \u2502 \u2502 \u2502 \u251c\u2500 liblapack[3.9.0] \u2502 \u2502 \u2502 \u2514\u2500 python_abi[3.11] \u2502 \u251c\u2500 munch[2.5.0] \u2502 \u2502 \u2514\u2500 six[1.16.0] \u2502 \u251c\u2500 shapely[2.0.0] \u251c\u2500 folium[0.14.0] \u2502 \u251c\u2500 branca[0.6.0] \u2502 \u2502 \u251c\u2500 jinja2[3.1.2] \u2502 \u2502 \u2502 \u251c\u2500 markupsafe[2.1.1] \u2502 \u2514\u2500 requests[2.28.1] \u2502 \u251c\u2500 certifi[2022.12.7] \u2502 \u251c\u2500 charset-normalizer[2.1.1] \u2502 \u251c\u2500 idna[3.4] \u2502 \u2514\u2500 urllib3[1.26.13] \u2502 \u251c\u2500 brotlipy[0.7.0] \u2502 \u2502 \u251c\u2500 cffi[1.15.1] \u2502 \u2502 \u2502 \u251c\u2500 pycparser[2.21] \u2502 \u251c\u2500 cryptography[38.0.4] \u2502 \u251c\u2500 pyopenssl[22.1.0] \u2502 \u251c\u2500 pysocks[1.7.1] \u251c\u2500 geopandas-base[0.12.2] \u2502 \u251c\u2500 packaging[22.0] \u2502 \u251c\u2500 pandas[1.5.2] \u2502 \u2502 \u251c\u2500 python-dateutil[2.8.2] \u2502 \u2502 \u2514\u2500 pytz[2022.6] \u2502 \u251c\u2500 pyproj[3.4.1] \u251c\u2500 mapclassify[2.4.3] \u2502 \u251c\u2500 networkx[2.8.8] \u2502 \u251c\u2500 scikit-learn[1.2.0] \u2502 \u2502 \u251c\u2500 joblib[1.2.0] \u2502 \u2502 \u251c\u2500 scipy[1.9.3] \u2502 \u2502 \u2514\u2500 threadpoolctl[3.1.0] \u251c\u2500 matplotlib-base[3.6.2] \u2502 \u251c\u2500 contourpy[1.0.6] \u2502 \u251c\u2500 cycler[0.11.0] \u2502 \u251c\u2500 fonttools[4.38.0] \u2502 \u2502 \u251c\u2500 brotli[1.0.9] \u2502 \u2502 \u2502 \u251c\u2500 brotli-bin[1.0.9] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libbrotlidec[1.0.9] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libbrotlicommon[1.0.9] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libbrotlienc[1.0.9] \u2502 \u2502 \u251c\u2500 munkres[1.1.4] \u2502 \u251c\u2500 kiwisolver[1.4.4] \u2502 \u251c\u2500 pillow[9.2.0] \u2502 \u251c\u2500 pyparsing[3.0.9] \u251c\u2500 rtree[1.0.1] \u2502 \u251c\u2500 libspatialindex[1.9.3] \u2514\u2500 xyzservices[2022.9.0] Building all those libraries in a consistent fashion as a set of wheels is effectively infeasible, due to some of the \"meta topics\" around packaging on PyPI ( the author-led social model , lack of a build farm ). And that's just the tip of the iceberg. Geopandas is positioned relatively low in the stack: libraries.io tells us that it has ~660 dependent packages on PyPI, quite a few of which are quite popular themselves. Problems The whole geospatial stack is effectively not usable from PyPI, and most projects and domain-specific user guides recommend using Conda. Multiple native libraries as a foundation requires coordination and shared build infrastructure. As a specific domain, geospatial projects have less energy/people available to solve problems compared to the largest projects in the PyData space. History TODO Relevant resources TODO Potential solutions or mitigations The current state - the geospatial stack being almost uninstallable from PyPI, and projects and users mostly using conda-forge - is hard to improve upon unless some of the meta issues with PyPI that are the root causes of that current state get addressed. The most effective direction forward is probably to acknowledge that this use case is too difficult for PyPI, and to improve the interoperability between PyPI/Pip and system package managers. GDAL seems to be using the \"flow of source code to distributors\" function of PyPI here (see \"purposes of PyPI\" ). Unofficial wheels used to be provided by Christoph Gohlke on his website . \u21a9","title":"The Geospatial stack"},{"location":"key-issues/native-dependencies/geospatial_stack/#the-geospatial-stack","text":"Python users have a rich set of packages for geospatial data I/O, manipulation, analytics and visualization available to them. Those include xarray , Shapely , Geopandas , Rasterio , Fiona , GDAL , pyproj , PySAL , Folium , Geoviews , and descartes . Users typically use multiple of these packages together. It has always been difficult to set up a working Python environment for that. Especially when installing from PyPI.","title":"The Geospatial stack"},{"location":"key-issues/native-dependencies/geospatial_stack/#current-state","text":"The foundation for all of the Python geospatial packages are a set of native libraries, in particular GDAL (C/C++), PROJ (C++), libspatialindex (C++), and libtiff (C). These libraries are difficult to build, GDAL in particular (it has a long list of other native dependencies , some mandatory and some optional). When there are multiple fundamental C/C++-only libraries and multiple consumers of those libraries, there is a problem. The PyPI/wheels design requires that each Python package rebuilds all those C libraries and vendors them. This is (a) a hard job for any individual package author, (b) requires coordination in order not to end up with different versions of vendored libraries, and (c) a big enough hurdle in practice that Python package authors have not been able to solve the problems. To illustrate that, this is the warning displayed in the Geopandas documentations ( v0.12.2 install page ) for installing with pip : Warning When using pip to install GeoPandas, you need to make sure that all dependencies are installed correctly. fiona provides binary wheels with the dependencies included for Mac and Linux, but not for Windows. Alternatively, you can install pyogrio which does have wheels for Windows. pyproj , rtree , and shapely provide binary wheels with dependencies included for Mac, Linux, and Windows. Depending on your platform, you might need to compile and install their C dependencies manually. We refer to the individual packages for more details on installing those. Using conda (see above) avoids the need to compile the dependencies yourself. The description tells a clear story: there are four dependencies with native code, and those then have other native dependencies that may not be included . Why weren't those dependencies all vendored? Likely because it was simply too hard - building for example GDAL correctly is notoriously difficult. Also, while GDAL is a large C/C++ library, it has a Python API and is present on PyPI but does not provide wheels ( its PyPI project description recommends using conda). That brings up a question - should other packages express a dependency on GDAL, knowing it probably won't build, or try to vendor it in their own wheels? 1 We can use conda-forge to look at the full dependency tree for Geopandas, which shows how many native dependencies this one pure Python package has: Geopandas dependency tree This is the dependency tree when installing only geopandas from conda-forge (duplicate entries removed from tree): $ mamba create -n geo-env geopandas $ mamba activate geo-env $ mamba repoquery depends geopandas --tree geopandas[0.12.2] \u251c\u2500 fiona[1.8.22] \u2502 \u251c\u2500 attrs[22.1.0] \u2502 \u2502 \u2514\u2500 python[3.11.0] \u2502 \u2502 \u251c\u2500 bzip2[1.0.8] \u2502 \u2502 \u2502 \u2514\u2500 libgcc-ng[12.2.0] \u2502 \u2502 \u2502 \u251c\u2500 _libgcc_mutex[0.1] \u2502 \u2502 \u2502 \u2514\u2500 _openmp_mutex[4.5] \u2502 \u2502 \u2502 \u2514\u2500 libgomp[12.2.0] \u2502 \u2502 \u251c\u2500 ld_impl_linux-64[2.39] \u2502 \u2502 \u251c\u2500 libffi[3.4.2] \u2502 \u2502 \u251c\u2500 libnsl[2.0.0] \u2502 \u2502 \u251c\u2500 libsqlite[3.40.0] \u2502 \u2502 \u2502 \u2514\u2500 libzlib[1.2.13] \u2502 \u2502 \u251c\u2500 libuuid[2.32.1] \u2502 \u2502 \u251c\u2500 ncurses[6.3] \u2502 \u2502 \u251c\u2500 openssl[3.0.7] \u2502 \u2502 \u2502 \u251c\u2500 ca-certificates[2022.12.7] \u2502 \u2502 \u251c\u2500 readline[8.1.2] \u2502 \u2502 \u251c\u2500 tk[8.6.12] \u2502 \u2502 \u251c\u2500 tzdata[2022g] \u2502 \u2502 \u251c\u2500 xz[5.2.6] \u2502 \u2502 \u2514\u2500 pip[22.3.1] \u2502 \u2502 \u251c\u2500 setuptools[65.5.1] \u2502 \u2502 \u251c\u2500 wheel[0.38.4] \u2502 \u251c\u2500 click[8.1.3] \u2502 \u251c\u2500 click-plugins[1.1.1] \u2502 \u251c\u2500 cligj[0.7.2] \u2502 \u251c\u2500 gdal[3.6.1] \u2502 \u2502 \u251c\u2500 hdf5[1.12.2] \u2502 \u2502 \u2502 \u251c\u2500 libcurl[7.86.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 krb5[1.20.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 keyutils[1.6.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libedit[3.1.20191231] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libstdcxx-ng[12.2.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libnghttp2[1.47.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 c-ares[1.18.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libev[4.33] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libssh2[1.10.0] \u2502 \u2502 \u2502 \u251c\u2500 libgfortran-ng[12.2.0] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 libgfortran5[12.2.0] \u2502 \u2502 \u251c\u2500 libgdal[3.6.1] \u2502 \u2502 \u2502 \u251c\u2500 blosc[1.21.3] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 lz4-c[1.9.3] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 snappy[1.1.9] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 zstd[1.5.2] \u2502 \u2502 \u2502 \u251c\u2500 cfitsio[4.2.0] \u2502 \u2502 \u2502 \u251c\u2500 expat[2.5.0] \u2502 \u2502 \u2502 \u251c\u2500 freexl[1.0.6] \u2502 \u2502 \u2502 \u251c\u2500 geos[3.11.1] \u2502 \u2502 \u2502 \u251c\u2500 geotiff[1.7.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 jpeg[9e] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libtiff[4.4.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 lerc[4.0.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libdeflate[1.14] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libwebp-base[1.2.4] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 proj[9.1.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 sqlite[3.40.0] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 zlib[1.2.13] \u2502 \u2502 \u2502 \u251c\u2500 giflib[5.2.1] \u2502 \u2502 \u2502 \u251c\u2500 hdf4[4.2.15] \u2502 \u2502 \u2502 \u251c\u2500 icu[70.1] \u2502 \u2502 \u2502 \u251c\u2500 json-c[0.16] \u2502 \u2502 \u2502 \u251c\u2500 kealib[1.5.0] \u2502 \u2502 \u2502 \u251c\u2500 libiconv[1.17] \u2502 \u2502 \u2502 \u251c\u2500 libkml[1.3.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 boost-cpp[1.78.0] \u2502 \u2502 \u2502 \u251c\u2500 libnetcdf[4.8.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 curl[7.86.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libxml2[2.10.3] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libzip[1.9.2] \u2502 \u2502 \u2502 \u251c\u2500 libpng[1.6.39] \u2502 \u2502 \u2502 \u251c\u2500 libpq[15.1] \u2502 \u2502 \u2502 \u251c\u2500 libspatialite[5.0.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 librttopo[1.1.0] \u2502 \u2502 \u2502 \u251c\u2500 openjpeg[2.5.0] \u2502 \u2502 \u2502 \u251c\u2500 pcre2[10.40] \u2502 \u2502 \u2502 \u251c\u2500 poppler[22.12.0] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 cairo[1.16.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 fontconfig[2.14.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 freetype[2.12.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 fonts-conda-ecosystem[1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 fonts-conda-forge[1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 font-ttf-dejavu-sans-mono[2.37] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 font-ttf-inconsolata[3.000] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 font-ttf-source-code-pro[2.038] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 font-ttf-ubuntu[0.83] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libglib[2.74.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 gettext[0.21.1] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libxcb[1.13] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 pthread-stubs[0.4] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libxau[1.0.9] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-libxdmcp[1.1.3] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 pixman[0.40.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libice[1.0.10] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libsm[1.2.3] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libx11[1.7.2] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-kbproto[1.0.7] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-xproto[7.0.31] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libxext[1.3.4] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-xextproto[7.3.0] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 xorg-libxrender[0.9.10] \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 xorg-renderproto[0.11.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 lcms2[2.14] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 nss[3.82] \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500 nspr[4.35] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 poppler-data[0.4.11] \u2502 \u2502 \u2502 \u251c\u2500 postgresql[15.1] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 tzcode[2022g] \u2502 \u2502 \u2502 \u251c\u2500 qhull[2020.2] \u2502 \u2502 \u2502 \u251c\u2500 tiledb[2.13.0] \u2502 \u2502 \u2502 \u251c\u2500 xerces-c[3.2.4] \u2502 \u2502 \u251c\u2500 numpy[1.23.5] \u2502 \u2502 \u2502 \u251c\u2500 libblas[3.9.0] \u2502 \u2502 \u2502 \u2502 \u2514\u2500 libopenblas[0.3.21] \u2502 \u2502 \u2502 \u251c\u2500 libcblas[3.9.0] \u2502 \u2502 \u2502 \u251c\u2500 liblapack[3.9.0] \u2502 \u2502 \u2502 \u2514\u2500 python_abi[3.11] \u2502 \u251c\u2500 munch[2.5.0] \u2502 \u2502 \u2514\u2500 six[1.16.0] \u2502 \u251c\u2500 shapely[2.0.0] \u251c\u2500 folium[0.14.0] \u2502 \u251c\u2500 branca[0.6.0] \u2502 \u2502 \u251c\u2500 jinja2[3.1.2] \u2502 \u2502 \u2502 \u251c\u2500 markupsafe[2.1.1] \u2502 \u2514\u2500 requests[2.28.1] \u2502 \u251c\u2500 certifi[2022.12.7] \u2502 \u251c\u2500 charset-normalizer[2.1.1] \u2502 \u251c\u2500 idna[3.4] \u2502 \u2514\u2500 urllib3[1.26.13] \u2502 \u251c\u2500 brotlipy[0.7.0] \u2502 \u2502 \u251c\u2500 cffi[1.15.1] \u2502 \u2502 \u2502 \u251c\u2500 pycparser[2.21] \u2502 \u251c\u2500 cryptography[38.0.4] \u2502 \u251c\u2500 pyopenssl[22.1.0] \u2502 \u251c\u2500 pysocks[1.7.1] \u251c\u2500 geopandas-base[0.12.2] \u2502 \u251c\u2500 packaging[22.0] \u2502 \u251c\u2500 pandas[1.5.2] \u2502 \u2502 \u251c\u2500 python-dateutil[2.8.2] \u2502 \u2502 \u2514\u2500 pytz[2022.6] \u2502 \u251c\u2500 pyproj[3.4.1] \u251c\u2500 mapclassify[2.4.3] \u2502 \u251c\u2500 networkx[2.8.8] \u2502 \u251c\u2500 scikit-learn[1.2.0] \u2502 \u2502 \u251c\u2500 joblib[1.2.0] \u2502 \u2502 \u251c\u2500 scipy[1.9.3] \u2502 \u2502 \u2514\u2500 threadpoolctl[3.1.0] \u251c\u2500 matplotlib-base[3.6.2] \u2502 \u251c\u2500 contourpy[1.0.6] \u2502 \u251c\u2500 cycler[0.11.0] \u2502 \u251c\u2500 fonttools[4.38.0] \u2502 \u2502 \u251c\u2500 brotli[1.0.9] \u2502 \u2502 \u2502 \u251c\u2500 brotli-bin[1.0.9] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libbrotlidec[1.0.9] \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libbrotlicommon[1.0.9] \u2502 \u2502 \u2502 \u2502 \u251c\u2500 libbrotlienc[1.0.9] \u2502 \u2502 \u251c\u2500 munkres[1.1.4] \u2502 \u251c\u2500 kiwisolver[1.4.4] \u2502 \u251c\u2500 pillow[9.2.0] \u2502 \u251c\u2500 pyparsing[3.0.9] \u251c\u2500 rtree[1.0.1] \u2502 \u251c\u2500 libspatialindex[1.9.3] \u2514\u2500 xyzservices[2022.9.0] Building all those libraries in a consistent fashion as a set of wheels is effectively infeasible, due to some of the \"meta topics\" around packaging on PyPI ( the author-led social model , lack of a build farm ). And that's just the tip of the iceberg. Geopandas is positioned relatively low in the stack: libraries.io tells us that it has ~660 dependent packages on PyPI, quite a few of which are quite popular themselves.","title":"Current state"},{"location":"key-issues/native-dependencies/geospatial_stack/#problems","text":"The whole geospatial stack is effectively not usable from PyPI, and most projects and domain-specific user guides recommend using Conda. Multiple native libraries as a foundation requires coordination and shared build infrastructure. As a specific domain, geospatial projects have less energy/people available to solve problems compared to the largest projects in the PyData space.","title":"Problems"},{"location":"key-issues/native-dependencies/geospatial_stack/#history","text":"TODO","title":"History"},{"location":"key-issues/native-dependencies/geospatial_stack/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"key-issues/native-dependencies/geospatial_stack/#potential-solutions-or-mitigations","text":"The current state - the geospatial stack being almost uninstallable from PyPI, and projects and users mostly using conda-forge - is hard to improve upon unless some of the meta issues with PyPI that are the root causes of that current state get addressed. The most effective direction forward is probably to acknowledge that this use case is too difficult for PyPI, and to improve the interoperability between PyPI/Pip and system package managers. GDAL seems to be using the \"flow of source code to distributors\" function of PyPI here (see \"purposes of PyPI\" ). Unofficial wheels used to be provided by Christoph Gohlke on his website . \u21a9","title":"Potential solutions or mitigations"},{"location":"meta-topics/build_steps_conceptual/","text":"Build & package management concepts and terminology Having a clear picture of how build systems, package and environment managers interact and using terminology that is well-defined is important when discussing any concrete issue around using native code/dependencies. This section attempts to provide such a picture. We'll start with build & packaging concepts and design; if terminology is unclear, please refer to the terminology section further down. Package/dependency management & build systems When using a dependency in a package at build time, we need it installed first. Installing and using are well separated: the former is done by the package manager, the latter by the build system. Package managers need metadata - typically as a separate metadata file in a specific format, including applicable version constraints - to be able to manage dependencies. This is done in similar ways across package managers. As an example, this is what that looks like for a package depending on pybind11 version 2.4.3 or later: PyPI/pip Conda Spack Homebrew Nix For PyPI we have a pyproject.toml file: [build-system] requires = [ \"pybind11>=2.4.3\" ] For Conda we have a meta.yaml file: requirements : build : - pybind11 >=2.4.3 For Spack we have a package.py file: from spack.package import * class PyMypkg ( PythonPackage ): depends_on ( \"py-pybind11@2.4.3:\" , when = \"@1.9.1:\" , type = ( \"build\" , \"link\" )) For Homebrew we have a pkgname.rb file: class Pkgname < Formula depends_on \"pybind11\" Note: no >=2.4.3 version constraint, because Homebrew provides only a single version per package. For Nix we have a default.nix file: { lib , pybind11 }: buildPythonPackage rec { buildInputs = [ pybind11 ]; Note: no >=2.4.3 version constraint, because Nix provides only a single version per package. At build time, the package manager is used to ensure that a dependency is installed before the build system is invoked. Inside the config files for the build system, the dependency can then be used: setuptools Meson CMake For setuptools we use a setup.py file with: import pybind11 incdir_pybind11 = pybind11 . get_include () ext_modules = [ Extension ( '_uses_pybind' , [ 'src/main.cpp' ], include_dirs = [ incdir_pybind11 ], ) ] For Meson we use a meson.build file with: incdir_pybind11 = run_command ( py , [ '-c' , 'import pybind11; print(pybind11.get_include())' ], check : true , ). stdout (). strip () inc_pybind11 = include_directories ( incdir_pybind11 ) py . extension_module ( '_uses_pybind' , [ 'src/main.cpp' ], include_directories : [ inc_pybind11 ], ) Note: the above should be replaced by this once pybind11 support is built-in: dep_pybind11 = dependency ( 'pybind11' ) py . extension_module ( '_uses_pybind' , [ 'src/main.cpp' ], dependencies : [ dep_pybind11 ] ) For CMake used through scikit-build we use a CMakeLists.txt file with: find_package ( pybind11 CONFIG REQUIRED ) pybind11_add_module ( _core MODULE src/main.cpp ) Important There are quite a few build systems and even more different package managers. These can be mixed and matched 1 2 - each build system can be used in any package manager. Types of package managers The word \"package manager\" is applied to a wide variety of tools. It is helpful to distinguish two different types of package managers. (A) System package managers : Single version of each package (so may not need a solver for dependency resolution) All libraries needed for a functioning system (sometimes modulo libc ) Multi-language Single-platform (often, not always) Examples: Linux package managers (APT, RPM, Pacman, YUM, etc.), Homebrew, Nix, Guix, Chocolatey (B) Dependency package managers : Multiple versions of each package at the same time (key ingredient: dependency resolution) Single language (often, not always) Multi-platform Examples: Pip, Poetry, NPM, Cargo, Conan, vcpkg, Pkg (Julia), RubyGems, NuGet Additionally we have (C) Environment managers : Allow using different sets of packages - possibly including compilers, language runtime, and package manager. Examples: venv, virtualenv, pyenv, nvm, RVM, nix-shell Important A key point to understand is that dependency package managers rely on a system package manager to provide packages that are not present in the dependency package manager's repository. See native dependencies . This classification isn't perfect. Some tools given as above touch more or less of another category they're in. Or they span multiple of the three categories and some don't quite fit. A few of the most prominent examples: Conda spans all three categories: it relies on libc from the OS, but includes all other \"system\" packages, supports multiple versions for all packages, and supports environments. Spack is quite similar to Conda, at least in this classification. The main differences are that (a) it defaults to building from source on the user system (with a binary cache system), and (b) it treats compilers differently; they can come from the system, but Spack does know about them and allows using multiple compilers in a controlled fashion. What about PDM, Hatch, Pipenv, Nox, Tox & co.? These are all more developer tools - they provide a UX for environment and dependency management, but rely on Pip, venv/virtualenv for dependency resolution and environment creation and (de)activation. Alternatively, one could say that they are dependency plus environment managers. The characteristics of their behavior will still be determined by the tools they wrap though, so for the purposes of discussing Python packaging design as it relates to dealing with native code it's not too useful to consider them separately. Building and installing or uploading artifacts Now that we have all the needed concepts for \"building a package\", let's look at how that works at a conceptual level. An artifact is an output of building a package - this can be an sdist, a wheel, or a binary in a different format. When a build gets triggered, either by a user or by an application or automated system, these are the phases of the process that follows: Steps taken by a build or install tool to produce a build artifact for a Python package and install or upload it. Those phases are more or less the same, independent of which package manager is used or whether we're building for the local system or for redistribution. Note that the build frontend and build backend may be involved in one or more places. Their role is to trigger the package manager and build system, and handle the metadata of the package appropriately and communicate it to the package manager and build system. The details of every step above may be different. Here this is worked out for three concrete cases: (a) installing an sdist from PyPI, (b) building a wheel for redistribution on PyPI, and (c) building a conda package for redistribution on conda-forge: Examples of concrete steps taken for three scenarios: installing an sdist from PyPI, building a wheel to distribute on PyPI, and building a conda package to distribute on conda-forge. Terminology Build system - the tool responsible for \"building\" a package. Note that there can be multiple interpretations of what that actually means. Here we take the operational definition that it is the tool for which you write your build configuration files. So: setup.py -> setuptools CMakelists.txt -> CMake meson.build -> Meson Makefile -> make SConstruct -> SCons This definition is probably the most commonly used one across technologies and language ecosystems, and can be applied irrespective of whether or not a package contains native code 3 , or whether or not the build system invokes a compiler directly or via a backend (e.g. CMake and Meson may use Ninja for that). Build frontend - the tool that is invoked by a user or by another tool to build a wheel or sdist (e.g., pip or pypa/build ). See PEP 517 for a more detailed description. Build backend - the tool that is invoked, via the build-backend hook in pyproject.toml , by a build frontend and is responsible for returning a wheel or sdist to that frontend (e.g., setuptools , Flit , meson-python , scikit-build-core or hatch ). The build backend may be part of a build system, or it may be a fairly thin layer in between the build frontend and the actual build system. Installer - a tool that accepts a set of package requirements, and installs these packages on the user's system. Note that \"installer\" is named \"integration frontend\" in PEP 517. The latter term does not seem to be commonly used. Problems One problem here is social. The Python packaging community tends to think in terms of PyPI, wheels and sdists almost exclusively. With a lot of focus on build frontends and build backends. For the process of building a binary, a wheel is not that different from an RPM, Debian or Conda package. Those often go through a wheel as an intermediate step, and use tools like pip . Packaging terminology and tool design should make room for this - wheels are not that special, they're a zip file containing a built package plus its metadata. The most important thing about wheels are the specifications around how the binaries inside them should be built and on what systems they are expected to work. In terms of the build process itself, what the build system does is by far the most time-consuming and complex part of producing a binary. A build frontend only has set up a clean build environment 4 and to invoke a backend, passing along CLI options if given. A build backend is a thin layer, whose main purpose is to invoke the build system, and then add the relevant metadata to the final wheel. There are lots of practical issues not discussed on this page (e.g., wheel tags are poorly documented and are a constant source of bugs and maintenance effort), however those can be dealt with one by one. That is outside the scope of this \"meta topic\". History TODO Relevant resources TODO Potential solutions or mitigations Better documentation and more education around how build & packaging works. ... ? All-in-one systems like Bazel, Buck and Pants are the exception. Those are primarily used inside large companies where a single way of working is enforced, either through a monorepo or other tooling and ways of working. The build system parts of such systems can still be integrated with other package managers (e.g., TensorFlow is built with Bazel and distributed as wheels and conda packages) - but that is typically challenging. We won't consider that further here. \u21a9 This is not true for the packages/libraries themselves though, which in the vast majority of cases have only a single build system. Exceptions include: TODO SCIPY PYTORCH \u21a9 Note that in very simple pure Python cases, the \"build configuration\" may be no more than static metadata that is all contained in [tool.build-system-name] table in pyproject.toml . \u21a9 Often a build frontend does not have to set up a build environment. By default pip and pypa/build do set one up, but disabling build isolation with --no-build-isolation or --no-isolation is quite important, both for local development and for building artifacts other than wheels for redistribution. Whether or not to default to build isolation was a long debate, and there are pros and cons either way. \u21a9","title":"Build & package management concepts and terminology"},{"location":"meta-topics/build_steps_conceptual/#build-package-management-concepts-and-terminology","text":"Having a clear picture of how build systems, package and environment managers interact and using terminology that is well-defined is important when discussing any concrete issue around using native code/dependencies. This section attempts to provide such a picture. We'll start with build & packaging concepts and design; if terminology is unclear, please refer to the terminology section further down.","title":"Build &amp; package management concepts and terminology"},{"location":"meta-topics/build_steps_conceptual/#packagedependency-management-build-systems","text":"When using a dependency in a package at build time, we need it installed first. Installing and using are well separated: the former is done by the package manager, the latter by the build system. Package managers need metadata - typically as a separate metadata file in a specific format, including applicable version constraints - to be able to manage dependencies. This is done in similar ways across package managers. As an example, this is what that looks like for a package depending on pybind11 version 2.4.3 or later: PyPI/pip Conda Spack Homebrew Nix For PyPI we have a pyproject.toml file: [build-system] requires = [ \"pybind11>=2.4.3\" ] For Conda we have a meta.yaml file: requirements : build : - pybind11 >=2.4.3 For Spack we have a package.py file: from spack.package import * class PyMypkg ( PythonPackage ): depends_on ( \"py-pybind11@2.4.3:\" , when = \"@1.9.1:\" , type = ( \"build\" , \"link\" )) For Homebrew we have a pkgname.rb file: class Pkgname < Formula depends_on \"pybind11\" Note: no >=2.4.3 version constraint, because Homebrew provides only a single version per package. For Nix we have a default.nix file: { lib , pybind11 }: buildPythonPackage rec { buildInputs = [ pybind11 ]; Note: no >=2.4.3 version constraint, because Nix provides only a single version per package. At build time, the package manager is used to ensure that a dependency is installed before the build system is invoked. Inside the config files for the build system, the dependency can then be used: setuptools Meson CMake For setuptools we use a setup.py file with: import pybind11 incdir_pybind11 = pybind11 . get_include () ext_modules = [ Extension ( '_uses_pybind' , [ 'src/main.cpp' ], include_dirs = [ incdir_pybind11 ], ) ] For Meson we use a meson.build file with: incdir_pybind11 = run_command ( py , [ '-c' , 'import pybind11; print(pybind11.get_include())' ], check : true , ). stdout (). strip () inc_pybind11 = include_directories ( incdir_pybind11 ) py . extension_module ( '_uses_pybind' , [ 'src/main.cpp' ], include_directories : [ inc_pybind11 ], ) Note: the above should be replaced by this once pybind11 support is built-in: dep_pybind11 = dependency ( 'pybind11' ) py . extension_module ( '_uses_pybind' , [ 'src/main.cpp' ], dependencies : [ dep_pybind11 ] ) For CMake used through scikit-build we use a CMakeLists.txt file with: find_package ( pybind11 CONFIG REQUIRED ) pybind11_add_module ( _core MODULE src/main.cpp ) Important There are quite a few build systems and even more different package managers. These can be mixed and matched 1 2 - each build system can be used in any package manager.","title":"Package/dependency management &amp; build systems"},{"location":"meta-topics/build_steps_conceptual/#types-of-package-managers","text":"The word \"package manager\" is applied to a wide variety of tools. It is helpful to distinguish two different types of package managers. (A) System package managers : Single version of each package (so may not need a solver for dependency resolution) All libraries needed for a functioning system (sometimes modulo libc ) Multi-language Single-platform (often, not always) Examples: Linux package managers (APT, RPM, Pacman, YUM, etc.), Homebrew, Nix, Guix, Chocolatey (B) Dependency package managers : Multiple versions of each package at the same time (key ingredient: dependency resolution) Single language (often, not always) Multi-platform Examples: Pip, Poetry, NPM, Cargo, Conan, vcpkg, Pkg (Julia), RubyGems, NuGet Additionally we have (C) Environment managers : Allow using different sets of packages - possibly including compilers, language runtime, and package manager. Examples: venv, virtualenv, pyenv, nvm, RVM, nix-shell Important A key point to understand is that dependency package managers rely on a system package manager to provide packages that are not present in the dependency package manager's repository. See native dependencies . This classification isn't perfect. Some tools given as above touch more or less of another category they're in. Or they span multiple of the three categories and some don't quite fit. A few of the most prominent examples: Conda spans all three categories: it relies on libc from the OS, but includes all other \"system\" packages, supports multiple versions for all packages, and supports environments. Spack is quite similar to Conda, at least in this classification. The main differences are that (a) it defaults to building from source on the user system (with a binary cache system), and (b) it treats compilers differently; they can come from the system, but Spack does know about them and allows using multiple compilers in a controlled fashion. What about PDM, Hatch, Pipenv, Nox, Tox & co.? These are all more developer tools - they provide a UX for environment and dependency management, but rely on Pip, venv/virtualenv for dependency resolution and environment creation and (de)activation. Alternatively, one could say that they are dependency plus environment managers. The characteristics of their behavior will still be determined by the tools they wrap though, so for the purposes of discussing Python packaging design as it relates to dealing with native code it's not too useful to consider them separately.","title":"Types of package managers"},{"location":"meta-topics/build_steps_conceptual/#building-and-installing-or-uploading-artifacts","text":"Now that we have all the needed concepts for \"building a package\", let's look at how that works at a conceptual level. An artifact is an output of building a package - this can be an sdist, a wheel, or a binary in a different format. When a build gets triggered, either by a user or by an application or automated system, these are the phases of the process that follows: Steps taken by a build or install tool to produce a build artifact for a Python package and install or upload it. Those phases are more or less the same, independent of which package manager is used or whether we're building for the local system or for redistribution. Note that the build frontend and build backend may be involved in one or more places. Their role is to trigger the package manager and build system, and handle the metadata of the package appropriately and communicate it to the package manager and build system. The details of every step above may be different. Here this is worked out for three concrete cases: (a) installing an sdist from PyPI, (b) building a wheel for redistribution on PyPI, and (c) building a conda package for redistribution on conda-forge: Examples of concrete steps taken for three scenarios: installing an sdist from PyPI, building a wheel to distribute on PyPI, and building a conda package to distribute on conda-forge.","title":"Building and installing or uploading artifacts"},{"location":"meta-topics/build_steps_conceptual/#terminology","text":"Build system - the tool responsible for \"building\" a package. Note that there can be multiple interpretations of what that actually means. Here we take the operational definition that it is the tool for which you write your build configuration files. So: setup.py -> setuptools CMakelists.txt -> CMake meson.build -> Meson Makefile -> make SConstruct -> SCons This definition is probably the most commonly used one across technologies and language ecosystems, and can be applied irrespective of whether or not a package contains native code 3 , or whether or not the build system invokes a compiler directly or via a backend (e.g. CMake and Meson may use Ninja for that). Build frontend - the tool that is invoked by a user or by another tool to build a wheel or sdist (e.g., pip or pypa/build ). See PEP 517 for a more detailed description. Build backend - the tool that is invoked, via the build-backend hook in pyproject.toml , by a build frontend and is responsible for returning a wheel or sdist to that frontend (e.g., setuptools , Flit , meson-python , scikit-build-core or hatch ). The build backend may be part of a build system, or it may be a fairly thin layer in between the build frontend and the actual build system. Installer - a tool that accepts a set of package requirements, and installs these packages on the user's system. Note that \"installer\" is named \"integration frontend\" in PEP 517. The latter term does not seem to be commonly used.","title":"Terminology"},{"location":"meta-topics/build_steps_conceptual/#problems","text":"One problem here is social. The Python packaging community tends to think in terms of PyPI, wheels and sdists almost exclusively. With a lot of focus on build frontends and build backends. For the process of building a binary, a wheel is not that different from an RPM, Debian or Conda package. Those often go through a wheel as an intermediate step, and use tools like pip . Packaging terminology and tool design should make room for this - wheels are not that special, they're a zip file containing a built package plus its metadata. The most important thing about wheels are the specifications around how the binaries inside them should be built and on what systems they are expected to work. In terms of the build process itself, what the build system does is by far the most time-consuming and complex part of producing a binary. A build frontend only has set up a clean build environment 4 and to invoke a backend, passing along CLI options if given. A build backend is a thin layer, whose main purpose is to invoke the build system, and then add the relevant metadata to the final wheel. There are lots of practical issues not discussed on this page (e.g., wheel tags are poorly documented and are a constant source of bugs and maintenance effort), however those can be dealt with one by one. That is outside the scope of this \"meta topic\".","title":"Problems"},{"location":"meta-topics/build_steps_conceptual/#history","text":"TODO","title":"History"},{"location":"meta-topics/build_steps_conceptual/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"meta-topics/build_steps_conceptual/#potential-solutions-or-mitigations","text":"Better documentation and more education around how build & packaging works. ... ? All-in-one systems like Bazel, Buck and Pants are the exception. Those are primarily used inside large companies where a single way of working is enforced, either through a monorepo or other tooling and ways of working. The build system parts of such systems can still be integrated with other package managers (e.g., TensorFlow is built with Bazel and distributed as wheels and conda packages) - but that is typically challenging. We won't consider that further here. \u21a9 This is not true for the packages/libraries themselves though, which in the vast majority of cases have only a single build system. Exceptions include: TODO SCIPY PYTORCH \u21a9 Note that in very simple pure Python cases, the \"build configuration\" may be no more than static metadata that is all contained in [tool.build-system-name] table in pyproject.toml . \u21a9 Often a build frontend does not have to set up a build environment. By default pip and pypa/build do set one up, but disabling build isolation with --no-build-isolation or --no-isolation is quite important, both for local development and for building artifacts other than wheels for redistribution. Whether or not to default to build isolation was a long debate, and there are pros and cons either way. \u21a9","title":"Potential solutions or mitigations"},{"location":"meta-topics/no_build_farm/","text":"Lack of a build farm for PyPI There is no centralized service (or \"build farm\") for building wheels for packages on PyPI. This is not for lack of agreement on whether that is desirable or not (it's the top item on the PSF's fundable packaging improvements ), but because it's a huge amount of work to create and maintain such a service. Benefits of a build farm include: Easier to build wheels, which means less effort for package authors and a higher percentage of packages having wheels available, Ability to use a consistent build environment, leading to higher quality wheels and fewer issues with binary compatibility between different packages (see, e.g., managing ABI compatibility and complex C++ dependencies ), Ability to roll out upgrades to compiler toolchains or rebuilds of many packages at once (e.g., for a new Python version), Detecting of some classes of issues with releases at upload-to-PyPI time rather than when users open bug reports (e.g., ensure all .py files in a package can actually be imported), Opportunity to grow a packaging community, with collective expertise, resources and policies. Two package repositories that are fairly similar to PyPI, CPAN for Perl and CRAN for R, both don't allow hosting packages on the repository unless the package passes a series of tests. This includes tests that attempt to build the library in an isolated environment to ensure that all dependencies are correctly listed, and running unit tests. From anecdotal evidence, the average quality of those packages is higher, and it's easier to redistribute them in other packaging systems. Pretty much all system package managers as well as Conda and Spack follow similar designs - at least a build should pass and the package should be importable. Current state This topic has been discussed on and of, however there is no concrete effort happening in this direction as of Dec 2022. One early attempt at a solution was conda-press , which aimed to repackage conda packages into wheels, has not been updated since 2019. There were some unresolved design questions or issues with it, and it is unclear if the approach would result in problem-free wheels for the more complex packages. Even if conda-press isn't going anywhere, the conda-forge infrastructure is perhaps the most suitable infrastructure to use as a base to create a new build farm from. It would still be a lot of work to adapt it to PyPI - but far less than starting from scratch. A build farm needs maintainers and its own community around it. That community doesn't exist, and bootstrapping it isn't easy. However, there are places where maintainers have coalesced around common tooling. cibuildwheel is probably the most central point. Now that multibuild is abandoned, cibuildwheel is being used to build wheels on CI systems that are free for open source projects to use by many of the most popular projects with native code. The cost of a build farm Build farms come with substantial-to-massive costs in terms of build & hosting infrastructure, automation that needs to be maintained, and generally human attention/intervention for cases that go wrong. These costs already appear for the above-mentioned \"build and verify\" model used for CRAN. However, when wishing to follow the widely-used practice among distributions to rely predominantly on shared libraries, this brings a massive further increase in necessary automation and maintenance (though the benefits are such that this is the norm rather than the exception). In particular, doing so needs careful tracking which packages are exposed to any given (shared) library's ABI, the ABI stability of that library as expressed through its version scheme, rebuilding all dependent packages once an ABI-changing library version is released, and finally representing all that accurately enough in metadata so that the resolver will only choose compatible package combinations for a given environment. Due to the limitation of only having a single shared library on the path searched by the linker for symbols at runtime (see here for more details), this kind of ecosystem-wide rebuild needs to be done relatively quickly. This is because any given package that has a release in the meantime can only be compiled for either the old or for the new ABI (with very few exceptions for transitions that are more onerous, otherwise one quickly suffers a combinatorial explosion of required build variants), and not moving the ecosystem as a whole to the new baseline essentially means a bifurcation which packages can be co-installed with each other. For an impression of the amount of effort required for the maintenance of such an undertaking, see for example the permanently ongoing so-called \"migrations\" in conda-forge, e.g. here and here . While a lot of rebuilds can be automated (requiring infrastructure that is maintained and operated), the initial integration of a library needs to be done manually, and a persistent percentage of packages (say ~1-10%) will fail any given migration for various reasons, requiring further intervention of a dedicated group of build farm maintainers (occasionally requiring backporting or even authoring patches against the library sources). This is not unique to conda-forge, but a reality for distributions that follow this model, from Debian, Fedora and Ubuntu to Gentoo, vcpkg etc. Problems A build farm has many advantages, listed above. Not having a build farm means not having those advantages. History TODO Relevant resources TODO Potential solutions or mitigations \"Build a build farm\" is easy to write down here, but it's a huge effort and it is unclear how to plan and fund such an effort.","title":"Lack of a build farm for PyPI"},{"location":"meta-topics/no_build_farm/#lack-of-a-build-farm-for-pypi","text":"There is no centralized service (or \"build farm\") for building wheels for packages on PyPI. This is not for lack of agreement on whether that is desirable or not (it's the top item on the PSF's fundable packaging improvements ), but because it's a huge amount of work to create and maintain such a service. Benefits of a build farm include: Easier to build wheels, which means less effort for package authors and a higher percentage of packages having wheels available, Ability to use a consistent build environment, leading to higher quality wheels and fewer issues with binary compatibility between different packages (see, e.g., managing ABI compatibility and complex C++ dependencies ), Ability to roll out upgrades to compiler toolchains or rebuilds of many packages at once (e.g., for a new Python version), Detecting of some classes of issues with releases at upload-to-PyPI time rather than when users open bug reports (e.g., ensure all .py files in a package can actually be imported), Opportunity to grow a packaging community, with collective expertise, resources and policies. Two package repositories that are fairly similar to PyPI, CPAN for Perl and CRAN for R, both don't allow hosting packages on the repository unless the package passes a series of tests. This includes tests that attempt to build the library in an isolated environment to ensure that all dependencies are correctly listed, and running unit tests. From anecdotal evidence, the average quality of those packages is higher, and it's easier to redistribute them in other packaging systems. Pretty much all system package managers as well as Conda and Spack follow similar designs - at least a build should pass and the package should be importable.","title":"Lack of a build farm for PyPI"},{"location":"meta-topics/no_build_farm/#current-state","text":"This topic has been discussed on and of, however there is no concrete effort happening in this direction as of Dec 2022. One early attempt at a solution was conda-press , which aimed to repackage conda packages into wheels, has not been updated since 2019. There were some unresolved design questions or issues with it, and it is unclear if the approach would result in problem-free wheels for the more complex packages. Even if conda-press isn't going anywhere, the conda-forge infrastructure is perhaps the most suitable infrastructure to use as a base to create a new build farm from. It would still be a lot of work to adapt it to PyPI - but far less than starting from scratch. A build farm needs maintainers and its own community around it. That community doesn't exist, and bootstrapping it isn't easy. However, there are places where maintainers have coalesced around common tooling. cibuildwheel is probably the most central point. Now that multibuild is abandoned, cibuildwheel is being used to build wheels on CI systems that are free for open source projects to use by many of the most popular projects with native code. The cost of a build farm Build farms come with substantial-to-massive costs in terms of build & hosting infrastructure, automation that needs to be maintained, and generally human attention/intervention for cases that go wrong. These costs already appear for the above-mentioned \"build and verify\" model used for CRAN. However, when wishing to follow the widely-used practice among distributions to rely predominantly on shared libraries, this brings a massive further increase in necessary automation and maintenance (though the benefits are such that this is the norm rather than the exception). In particular, doing so needs careful tracking which packages are exposed to any given (shared) library's ABI, the ABI stability of that library as expressed through its version scheme, rebuilding all dependent packages once an ABI-changing library version is released, and finally representing all that accurately enough in metadata so that the resolver will only choose compatible package combinations for a given environment. Due to the limitation of only having a single shared library on the path searched by the linker for symbols at runtime (see here for more details), this kind of ecosystem-wide rebuild needs to be done relatively quickly. This is because any given package that has a release in the meantime can only be compiled for either the old or for the new ABI (with very few exceptions for transitions that are more onerous, otherwise one quickly suffers a combinatorial explosion of required build variants), and not moving the ecosystem as a whole to the new baseline essentially means a bifurcation which packages can be co-installed with each other. For an impression of the amount of effort required for the maintenance of such an undertaking, see for example the permanently ongoing so-called \"migrations\" in conda-forge, e.g. here and here . While a lot of rebuilds can be automated (requiring infrastructure that is maintained and operated), the initial integration of a library needs to be done manually, and a persistent percentage of packages (say ~1-10%) will fail any given migration for various reasons, requiring further intervention of a dedicated group of build farm maintainers (occasionally requiring backporting or even authoring patches against the library sources). This is not unique to conda-forge, but a reality for distributions that follow this model, from Debian, Fedora and Ubuntu to Gentoo, vcpkg etc.","title":"Current state"},{"location":"meta-topics/no_build_farm/#problems","text":"A build farm has many advantages, listed above. Not having a build farm means not having those advantages.","title":"Problems"},{"location":"meta-topics/no_build_farm/#history","text":"TODO","title":"History"},{"location":"meta-topics/no_build_farm/#relevant-resources","text":"TODO","title":"Relevant resources"},{"location":"meta-topics/no_build_farm/#potential-solutions-or-mitigations","text":"\"Build a build farm\" is easy to write down here, but it's a huge effort and it is unclear how to plan and fund such an effort.","title":"Potential solutions or mitigations"},{"location":"meta-topics/purposes_of_pypi/","text":"The multiple purposes of PyPI Current state PyPI serves more than one purpose. In its own words, from the pypi.org front page , it is a repository of software for the Python programming language that \"package authors use to distribute their software\" \"helps you find and install software developed and shared by the Python community\" For (2), we can further distinguish: installing packages from binaries (wheels) for a select set of supported platforms installing packages from a source distribution (sdist) on any platform Package authors distributing their software Publishing versions of Python package as source code on PyPI is the original purpose of PyPI. It is the way authors \"announce\" that there is a new version - if it's not on PyPI, then effectively it does not exist as a reusable, public Python package. It is the authoritative flow of source code flow from authors to end users, downstream dependents, and (importantly) to all redistributors of the package. That last category includes: operating system providers: Linux distros, Apple for macOS, Microsoft for Windows, ... standalone packaging systems: Conda, Spack, Nix, Homebrew, Chocolatey, ... sysadmins for deployments inside companies, universities, HPC centers, ... developers of Python distributions: Anaconda Distribution , ActiveState Python , WinPython , RStudio , ... maintainers of other wheel indexes: piwheels (for Raspberry Pi), Christoph Gohkle's Windows wheels (discontinued June '22), Alpine Wheels , ... developers of standalone apps and embedded systems containing Python packages, and probably quite a few more types of consumers, Users installing Python packages from binaries Installing from binaries typically means \"install wheels with pip \" 1 . Today, wheels are responsible of the vast majority of package installs from PyPI, and they are a very convenient way for most Python developers to quickly install up-to-date versions of the packages they need in their development environment. The number of types of wheels which PyPI allows uploading is growing. It is a (sparse) matrix of: Python interpreter 2 : CPython, PyPy Operating system: Windows, macOS, Linux CPU architecture: i686, x86-64, aarch64/arm64, ppc64le, s390x libc flavor: glibc, musl libc A good overview is maintained in the cibuildwheel docs . When installing a wheel, its declared runtime dependencies will also be installed. Those dependencies are found in the metadata inside the wheel. pip 's dependency resolver takes care of this process. Dependency resolution and management is an oft-debated and nontrivial topic in general. Challenges in this area are not specific to packages with native code; once we have wheels, they should behave similarly to pure Python wheels: dependencies are only on other packages on PyPI, everything else should be present within the wheel itself. Users installing Python packages from sdists Installing a package from an sdist typically implies using pip . pip downloads the sdist, builds a wheel from it (using a pyproject.toml build backend hook, or invoking setuptools if such a hook is not present), and then installs it. Installing runtime dependencies then works the same way as described in the section above. The key issue here is dependencies that are needed at build time. It is currently not possible to describe build dependencies other than those that can be found on PyPI. And every package containing native code has such dependencies. In some cases it's only a C compiler - which is usually present on (non-Windows) end user machines. Once one needs other compilers, shared libraries from the system, or other non-PyPI dependencies, things become fragile very quickly. This topic is discussed in Metadata handling on PyPI . Problems The different purposes of PyPI not being separated well enough results in an important problem for package authors. Due to these two \"mix-ups of purposes\": Uploading an sdist makes it available to redistributors (purpose 1) and causes it to be available for users to install from source. pip prefers installing from a wheel, but automatically tries to install from sdist if a wheel cannot be found for the most recent version on PyPI. authors of packages that are hard to build from source have a difficult choice to make. If they do upload an sdist , they may get a flood of bug reports from users with failed builds. If they don't upload an sdist but only wheels, they (a) disrupt the flow of source code to redistributors, (b) diminish the value of PyPI as the authoritative archive of Python packages, and (c) are doing something that makes it harder to apply security best practices (ideally binaries for open source projects always come with source code and can be recreated from them). We'll also remark here that wheels have to be built from source under a very specific set of conditions (e.g., in a manylinux Docker container), which are typically not met on end user systems. Locally built wheels may end up in pip 's cache, which is then \"polluted\" with a wheel that may not work well. There are only a few packaging systems that mix building from source with binary caches - Nix and Spack are two good examples. That only works reliably because the builds are deterministic enough; the package managers know everything that is relevant ( libc , compilers, native dependencies) so binaries in a shared cache can be relied on and really do function like a cache. For PyPI/pip/wheels that is not the case, and as a result this mixing may work in simple cases and lead to unpredictable problems for more complex builds. History Wheels are a relatively recent addition to PyPI - the first release of the wheel package happened in 2012, PEP 427 \u2013 The Wheel Binary Package Format 1.0 was accepted in 2012, and it took a couple of years before it became the norm to upload wheels for Windows, macOS and Linux for every release. Before wheels arrived on the scene, the setuptools -specific Egg format ( .egg files) was regularly used to provide binaries. easy_install plus eggs gave a similar, if less polished, install experience to pip plus wheels. Even before eggs and wheels, it was possible to upload binaries to PyPI. For example, NumPy uploaded .exe installers for Windows (created with NSIS) for years, to help users avoid having to build from source 3 . To use those installers, they had to be manually downloaded and run. These kinds of installers were not common however, they only made sense for packages that were difficult to build from source. Relevant resources Twitter thread (2021 - Steve Dower, Juan Luis Cano Rodr\u00edguez, Ralf Gommers) about pros/cons and importance of uploading sdist's to PyPI. Potential solutions or mitigations Update package installers to not install from sdist by default if a wheel cannot be found (see pip#9140 ). Update PyPI so the three purposes are better separated. E.g., allow upload of sdist's for archival and \"source code to distributors flow\" without making them available for direct installation. There are other installers besides pip , such as pypa/installer .Usage of alternative installers isn't widespread yet. \u21a9 Note that each minor version of CPython and PyPy has to be treated separately (unless one is using the stable ABI), so typically a package will have 3 or 4 wheels for different minor versions of CPython with the same OS, CPU and libc flavors. \u21a9 The .exe and .whl files and their upload dates for the NumPy 1.6.0 release illustrate how long .exe files remained in use, and when wheels were added (retroactively in that case). \u21a9","title":"The multiple purposes of PyPI"},{"location":"meta-topics/purposes_of_pypi/#the-multiple-purposes-of-pypi","text":"","title":"The multiple purposes of PyPI"},{"location":"meta-topics/purposes_of_pypi/#current-state","text":"PyPI serves more than one purpose. In its own words, from the pypi.org front page , it is a repository of software for the Python programming language that \"package authors use to distribute their software\" \"helps you find and install software developed and shared by the Python community\" For (2), we can further distinguish: installing packages from binaries (wheels) for a select set of supported platforms installing packages from a source distribution (sdist) on any platform","title":"Current state"},{"location":"meta-topics/purposes_of_pypi/#package-authors-distributing-their-software","text":"Publishing versions of Python package as source code on PyPI is the original purpose of PyPI. It is the way authors \"announce\" that there is a new version - if it's not on PyPI, then effectively it does not exist as a reusable, public Python package. It is the authoritative flow of source code flow from authors to end users, downstream dependents, and (importantly) to all redistributors of the package. That last category includes: operating system providers: Linux distros, Apple for macOS, Microsoft for Windows, ... standalone packaging systems: Conda, Spack, Nix, Homebrew, Chocolatey, ... sysadmins for deployments inside companies, universities, HPC centers, ... developers of Python distributions: Anaconda Distribution , ActiveState Python , WinPython , RStudio , ... maintainers of other wheel indexes: piwheels (for Raspberry Pi), Christoph Gohkle's Windows wheels (discontinued June '22), Alpine Wheels , ... developers of standalone apps and embedded systems containing Python packages, and probably quite a few more types of consumers,","title":"Package authors distributing their software"},{"location":"meta-topics/purposes_of_pypi/#users-installing-python-packages-from-binaries","text":"Installing from binaries typically means \"install wheels with pip \" 1 . Today, wheels are responsible of the vast majority of package installs from PyPI, and they are a very convenient way for most Python developers to quickly install up-to-date versions of the packages they need in their development environment. The number of types of wheels which PyPI allows uploading is growing. It is a (sparse) matrix of: Python interpreter 2 : CPython, PyPy Operating system: Windows, macOS, Linux CPU architecture: i686, x86-64, aarch64/arm64, ppc64le, s390x libc flavor: glibc, musl libc A good overview is maintained in the cibuildwheel docs . When installing a wheel, its declared runtime dependencies will also be installed. Those dependencies are found in the metadata inside the wheel. pip 's dependency resolver takes care of this process. Dependency resolution and management is an oft-debated and nontrivial topic in general. Challenges in this area are not specific to packages with native code; once we have wheels, they should behave similarly to pure Python wheels: dependencies are only on other packages on PyPI, everything else should be present within the wheel itself.","title":"Users installing Python packages from binaries"},{"location":"meta-topics/purposes_of_pypi/#users-installing-python-packages-from-sdists","text":"Installing a package from an sdist typically implies using pip . pip downloads the sdist, builds a wheel from it (using a pyproject.toml build backend hook, or invoking setuptools if such a hook is not present), and then installs it. Installing runtime dependencies then works the same way as described in the section above. The key issue here is dependencies that are needed at build time. It is currently not possible to describe build dependencies other than those that can be found on PyPI. And every package containing native code has such dependencies. In some cases it's only a C compiler - which is usually present on (non-Windows) end user machines. Once one needs other compilers, shared libraries from the system, or other non-PyPI dependencies, things become fragile very quickly. This topic is discussed in Metadata handling on PyPI .","title":"Users installing Python packages from sdists"},{"location":"meta-topics/purposes_of_pypi/#problems","text":"The different purposes of PyPI not being separated well enough results in an important problem for package authors. Due to these two \"mix-ups of purposes\": Uploading an sdist makes it available to redistributors (purpose 1) and causes it to be available for users to install from source. pip prefers installing from a wheel, but automatically tries to install from sdist if a wheel cannot be found for the most recent version on PyPI. authors of packages that are hard to build from source have a difficult choice to make. If they do upload an sdist , they may get a flood of bug reports from users with failed builds. If they don't upload an sdist but only wheels, they (a) disrupt the flow of source code to redistributors, (b) diminish the value of PyPI as the authoritative archive of Python packages, and (c) are doing something that makes it harder to apply security best practices (ideally binaries for open source projects always come with source code and can be recreated from them). We'll also remark here that wheels have to be built from source under a very specific set of conditions (e.g., in a manylinux Docker container), which are typically not met on end user systems. Locally built wheels may end up in pip 's cache, which is then \"polluted\" with a wheel that may not work well. There are only a few packaging systems that mix building from source with binary caches - Nix and Spack are two good examples. That only works reliably because the builds are deterministic enough; the package managers know everything that is relevant ( libc , compilers, native dependencies) so binaries in a shared cache can be relied on and really do function like a cache. For PyPI/pip/wheels that is not the case, and as a result this mixing may work in simple cases and lead to unpredictable problems for more complex builds.","title":"Problems"},{"location":"meta-topics/purposes_of_pypi/#history","text":"Wheels are a relatively recent addition to PyPI - the first release of the wheel package happened in 2012, PEP 427 \u2013 The Wheel Binary Package Format 1.0 was accepted in 2012, and it took a couple of years before it became the norm to upload wheels for Windows, macOS and Linux for every release. Before wheels arrived on the scene, the setuptools -specific Egg format ( .egg files) was regularly used to provide binaries. easy_install plus eggs gave a similar, if less polished, install experience to pip plus wheels. Even before eggs and wheels, it was possible to upload binaries to PyPI. For example, NumPy uploaded .exe installers for Windows (created with NSIS) for years, to help users avoid having to build from source 3 . To use those installers, they had to be manually downloaded and run. These kinds of installers were not common however, they only made sense for packages that were difficult to build from source.","title":"History"},{"location":"meta-topics/purposes_of_pypi/#relevant-resources","text":"Twitter thread (2021 - Steve Dower, Juan Luis Cano Rodr\u00edguez, Ralf Gommers) about pros/cons and importance of uploading sdist's to PyPI.","title":"Relevant resources"},{"location":"meta-topics/purposes_of_pypi/#potential-solutions-or-mitigations","text":"Update package installers to not install from sdist by default if a wheel cannot be found (see pip#9140 ). Update PyPI so the three purposes are better separated. E.g., allow upload of sdist's for archival and \"source code to distributors flow\" without making them available for direct installation. There are other installers besides pip , such as pypa/installer .Usage of alternative installers isn't widespread yet. \u21a9 Note that each minor version of CPython and PyPy has to be treated separately (unless one is using the stable ABI), so typically a package will have 3 or 4 wheels for different minor versions of CPython with the same OS, CPU and libc flavors. \u21a9 The .exe and .whl files and their upload dates for the NumPy 1.6.0 release illustrate how long .exe files remained in use, and when wheels were added (retroactively in that case). \u21a9","title":"Potential solutions or mitigations"},{"location":"meta-topics/pypi_social_model/","text":"PyPI's author-led social model and its limitations PyPI has an \"author-led social model\", meaning that individual package authors (or groups of authors) control if and when packages get published on PyPI and what they contain. PyPI itself has maintainers, but their purview is the functioning of the PyPI service/infrastructure - dealing with file size limits and issues around names of packages on PyPI (see PEP 541 - Package Index Name Retention ) is the very limited extent to which they get involved in what happens for any particular package name on PyPI. An author-led model is common for language-specific package repositories - npm (JavaScript), crates.io (Rust), CRAN (R), RubyGems (Ruby), and CPAN (Perl) and work roughly the same way 1 . This contrasts with system package managers, which typically have a more centralized social model - they typically have a set of policies to specify that packaging gets done a certain way, and mechanisms and tools for a central team to enforce those policies. Current state There are pros and cons to an author-led social model. Pros include: Package authors have few constraints, and can release their package on PyPI at the time and with the content that they alone determine. Because authors are in control, the latest version of a package is (almost) always available on PyPI. Typically well before it makes its way into a system package manager 2 . Users get a new release very quickly - this helps package development, which benefits from fast feedback cycles. The package contents can be changed in whatever way the authors desire. For distributions there are sometimes issues with packagers patching a package in a way that the original authors disagree with. Those benefits are compelling, and most language specific repositories work this way. So what are the cons? First, there is an important difference between source releases and distributing binaries here (also see The multiple purposes of PyPI ). For source releases, it seems clear that authors should be in full control - it's their source code after all. The vast majority of system package managers don't deal with distributing source code at all. The cons of an author-led model are all related to binaries: A key issue is that coordination across packages is extremely difficult, but necessary. For distributing a working set of binaries that depend on each other, there are myriad coordination points: around ABI management and a common toolchain (see Depending on packages for which an ABI matters ), about building native dependencies once instead of having every consumer of that dependency rebuild and vendor it, about upgrades and rolling out support for new platforms, Python versions, or interpreters like PyPy or Pyston, about runtimes that should be unique in an environment (e.g., see the content about OpenMP ). The ability to do sytem integration and integration testing is very limited. With PyPI, there is a limited amout of integration testing that package authors do in an ad-hoc fashion (e.g., downstream package authors may test pre-releases). For everything else, the user is the integrator. The requirement to avoid external dependencies through vendoring or static linking of dependencies in wheels is directly caused by the author-led social model. This is a significant problem, as discussed in detail in content on native dependencies . During the introduction of support for binaries on PyPI in the form of wheels, this social model seems to have only been used implicitly - it was more or less a given (wheels replaced eggs, which already had similar characteristics). The key issue of vendoring wasn't touched upon in PEP 427 \u2013 The Wheel Binary Package Format 1.0 , and only appears in PEP 513 \u2013 A Platform Tag for Portable Linux Built Distributions where it gets a significant amount of consideration: Bundled Wheels on Linux (PEP 513) While we acknowledge many approaches for dealing with third-party library dependencies within manylinux1 wheels, we recognize that the manylinux1 policy encourages bundling external dependencies, a practice which runs counter to the package management policies of many linux distributions' system package managers. The primary purpose of this is cross-distro compatibility. Furthermore, manylinux1 wheels on PyPI occupy a different niche than the Python packages available through the system package manager. ... The model described in this PEP is most ideally suited for cross-platform Python packages, because it means they can reuse much of the work that they\u2019re already doing to make static Windows and OS X wheels. We recognize that it is less optimal for Linux-specific packages that might prefer to interact more closely with Linux\u2019s unique package management functionality and only care about targeting a small set of particular distos. In hindsight 3 , the viewpoint in the quote above was problematic. Linux is not special here, aside from there being so many distros. All the major issues around coordination, system integration and avoiding external dependencies apply equally on Windows, macOS and Linux. And in practice, complex projects with native code have similar issues on all platforms. Who should be hitting the bugs in a new release? This is a bit of a philosophical question, and only partially related to the author-led vs. centralized social model. However, it is also an important question. Packages with native code tend to have an above-average amount of unexpected issues for any given new version, and those issues take longer to fix. Package authors benefit from fast, high-quality bug reports for those issues. The average end user benefits from stability , and is often not able to generate the desired high-quality bug reports. So to answer the question: ideally, distro packagers, downstream package authors, and early adopters (power users who understand and can deal with the occasional hiccup) should be hitting the bugs in new releases. End users, who may not even realize that they're living on the edge, should not. A problem with PyPI/pip/wheels is that the average Python user is indeed living on the edge, and doesn't know it. Python usage in 2020 and 2021 From the Python Developers Survey 2021 results : There are no great changes in the distribution of Python use cases over the years. Data analysis (51-54%), machine learning (36-38%), web development (45-48%), and DevOps (36-38%) are still the most popular fields for Python usage. About half of all Python users are scientific, engineering, and data science users now. These users are not \"developers\" - they are scientists and engineers first, and programming is a tool to do their actual job. They shouldn't be the ones consuming .0 releases the day they're available. See, e.g., also this \"using software\" vs. \"developing software\" post . Problems It's difficult to do anything that requires coordination across projects. For example, rebuilding all projects for a new Python interpreter, toolchain change, or common library or protocol like Protobuf or DLPack (upgrading in sync after a possible breaking change is easier than staggered upgrades). The limitations of PyPI force themselves on Python projects as limitations on the project as a whole 4 . Because there's a lot of user demand for PyPI and because package authors are responsible for building wheels themselves, they tend to not do anything that doesn't work well on PyPI. Even though the opportunity cost is large. They are not making such trade-offs for other package repositories - they tend to consider that the downstream packagers' problem. The large amount of extra effort for package authors when dealing with native dependencies. For any other packaging system, that cost gets shared across all users of the native dependency. For PyPI, there's a significant extra cost per project of rebuilding, vendoring, etc. Lack of understanding by the Python packaging community about the different types of Python users. In particular, what is often forgotten is the distinction between users who are developers (which includes ~100% of participants in Python packaging design discussions) and users who write Python to get their job done but are not developers (at least, they are not thinking about themselves as developers - Python is simply one tool in their toolbox). Most decisions around Python packaging only consider the developer role. History Some history on introduction of wheels captured above. More history is TODO. Relevant resources Some threads on language-specific vs. system package managers: Usage of language-specific package managers Debian mailing list thread (2013), Language Specific Package Managers blog post by Kevin Cox (2013), Maintaining language-specific module package stacks Ubuntu Discourse thread (2019), Nix vs Language Package manager Nix Discourse thread (2020), Potential solutions or mitigations Improve interoperability with system package managers, so that the strengths and weaknesses of language and system package managers can be combined by users. Separating source and binary distribution on PyPI better (as discussed here ) and/or building a build farm (as discussed here ) may mitigate some of the issues. Develop an explicit shared understanding within the Python packaging community of where the author-led social model really breaks down and the effort of distributing wheels is prohibitive (e.g., the geospatial stack ). PyPI has perhaps the least amount of rules and uniformity of all of the mentioned language-specific repositories, however the big picture is the same for all of them. \u21a9 For Conda, Spack, or Homebrew, the delay of a package updating may be days to weeks. For a Linux distros like Debian it could be weeks (unstable) to years (stable). \u21a9 Hindsight is 20/20. PEP 513 and the introduction of manylinux was still an impressive feat; some challenging technical problems around dealing with ABI compatibility across a large collection of Linux distros were solved in that process. \u21a9 \"We can't use X because on PyPI we cannot guarantee property Y for X\". Example: SciPy forbids using OpenMP, because on PyPI there is no way to ensure that only a single OpenMP runtime (and not libgomp ) is used (see here ). Similar for C++17 usage (blocked by manylinux for a long time), MPI (shared runtime), etc. \u21a9","title":"PyPI's author-led social model and its limitations"},{"location":"meta-topics/pypi_social_model/#pypis-author-led-social-model-and-its-limitations","text":"PyPI has an \"author-led social model\", meaning that individual package authors (or groups of authors) control if and when packages get published on PyPI and what they contain. PyPI itself has maintainers, but their purview is the functioning of the PyPI service/infrastructure - dealing with file size limits and issues around names of packages on PyPI (see PEP 541 - Package Index Name Retention ) is the very limited extent to which they get involved in what happens for any particular package name on PyPI. An author-led model is common for language-specific package repositories - npm (JavaScript), crates.io (Rust), CRAN (R), RubyGems (Ruby), and CPAN (Perl) and work roughly the same way 1 . This contrasts with system package managers, which typically have a more centralized social model - they typically have a set of policies to specify that packaging gets done a certain way, and mechanisms and tools for a central team to enforce those policies.","title":"PyPI's author-led social model and its limitations"},{"location":"meta-topics/pypi_social_model/#current-state","text":"There are pros and cons to an author-led social model. Pros include: Package authors have few constraints, and can release their package on PyPI at the time and with the content that they alone determine. Because authors are in control, the latest version of a package is (almost) always available on PyPI. Typically well before it makes its way into a system package manager 2 . Users get a new release very quickly - this helps package development, which benefits from fast feedback cycles. The package contents can be changed in whatever way the authors desire. For distributions there are sometimes issues with packagers patching a package in a way that the original authors disagree with. Those benefits are compelling, and most language specific repositories work this way. So what are the cons? First, there is an important difference between source releases and distributing binaries here (also see The multiple purposes of PyPI ). For source releases, it seems clear that authors should be in full control - it's their source code after all. The vast majority of system package managers don't deal with distributing source code at all. The cons of an author-led model are all related to binaries: A key issue is that coordination across packages is extremely difficult, but necessary. For distributing a working set of binaries that depend on each other, there are myriad coordination points: around ABI management and a common toolchain (see Depending on packages for which an ABI matters ), about building native dependencies once instead of having every consumer of that dependency rebuild and vendor it, about upgrades and rolling out support for new platforms, Python versions, or interpreters like PyPy or Pyston, about runtimes that should be unique in an environment (e.g., see the content about OpenMP ). The ability to do sytem integration and integration testing is very limited. With PyPI, there is a limited amout of integration testing that package authors do in an ad-hoc fashion (e.g., downstream package authors may test pre-releases). For everything else, the user is the integrator. The requirement to avoid external dependencies through vendoring or static linking of dependencies in wheels is directly caused by the author-led social model. This is a significant problem, as discussed in detail in content on native dependencies . During the introduction of support for binaries on PyPI in the form of wheels, this social model seems to have only been used implicitly - it was more or less a given (wheels replaced eggs, which already had similar characteristics). The key issue of vendoring wasn't touched upon in PEP 427 \u2013 The Wheel Binary Package Format 1.0 , and only appears in PEP 513 \u2013 A Platform Tag for Portable Linux Built Distributions where it gets a significant amount of consideration: Bundled Wheels on Linux (PEP 513) While we acknowledge many approaches for dealing with third-party library dependencies within manylinux1 wheels, we recognize that the manylinux1 policy encourages bundling external dependencies, a practice which runs counter to the package management policies of many linux distributions' system package managers. The primary purpose of this is cross-distro compatibility. Furthermore, manylinux1 wheels on PyPI occupy a different niche than the Python packages available through the system package manager. ... The model described in this PEP is most ideally suited for cross-platform Python packages, because it means they can reuse much of the work that they\u2019re already doing to make static Windows and OS X wheels. We recognize that it is less optimal for Linux-specific packages that might prefer to interact more closely with Linux\u2019s unique package management functionality and only care about targeting a small set of particular distos. In hindsight 3 , the viewpoint in the quote above was problematic. Linux is not special here, aside from there being so many distros. All the major issues around coordination, system integration and avoiding external dependencies apply equally on Windows, macOS and Linux. And in practice, complex projects with native code have similar issues on all platforms. Who should be hitting the bugs in a new release? This is a bit of a philosophical question, and only partially related to the author-led vs. centralized social model. However, it is also an important question. Packages with native code tend to have an above-average amount of unexpected issues for any given new version, and those issues take longer to fix. Package authors benefit from fast, high-quality bug reports for those issues. The average end user benefits from stability , and is often not able to generate the desired high-quality bug reports. So to answer the question: ideally, distro packagers, downstream package authors, and early adopters (power users who understand and can deal with the occasional hiccup) should be hitting the bugs in new releases. End users, who may not even realize that they're living on the edge, should not. A problem with PyPI/pip/wheels is that the average Python user is indeed living on the edge, and doesn't know it. Python usage in 2020 and 2021 From the Python Developers Survey 2021 results : There are no great changes in the distribution of Python use cases over the years. Data analysis (51-54%), machine learning (36-38%), web development (45-48%), and DevOps (36-38%) are still the most popular fields for Python usage. About half of all Python users are scientific, engineering, and data science users now. These users are not \"developers\" - they are scientists and engineers first, and programming is a tool to do their actual job. They shouldn't be the ones consuming .0 releases the day they're available. See, e.g., also this \"using software\" vs. \"developing software\" post .","title":"Current state"},{"location":"meta-topics/pypi_social_model/#problems","text":"It's difficult to do anything that requires coordination across projects. For example, rebuilding all projects for a new Python interpreter, toolchain change, or common library or protocol like Protobuf or DLPack (upgrading in sync after a possible breaking change is easier than staggered upgrades). The limitations of PyPI force themselves on Python projects as limitations on the project as a whole 4 . Because there's a lot of user demand for PyPI and because package authors are responsible for building wheels themselves, they tend to not do anything that doesn't work well on PyPI. Even though the opportunity cost is large. They are not making such trade-offs for other package repositories - they tend to consider that the downstream packagers' problem. The large amount of extra effort for package authors when dealing with native dependencies. For any other packaging system, that cost gets shared across all users of the native dependency. For PyPI, there's a significant extra cost per project of rebuilding, vendoring, etc. Lack of understanding by the Python packaging community about the different types of Python users. In particular, what is often forgotten is the distinction between users who are developers (which includes ~100% of participants in Python packaging design discussions) and users who write Python to get their job done but are not developers (at least, they are not thinking about themselves as developers - Python is simply one tool in their toolbox). Most decisions around Python packaging only consider the developer role.","title":"Problems"},{"location":"meta-topics/pypi_social_model/#history","text":"Some history on introduction of wheels captured above. More history is TODO.","title":"History"},{"location":"meta-topics/pypi_social_model/#relevant-resources","text":"Some threads on language-specific vs. system package managers: Usage of language-specific package managers Debian mailing list thread (2013), Language Specific Package Managers blog post by Kevin Cox (2013), Maintaining language-specific module package stacks Ubuntu Discourse thread (2019), Nix vs Language Package manager Nix Discourse thread (2020),","title":"Relevant resources"},{"location":"meta-topics/pypi_social_model/#potential-solutions-or-mitigations","text":"Improve interoperability with system package managers, so that the strengths and weaknesses of language and system package managers can be combined by users. Separating source and binary distribution on PyPI better (as discussed here ) and/or building a build farm (as discussed here ) may mitigate some of the issues. Develop an explicit shared understanding within the Python packaging community of where the author-led social model really breaks down and the effort of distributing wheels is prohibitive (e.g., the geospatial stack ). PyPI has perhaps the least amount of rules and uniformity of all of the mentioned language-specific repositories, however the big picture is the same for all of them. \u21a9 For Conda, Spack, or Homebrew, the delay of a package updating may be days to weeks. For a Linux distros like Debian it could be weeks (unstable) to years (stable). \u21a9 Hindsight is 20/20. PEP 513 and the introduction of manylinux was still an impressive feat; some challenging technical problems around dealing with ABI compatibility across a large collection of Linux distros were solved in that process. \u21a9 \"We can't use X because on PyPI we cannot guarantee property Y for X\". Example: SciPy forbids using OpenMP, because on PyPI there is no way to ensure that only a single OpenMP runtime (and not libgomp ) is used (see here ). Similar for C++17 usage (blocked by manylinux for a long time), MPI (shared runtime), etc. \u21a9","title":"Potential solutions or mitigations"},{"location":"meta-topics/user_expectations_wheels/","text":"Expectations that projects provide ever more wheels Most users expect pip install project-name to work on all platforms. When building from source is difficult, that expectation translates into requests to projects to provide more wheels. Building many wheels can result in a lot of maintenance work and load on CI systems. Current state See this summary for the combination of operating systems, Python interpreters, CPU architectures and libc flavors which are supported by wheel specs/tags. The cibuildwheel matrix adds up to 70 combinations - and that doesn't include multiple manylinux versions, universal2 for macOS, i686 (32-bit) Linux, or some architectures that were added in PEP 599 ( armv7l , ppc64 ). Nor does it include tags that are likely to be added ( WASM ), or for which tags exist but aren't supported by PyPI ( Pyston , AIX ). In practice, the less popular platforms are not supported by most projects and the number of wheels they upload to PyPI is in the 20-40 range. Examples: Kivy 2.1.0 (20 wheels), NumPy 1.23.5 (27 wheels), Numba 0.56.4 (27 wheels), Mypy 0.991 (29 wheels), asyncpg 0.27.0 (35 wheels), Pydantic 1.10.2 (35 wheels), PyGame 2.1.2 (57 wheels). That is still a large amount of wheels, and maintaining support for them is often problematic. A key ingredient for building wheels is availability of CI systems which support the target platforms. Most projects use 3-4 different CI systems, and/or make use of cross-compilation when a target platform isn't natively supported (e.g., macOS arm64 CI runners were unavailable for 2 years, and are still only available on Cirrus CI as of Dec 2022, and PowerPC ( ppc64le ) and IBM Z ( s390x ) are only available on Travis CI). cibuildwheel has lowered the effort to build wheels compared to multibuild , however the CI system requirements haven't changed. Each CI system requires maintenance - and self-hosted CI runners are even worse in this respect. That maintenance cost can be shared much more easily for packaging systems with centralized builds; for building wheels they have to be paid by every project (see Lack of a build farm for PyPI for more details). Problems The primary problem is the large amount of maintenance effort on CI systems for wheel building. This is particularly painful because: there are usually one a couple of maintainers (perhaps even a single person) who are responsible for or have expertise in wheel building, issues that show up are often specific to the platform and hence cannot be debugged locally on the maintainer's development machine, there are a lot of \"layers\" to wade through: CI system, cibuildwheel, pip, build backend, backend, build system. Another issue is that a lot of discussion needed each time there is a request to any project to support a new platform. History TODO Relevant resources Links to key issues, forum discussions, PEPs, blog posts, etc. Supporting WASM wheels on PyPI thread on Discourse (2022) Reducing effort spent on wheels? thread on the NumPy mailing list (2021) Related to platform support: PEP 11 - CPython platform support , NEP 29 - Recommend Python and NumPy version support as a community policy standard , SciPy Toolchain Roadmap & Official Builds . Potential solutions or mitigations Reducing the number of wheels needed. HPy is probably the most promising way of achieving this. Defining a \"supported platforms\" policy that can be used across projects to make and document decisions around this topic. A build farm for PyPI (see this meta topic ). Having PyPI integrate better with other packaging systems, so users can obtain pure Python packages from PyPI in combination with hard-to-build packages from a package manager with support for the platform of interest. Manage user expectations better, and be honest about limitations of PyPI/wheels . Users are often better served by a distribution that is built and tested in a consistent fashion than by downloading a standalone interpreter and then installing packages from PyPI. This is often not even presented on install pages as an option, let alone recommended.","title":"Expectations that projects provide ever more wheels"},{"location":"meta-topics/user_expectations_wheels/#expectations-that-projects-provide-ever-more-wheels","text":"Most users expect pip install project-name to work on all platforms. When building from source is difficult, that expectation translates into requests to projects to provide more wheels. Building many wheels can result in a lot of maintenance work and load on CI systems.","title":"Expectations that projects provide ever more wheels"},{"location":"meta-topics/user_expectations_wheels/#current-state","text":"See this summary for the combination of operating systems, Python interpreters, CPU architectures and libc flavors which are supported by wheel specs/tags. The cibuildwheel matrix adds up to 70 combinations - and that doesn't include multiple manylinux versions, universal2 for macOS, i686 (32-bit) Linux, or some architectures that were added in PEP 599 ( armv7l , ppc64 ). Nor does it include tags that are likely to be added ( WASM ), or for which tags exist but aren't supported by PyPI ( Pyston , AIX ). In practice, the less popular platforms are not supported by most projects and the number of wheels they upload to PyPI is in the 20-40 range. Examples: Kivy 2.1.0 (20 wheels), NumPy 1.23.5 (27 wheels), Numba 0.56.4 (27 wheels), Mypy 0.991 (29 wheels), asyncpg 0.27.0 (35 wheels), Pydantic 1.10.2 (35 wheels), PyGame 2.1.2 (57 wheels). That is still a large amount of wheels, and maintaining support for them is often problematic. A key ingredient for building wheels is availability of CI systems which support the target platforms. Most projects use 3-4 different CI systems, and/or make use of cross-compilation when a target platform isn't natively supported (e.g., macOS arm64 CI runners were unavailable for 2 years, and are still only available on Cirrus CI as of Dec 2022, and PowerPC ( ppc64le ) and IBM Z ( s390x ) are only available on Travis CI). cibuildwheel has lowered the effort to build wheels compared to multibuild , however the CI system requirements haven't changed. Each CI system requires maintenance - and self-hosted CI runners are even worse in this respect. That maintenance cost can be shared much more easily for packaging systems with centralized builds; for building wheels they have to be paid by every project (see Lack of a build farm for PyPI for more details).","title":"Current state"},{"location":"meta-topics/user_expectations_wheels/#problems","text":"The primary problem is the large amount of maintenance effort on CI systems for wheel building. This is particularly painful because: there are usually one a couple of maintainers (perhaps even a single person) who are responsible for or have expertise in wheel building, issues that show up are often specific to the platform and hence cannot be debugged locally on the maintainer's development machine, there are a lot of \"layers\" to wade through: CI system, cibuildwheel, pip, build backend, backend, build system. Another issue is that a lot of discussion needed each time there is a request to any project to support a new platform.","title":"Problems"},{"location":"meta-topics/user_expectations_wheels/#history","text":"TODO","title":"History"},{"location":"meta-topics/user_expectations_wheels/#relevant-resources","text":"Links to key issues, forum discussions, PEPs, blog posts, etc. Supporting WASM wheels on PyPI thread on Discourse (2022) Reducing effort spent on wheels? thread on the NumPy mailing list (2021) Related to platform support: PEP 11 - CPython platform support , NEP 29 - Recommend Python and NumPy version support as a community policy standard , SciPy Toolchain Roadmap & Official Builds .","title":"Relevant resources"},{"location":"meta-topics/user_expectations_wheels/#potential-solutions-or-mitigations","text":"Reducing the number of wheels needed. HPy is probably the most promising way of achieving this. Defining a \"supported platforms\" policy that can be used across projects to make and document decisions around this topic. A build farm for PyPI (see this meta topic ). Having PyPI integrate better with other packaging systems, so users can obtain pure Python packages from PyPI in combination with hard-to-build packages from a package manager with support for the platform of interest. Manage user expectations better, and be honest about limitations of PyPI/wheels . Users are often better served by a distribution that is built and tested in a consistent fashion than by downloading a standalone interpreter and then installing packages from PyPI. This is often not even presented on install pages as an option, let alone recommended.","title":"Potential solutions or mitigations"}]}